{
    "docs": [
        {
            "location": "/", 
            "text": "About BigGIS\n\n\nBigGIS is a new generation of GIS that supports decision making in multiple\nscenarios which require processing of large and heterogeneous data sets.\n\n\nThe novelty lies in an integrated analytical approach to spatio-temporal\ndata, that are unstructured and from unreliable sources. The system provides\npredictive, prescriptive and visual tool integrated in a common analytical\npipeline.\n\n\n\n\n\n\n\n\nThe project is evaluated on three scenarios:\n\n\n\n\nSmart city\n (urban heat islands, particulate matter)\n\n\nEnvironmental management\n (health threatening animals and plants)\n\n\nDisaster control, civil protection\n (air pollution, toxic chemicals)\n\n\n\n\nPlease visit the \nproject website\n for more\ninformation as well as some interactive demos.\n\n\n\n\n\n\nWhy BigGIS?\n\n\nCurrent GIS solution are mostly tackling big data related requirements in\nterms of data volume or data velocity. In the era of cloud computing,\nleveraging cloud-based resources is a widely adopted pattern. In addition,\nwith the advent of big data analytics, performing massively parallel\nanalytical tasks on large-scale data at rest or data in motion is as well\nbecoming a feasible approach shaping the design of today\u2019s GIS. Although\nscaling out enables GIS to tackle the aforementioned big data induced\nrequirements, there are still two major open issues. Firstly, dealing with\nvarying data types across multiple data sources (variety) lead to data and\nschema heterogeneity, e.g., to describe locations such as addresses, relative\nspatial relationships or different coordinates reference systems. Secondly,\nmodeling the inherent uncertainties in data (veracity), e.g., real-world\nnoise and erroneous values due to the nature of the data collecting process.\nBoth being crucial tasks in data management and analytics that directly\naffect the information retrieval and decision-making quality and moreover\nthe generated knowledge on human-side (value). By leveraging the the\ncontinuous refinement model, we present a holistic approach that explicitly\ndeals with all big data dimensions. By integrating the user in the\nprocess, computers can learn from the cognitive and perceptive skills of\nhuman analysis to create hidden connections between data and the problem\ndomain. This helps to decrease the noise and uncertainty and allows to\nbuild up trust in the analysis results on user side which will eventually\nlead to an increasing likelihood of relevant findings and generated\nknowledge.\n\n\nContact and Support\n\n\n\n\n\n\n\n\nRole\n\n\nName\n\n\nE-mail\n\n\n\n\n\n\n\n\n\n\nContact person\n\n\nProf. Dr. Thomas Setzer\n\n\n\n\n\n\n\n\nProject coordination\n\n\nDr. Viliam Simko", 
            "title": "About BigGIS"
        }, 
        {
            "location": "/#about-biggis", 
            "text": "BigGIS is a new generation of GIS that supports decision making in multiple\nscenarios which require processing of large and heterogeneous data sets.  The novelty lies in an integrated analytical approach to spatio-temporal\ndata, that are unstructured and from unreliable sources. The system provides\npredictive, prescriptive and visual tool integrated in a common analytical\npipeline.     The project is evaluated on three scenarios:   Smart city  (urban heat islands, particulate matter)  Environmental management  (health threatening animals and plants)  Disaster control, civil protection  (air pollution, toxic chemicals)   Please visit the  project website  for more\ninformation as well as some interactive demos.", 
            "title": "About BigGIS"
        }, 
        {
            "location": "/#why-biggis", 
            "text": "Current GIS solution are mostly tackling big data related requirements in\nterms of data volume or data velocity. In the era of cloud computing,\nleveraging cloud-based resources is a widely adopted pattern. In addition,\nwith the advent of big data analytics, performing massively parallel\nanalytical tasks on large-scale data at rest or data in motion is as well\nbecoming a feasible approach shaping the design of today\u2019s GIS. Although\nscaling out enables GIS to tackle the aforementioned big data induced\nrequirements, there are still two major open issues. Firstly, dealing with\nvarying data types across multiple data sources (variety) lead to data and\nschema heterogeneity, e.g., to describe locations such as addresses, relative\nspatial relationships or different coordinates reference systems. Secondly,\nmodeling the inherent uncertainties in data (veracity), e.g., real-world\nnoise and erroneous values due to the nature of the data collecting process.\nBoth being crucial tasks in data management and analytics that directly\naffect the information retrieval and decision-making quality and moreover\nthe generated knowledge on human-side (value). By leveraging the the\ncontinuous refinement model, we present a holistic approach that explicitly\ndeals with all big data dimensions. By integrating the user in the\nprocess, computers can learn from the cognitive and perceptive skills of\nhuman analysis to create hidden connections between data and the problem\ndomain. This helps to decrease the noise and uncertainty and allows to\nbuild up trust in the analysis results on user side which will eventually\nlead to an increasing likelihood of relevant findings and generated\nknowledge.", 
            "title": "Why BigGIS?"
        }, 
        {
            "location": "/#contact-and-support", 
            "text": "Role  Name  E-mail      Contact person  Prof. Dr. Thomas Setzer     Project coordination  Dr. Viliam Simko", 
            "title": "Contact and Support"
        }, 
        {
            "location": "/architecture/", 
            "text": "Overview\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/biggis-infrastructure\n\n\n\n\n\n\nThe BigGIS architecture, as depicted in the picture above, consists of several layers that are briefly discussed in the following.\n\n\nModelling\n\n\nStreamPipes\n\ntbd.\n\n\nAnalytics\n\n\nAnalytics and processing\n\n\n\n\nApache Flink\n\n\nApache Spark\n featuring \nGeoTrellis\n\n\nOther Languages and Notebooks: R, Java, ...\ntbd.\n\n\n\n\nMiddleware \n Connectors\n\n\nData and control flow, connectors are nodes in StreamPipes that enable the user to load various data sources in Apache Kafka or to\n\ntbd.\n\n\nStorage Backends\n\n\nInternally, BigGIS uses a variety of different storage backends for designated purposes.\n\n\n\n\nHDFS\n for GeoTrellis catalog\n\n\nExasol\n for xx\n\n\nCouchDB\n for StreamPipes user and pipeline configurations\n\n\nSesame\n for semantic framework of StreamPipes\ntbd.\n\n\n\n\nContainer Management\n\n\nRunning these containers in a distributed manner requires a wide variety of technologies, that must be integrated and managed throughout their lifecycle. To easily deploy our containers, our infrastructure is designed to run on \nRancher\n as our container management platform. \ntbd.\n\n\nInfrastructure\n\n\nBigGIS infrastructure leverages \nbwCloud\n Infrastructure-as-a-Service (IaaS) offer powered by Openstack.", 
            "title": "Overview"
        }, 
        {
            "location": "/architecture/#overview", 
            "text": "Note  Related repository is  https://github.com/biggis-project/biggis-infrastructure    The BigGIS architecture, as depicted in the picture above, consists of several layers that are briefly discussed in the following.", 
            "title": "Overview"
        }, 
        {
            "location": "/architecture/#modelling", 
            "text": "StreamPipes \ntbd.", 
            "title": "Modelling"
        }, 
        {
            "location": "/architecture/#analytics", 
            "text": "Analytics and processing   Apache Flink  Apache Spark  featuring  GeoTrellis  Other Languages and Notebooks: R, Java, ...\ntbd.", 
            "title": "Analytics"
        }, 
        {
            "location": "/architecture/#middleware-connectors", 
            "text": "Data and control flow, connectors are nodes in StreamPipes that enable the user to load various data sources in Apache Kafka or to \ntbd.", 
            "title": "Middleware &amp; Connectors"
        }, 
        {
            "location": "/architecture/#storage-backends", 
            "text": "Internally, BigGIS uses a variety of different storage backends for designated purposes.   HDFS  for GeoTrellis catalog  Exasol  for xx  CouchDB  for StreamPipes user and pipeline configurations  Sesame  for semantic framework of StreamPipes\ntbd.", 
            "title": "Storage Backends"
        }, 
        {
            "location": "/architecture/#container-management", 
            "text": "Running these containers in a distributed manner requires a wide variety of technologies, that must be integrated and managed throughout their lifecycle. To easily deploy our containers, our infrastructure is designed to run on  Rancher  as our container management platform. \ntbd.", 
            "title": "Container Management"
        }, 
        {
            "location": "/architecture/#infrastructure", 
            "text": "BigGIS infrastructure leverages  bwCloud  Infrastructure-as-a-Service (IaaS) offer powered by Openstack.", 
            "title": "Infrastructure"
        }, 
        {
            "location": "/architecture/Docker_Containers/", 
            "text": "Responsible person for this section\n\n\nPatrick Wiener\n\n\n\n\nComponents\n\n\ntbd. some infos about the structure and hierarchical composition of our Docker images.\n\n\n\n\nall sources should be on github\n\n\nimages should be hosted on dockerhub\n\n\nlist of docker images that should be available:\n\n\nHDFS (should use all bwCloud resources available to BigGIS)\n\n\nKafka with Zookeeper (overview of queues needed)\n\n\nFlink\n\n\nSpark\n\n\nGeotrellis libraries (part of the Spark container?)\n\n\nAccumulo with Geomesa (or Geowave)\n\n\nStreamPipes\n\n\nExasolution\n\n\nExasolution should support Accumulo(Geomesa/Geowave) through virtual schema\n\n\nExasolution should support indexing using 2D, 3D, 4D space filling curves (lat,lon,time,elevation)\n\n\n\n\n\n\nGeo-Server\n\n\nmit Plugin f\u00fcr Accumulo\n\n\nzur Transformation von Formaten\n\n\nauch als Datenquelle f\u00fcr Cadenza", 
            "title": "Components"
        }, 
        {
            "location": "/architecture/Docker_Containers/#components", 
            "text": "tbd. some infos about the structure and hierarchical composition of our Docker images.   all sources should be on github  images should be hosted on dockerhub  list of docker images that should be available:  HDFS (should use all bwCloud resources available to BigGIS)  Kafka with Zookeeper (overview of queues needed)  Flink  Spark  Geotrellis libraries (part of the Spark container?)  Accumulo with Geomesa (or Geowave)  StreamPipes  Exasolution  Exasolution should support Accumulo(Geomesa/Geowave) through virtual schema  Exasolution should support indexing using 2D, 3D, 4D space filling curves (lat,lon,time,elevation)    Geo-Server  mit Plugin f\u00fcr Accumulo  zur Transformation von Formaten  auch als Datenquelle f\u00fcr Cadenza", 
            "title": "Components"
        }, 
        {
            "location": "/architecture/Infrastructure_on_bwCloud/", 
            "text": "Responsible person for this section\n\n\nPatrick Wiener\n\n\n\n\nbwCloud\n\n\n\n\nbwCloud\n VMs should be available, latest status here: bwCloud Status\n\n\nWeb-based admin. interface - dashboard - based on Ambari", 
            "title": "bwCloud"
        }, 
        {
            "location": "/architecture/Infrastructure_on_bwCloud/#bwcloud", 
            "text": "bwCloud  VMs should be available, latest status here: bwCloud Status  Web-based admin. interface - dashboard - based on Ambari", 
            "title": "bwCloud"
        }, 
        {
            "location": "/architecture/StreamPipes/", 
            "text": "Responsible person for this section\n\n\nMatthias Frank\n\n\n\n\nBigGIS extensions\n\n\nWithin the BigGIS project, we added several GIS-specific extensions to the StreamPipes platform.\n\n\n\n\nRaster processing using geotrellis.\n\n\n...", 
            "title": "StreamPipes"
        }, 
        {
            "location": "/architecture/StreamPipes/#biggis-extensions", 
            "text": "Within the BigGIS project, we added several GIS-specific extensions to the StreamPipes platform.   Raster processing using geotrellis.  ...", 
            "title": "BigGIS extensions"
        }, 
        {
            "location": "/consortium/", 
            "text": "Project Consortium\n\n\n\n\n\n\n\nProject Partners\n\n\n\n\nFZI Forschungszentrum Informatik am KIT\n\n\nUniversit\u00e4t Konstanz\n\n\nHochschule Karlsruhe\n\n\nDisy Informationssysteme GmbH\n\n\nEXASOL AG\n\n\nEFTAS Fernerkundung Technologietransfer GmbH\n\n\nLandesanstalt f\u00fcr Umwelt Messungen und Naturschutz\n\n\n\n\nAssociated Partners\n\n\n\n\nTHW Karlsruhe\n\n\nStadt Karlsruhe", 
            "title": "Project Consortium"
        }, 
        {
            "location": "/consortium/#project-consortium", 
            "text": "", 
            "title": "Project Consortium"
        }, 
        {
            "location": "/consortium/#project-partners", 
            "text": "FZI Forschungszentrum Informatik am KIT  Universit\u00e4t Konstanz  Hochschule Karlsruhe  Disy Informationssysteme GmbH  EXASOL AG  EFTAS Fernerkundung Technologietransfer GmbH  Landesanstalt f\u00fcr Umwelt Messungen und Naturschutz", 
            "title": "Project Partners"
        }, 
        {
            "location": "/consortium/#associated-partners", 
            "text": "THW Karlsruhe  Stadt Karlsruhe", 
            "title": "Associated Partners"
        }, 
        {
            "location": "/contributing/", 
            "text": "Contributing\n\n\nWe value all kinds of contributions from the community, not just actual\ncode. If you do like to contribute actual code in the form of bug fixes, new\nfeatures or other patches this page gives you more info on how to do it.\n\n\nGit Branching Model\n\n\nThe BigGIS team follows the standard practice of using the\n\nmaster\n branch as main integration branch.\n\n\nGit Commit Messages\n\n\nWe follow the 'imperative present tense' style for commit messages.\n(e.g. \"Add new EnterpriseWidgetLoader instance\")\n\n\nIssue Tracking\n\n\nIf you find a bug and would like to report it please go to the github\nissue tracker of a given sub-project and file an issue.\n\n\nPull Requests\n\n\nIf you'd like to submit a code contribution please fork BigGIS and\nsend us pull request against the \nmaster\n branch. Like any other open\nsource project, we might ask you to go through some iterations of\ndiscussion and refinement before merging.\n\n\nContributing documentation\n\n\nsee \nDocumentation Howto", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#contributing", 
            "text": "We value all kinds of contributions from the community, not just actual\ncode. If you do like to contribute actual code in the form of bug fixes, new\nfeatures or other patches this page gives you more info on how to do it.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#git-branching-model", 
            "text": "The BigGIS team follows the standard practice of using the master  branch as main integration branch.", 
            "title": "Git Branching Model"
        }, 
        {
            "location": "/contributing/#git-commit-messages", 
            "text": "We follow the 'imperative present tense' style for commit messages.\n(e.g. \"Add new EnterpriseWidgetLoader instance\")", 
            "title": "Git Commit Messages"
        }, 
        {
            "location": "/contributing/#issue-tracking", 
            "text": "If you find a bug and would like to report it please go to the github\nissue tracker of a given sub-project and file an issue.", 
            "title": "Issue Tracking"
        }, 
        {
            "location": "/contributing/#pull-requests", 
            "text": "If you'd like to submit a code contribution please fork BigGIS and\nsend us pull request against the  master  branch. Like any other open\nsource project, we might ask you to go through some iterations of\ndiscussion and refinement before merging.", 
            "title": "Pull Requests"
        }, 
        {
            "location": "/contributing/#contributing-documentation", 
            "text": "see  Documentation Howto", 
            "title": "Contributing documentation"
        }, 
        {
            "location": "/data-sources/", 
            "text": "Datasets in BigGIS\n\n\nData in bwCloud\n\n\nPre-cached / downloaded to bwCloud as a part of BigGIS\n\n\n\n\nATKIS land use data (multiple options possible - TODO:Matthias) TODO:how big\n\n\nshapefiles in a directory\n\n\ndata in Accumulo/Exasolution\n\n\nNew York taxi drives\n\n\n2GB/month -\n for years 2009-2015 potentially ~160GB of storage space\n\n\nmultiple options possible - TODO:Matthias\n\n\nbunch of CSV files in a directories organized per year\n\n\npoints stored in Accumulo\n\n\npoints stored in Exasolution\n\n\nLarge historical data sets (HDFS/Accumulo - TODO:Matthias):\n\n\nLUBW (REST API for pulling)\n\n\nDWD (REST API for pulling)\n\n\nWunderground (TODO:UKON)\n\n\n\n\nConnectors to external services\n\n\nThe following connectors to external data sources should be available (for pulling data as a stream).\n\n\n\n\nPulling data:\n\n\nEnvisat raster data\n\n\nLandsat raster data\n\n\nDWD weather stations\n\n\nLUBW weather stations\n\n\n\n\nWunderground weather stations\n\n\n\n\n\n\nPushing data (REST API needs to be developed from our side):\n\n\n\n\nSensor data from LoRa weather stations\n\n\nVGI data -\n compare KA Feedback (TODO)\n\n\nTODO: hyperspectral images from drones (TODO)\n\n\nTODO: other sensor data from drones (TODO)", 
            "title": "Datasets in BigGIS"
        }, 
        {
            "location": "/data-sources/#datasets-in-biggis", 
            "text": "", 
            "title": "Datasets in BigGIS"
        }, 
        {
            "location": "/data-sources/#data-in-bwcloud", 
            "text": "Pre-cached / downloaded to bwCloud as a part of BigGIS   ATKIS land use data (multiple options possible - TODO:Matthias) TODO:how big  shapefiles in a directory  data in Accumulo/Exasolution  New York taxi drives  2GB/month -  for years 2009-2015 potentially ~160GB of storage space  multiple options possible - TODO:Matthias  bunch of CSV files in a directories organized per year  points stored in Accumulo  points stored in Exasolution  Large historical data sets (HDFS/Accumulo - TODO:Matthias):  LUBW (REST API for pulling)  DWD (REST API for pulling)  Wunderground (TODO:UKON)", 
            "title": "Data in bwCloud"
        }, 
        {
            "location": "/data-sources/#connectors-to-external-services", 
            "text": "The following connectors to external data sources should be available (for pulling data as a stream).   Pulling data:  Envisat raster data  Landsat raster data  DWD weather stations  LUBW weather stations   Wunderground weather stations    Pushing data (REST API needs to be developed from our side):   Sensor data from LoRa weather stations  VGI data -  compare KA Feedback (TODO)  TODO: hyperspectral images from drones (TODO)  TODO: other sensor data from drones (TODO)", 
            "title": "Connectors to external services"
        }, 
        {
            "location": "/data-sources/weather-stations/", 
            "text": "Weather stations\n\n\n\n\nTodo\n\n\nJulian, Hannes, Jochen\n\n\n\n\nLUBW weather station at FZI\n\n\n\n\nthe device was mounted at FZI in January 2017 for measuring air pressure, temperature, wind speed, precipitation (was bedeuten die Parameter \"strg_1\" und \"rlf_1\").\n\n\nmeasurement results (*.mw1.xml) are pushed via xml-files on BigGIS SFTP Server at the University Konstanz \n  -measurement devices will be dismounted at April 2018\n\n\n\n\nLoRa-based weather stations\n\n\n\n\nTwo LoRa gateways should be deployed, one at FZI, the other at LUBW-KA (TODO:clarify)\n\n\nData received by the LoRa gateways:\n\n\nshould be handled in a stream-processing way (pipeline modeled using StreamPipes)\n\n\nshould be stored within BigGIS database i.e. Accumulo/Exasolution (sensor,lat,lon,ts)\n\n\nshould be sent to \nhttps://opensensemap.org/\n (TODO:Jochen)\n\n\n34 LoRa sensor units should be deployed (by Jochen Lutz)\n\n\nREST API for pushing developed using Play2 framework (REST-\nkafka)\n\n\nOutlier filtering node (kafka-\nflink-\nkafka)\n\n\nData persisting node (kafka-\nflink-\naccumulo)\n\n\n\n\nWeb-based mobile-friendly app\n\n\n\n\nQR code contains stations id and URL that leads to public web\n\n\nthe web page contains info about the station and the project\n\n\nadmin can click and change station information (or register a new station)\n\n\nlat/lon is taken from the phone (HTML5 geolocation api)\n\n\nadmin can add additional parameters (placement details)", 
            "title": "Weather stations"
        }, 
        {
            "location": "/data-sources/weather-stations/#weather-stations", 
            "text": "Todo  Julian, Hannes, Jochen", 
            "title": "Weather stations"
        }, 
        {
            "location": "/data-sources/weather-stations/#lubw-weather-station-at-fzi", 
            "text": "the device was mounted at FZI in January 2017 for measuring air pressure, temperature, wind speed, precipitation (was bedeuten die Parameter \"strg_1\" und \"rlf_1\").  measurement results (*.mw1.xml) are pushed via xml-files on BigGIS SFTP Server at the University Konstanz \n  -measurement devices will be dismounted at April 2018", 
            "title": "LUBW weather station at FZI"
        }, 
        {
            "location": "/data-sources/weather-stations/#lora-based-weather-stations", 
            "text": "Two LoRa gateways should be deployed, one at FZI, the other at LUBW-KA (TODO:clarify)  Data received by the LoRa gateways:  should be handled in a stream-processing way (pipeline modeled using StreamPipes)  should be stored within BigGIS database i.e. Accumulo/Exasolution (sensor,lat,lon,ts)  should be sent to  https://opensensemap.org/  (TODO:Jochen)  34 LoRa sensor units should be deployed (by Jochen Lutz)  REST API for pushing developed using Play2 framework (REST- kafka)  Outlier filtering node (kafka- flink- kafka)  Data persisting node (kafka- flink- accumulo)", 
            "title": "LoRa-based weather stations"
        }, 
        {
            "location": "/data-sources/weather-stations/#web-based-mobile-friendly-app", 
            "text": "QR code contains stations id and URL that leads to public web  the web page contains info about the station and the project  admin can click and change station information (or register a new station)  lat/lon is taken from the phone (HTML5 geolocation api)  admin can add additional parameters (placement details)", 
            "title": "Web-based mobile-friendly app"
        }, 
        {
            "location": "/demos/", 
            "text": "About demos ...\n\n\n\n\nTodo\n\n\n\n\nadd some more text\n\n\n\n\n\n\nHere you can find all demos", 
            "title": "About demos ..."
        }, 
        {
            "location": "/demos/#about-demos", 
            "text": "Todo   add some more text    Here you can find all demos", 
            "title": "About demos ..."
        }, 
        {
            "location": "/demos/enviro-car/", 
            "text": "Responsible person for this section\n\n\nManuel Stein\n\n\n\n\nEnviroCar (Smart City)\n\n\nVisualisierung von Verkehrs- und Umweltdaten basierend auf mobilen Sensoren in Fahrzeugen\n\n\nMotivation\n\n\nShort section about available data\n\n\nVisual Analysis of Traffic Data\n\n\n\n\nHighdimensional\n\n\nspatial \n\n\ntemporal\n\n\nspatio temporal \n\n\n\n\nVisual Interactive Logging and Provenance\n\n\n\n\nAbstraction\n\n\nVisualization and Interaction\n\n\n\n\nRelated Scenarios\n\n\n\n\nSmart City\n\n\nEnvironment", 
            "title": "EnviroCar (Smart City)"
        }, 
        {
            "location": "/demos/enviro-car/#envirocar-smart-city", 
            "text": "Visualisierung von Verkehrs- und Umweltdaten basierend auf mobilen Sensoren in Fahrzeugen", 
            "title": "EnviroCar (Smart City)"
        }, 
        {
            "location": "/demos/enviro-car/#motivation", 
            "text": "", 
            "title": "Motivation"
        }, 
        {
            "location": "/demos/enviro-car/#short-section-about-available-data", 
            "text": "", 
            "title": "Short section about available data"
        }, 
        {
            "location": "/demos/enviro-car/#visual-analysis-of-traffic-data", 
            "text": "Highdimensional  spatial   temporal  spatio temporal", 
            "title": "Visual Analysis of Traffic Data"
        }, 
        {
            "location": "/demos/enviro-car/#visual-interactive-logging-and-provenance", 
            "text": "Abstraction  Visualization and Interaction", 
            "title": "Visual Interactive Logging and Provenance"
        }, 
        {
            "location": "/demos/enviro-car/#related-scenarios", 
            "text": "Smart City  Environment", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/demos/gas-detect/", 
            "text": "Responsible person for this section\n\n\nAlexander Groeschel\n\n\n\n\nGas Cloud Detection\n\n\n\n\nChlorophyll-Erkennung im Befliegungsexperiment\n\n\nUnsichtbare Schadgaswolke (IR-Bereich)\n\n\nHei\u00dfe oder kalte Gase/Gegenst\u00e4nde leicht zu erkennen\n\n\nGase mit Umgebungstemperatur im IR-Bild nicht zu erkennen.\n\n\nSubtraktion von georeferenzierten und zus\u00e4tzlich positionskorrigierten Bildern aus Zeitreihen macht kleine \n  Abweichungen in Bildern sichtbar -\n Wolken unsichtbarer Gase im Bild sichtbar\n\n\n\n\nBefliegungskampagne am 15./16.07.17\n\n\n\n\nAnalyse von Gaswolken aus der Luft\n\n\nTools:\n\n\nIR/RGB-Kamera\n\n\nHyperspektralkamera\n\n\n\n\n\n\n\n\n\n\nEtablierung einer Funkstrecke\n\n\n\u00dcbertragung von Flugplan-Daten/Bildergebnissen\n\n\n\n\n\n\n\n\nRelated Scenarios\n\n\n\n\nDisaster Management\n\n\nSmart City", 
            "title": "Gas Cloud Detection"
        }, 
        {
            "location": "/demos/gas-detect/#gas-cloud-detection", 
            "text": "Chlorophyll-Erkennung im Befliegungsexperiment  Unsichtbare Schadgaswolke (IR-Bereich)  Hei\u00dfe oder kalte Gase/Gegenst\u00e4nde leicht zu erkennen  Gase mit Umgebungstemperatur im IR-Bild nicht zu erkennen.  Subtraktion von georeferenzierten und zus\u00e4tzlich positionskorrigierten Bildern aus Zeitreihen macht kleine \n  Abweichungen in Bildern sichtbar -  Wolken unsichtbarer Gase im Bild sichtbar", 
            "title": "Gas Cloud Detection"
        }, 
        {
            "location": "/demos/gas-detect/#befliegungskampagne-am-15160717", 
            "text": "Analyse von Gaswolken aus der Luft  Tools:  IR/RGB-Kamera  Hyperspektralkamera      Etablierung einer Funkstrecke  \u00dcbertragung von Flugplan-Daten/Bildergebnissen", 
            "title": "Befliegungskampagne am 15./16.07.17"
        }, 
        {
            "location": "/demos/gas-detect/#related-scenarios", 
            "text": "Disaster Management  Smart City", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/demos/gas-predict/", 
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nGas Cloud Prediction\n\n\n\n\nModeling of gas clouds and their dispersion over time.\n\n\nJulian's bachelor student (maybe text from his bc-thesis?)", 
            "title": "Gas Cloud Prediction"
        }, 
        {
            "location": "/demos/gas-predict/#gas-cloud-prediction", 
            "text": "Modeling of gas clouds and their dispersion over time.  Julian's bachelor student (maybe text from his bc-thesis?)", 
            "title": "Gas Cloud Prediction"
        }, 
        {
            "location": "/demos/heatstress/", 
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nHeatstress Routing App\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/path-optimizer\n\n\n\n\nRelated Scenarios: \nSmart City\n\n\nThe back-end exposes a simple REST-API, that can be used for routing or to find\nthe optimal point in time. The API can be accessed on \nhttp://localhost:8080/heatstressrouting/api/v1\n.\nJSON is supported as the only output format.\n\n\nThe following sections describe the API in detail.\n\n\nServer information\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/info\n\n\nReturns some information about the running service, e.g. the supported area and time range\n\n\nParameters\n\n\nThe \n/info\n site takes no parameters.\n\n\nReturns\n\n\nReturns some information about the running service (see sample response below):\n\n\n\n\nbbox\n: the bounding box of the area supported by the service as an array of \n[min_lat, min_lng, max_lat, max_lng]\n.\n\n\ntime_range\n: the time range supported by the service, given as time stamps of the form \n2014-08-23T00:00\n.\n\n\nplace_types\n: a list of place types supported by the optimal time api\n\n\n\n\nExample\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/info\n\n\n\nSample Response:\n\n{\n\n  \nservice\n:\n \nheat stress routing\n,\n\n  \nversion\n:\n \n0.0.1-SNAPSHOT\n,\n\n  \nbuild_time\n:\n \n2016-09-27T07:50:42Z\n,\n\n  \nbbox\n:\n \n[\n\n    \n48.99\n,\n\n    \n8.385\n,\n\n    \n49.025\n,\n\n    \n8.435\n\n  \n],\n\n  \ntime_range\n:\n \n{\n\n    \nfrom\n:\n \n2014-08-23T00:00\n,\n\n    \nto\n:\n \n2016-02-23T23:00\n\n  \n},\n\n  \nplace_types\n:\n \n[\n\n    \nbakery\n,\n\n    \ntaxi\n,\n\n    \npost_office\n,\n\n    \nice_cream\n,\n\n    \ndentist\n,\n\n    \npost_box\n,\n\n    \nsupermarket\n,\n\n    \ntoilets\n,\n\n    \nbank\n,\n\n    \ncafe\n,\n\n    \npolice\n,\n\n    \ndoctors\n,\n\n    \npharmacy\n,\n\n    \ndrinking_water\n,\n\n    \natm\n,\n\n    \nclinic\n,\n\n    \nkiosk\n,\n\n    \nhospital\n,\n\n    \nchemist\n,\n\n    \nfast_food\n\n  \n]\n\n\n}\n\n\n\n\nRouting\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/routing\n\n\nComputes the optimal route (regarding heat stress) between a start and a destination at a given time.\n\n\nParameters\n\n\nThe \n/routing\n api supports the following parameter (some are optional):\n\n\n\n\nstart\n: the start point as pair of a latitude value and longitude value (in that order)\n  seperated by a comma, e.g. \nstart=49.0118083,8.4251357\n.\n\n\ndestination\n: the destination as pair of a latitude value and longitude value (in that order)\n  separated by a comma, e.g. \ndestination=49.0126868,8.4065707\n. \n\n\ntime\n: the date and time the optimal route should be searched for; a time stamp of the form\n  \nYYYY-MM-DDTHH:MM:SS\n, e.g. \ntime=2015-08-31T10:00:00\n. The value must be in the time range\n  returned by \n/info\n (see \nabove\n).\n\n\nweighting\n (optional): the weightings to be used; a comma seperated list of the supported\n  weightings (\nshortest\n, \nheatindex\n and \ntemperature\n), e.g. \nweighting=shortest,heatindex,temperature\n;\n  the default is \nweighting=shortest,heatindex\n; the results for the \nshortest\n weighting are always\n  returned, even if the value is omited in the weighings list.\n\n\n\n\nReturns\n\n\nThe path and some other information for each of the weightings:\n\n\n\n\nstatus\n: the status of the request; \nOK\n is everthing is okay, \nBAD_REQUEST\n if a invalid request was send or \nINTERNAL_SERVER_ERROR\n if an internal error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\nresults\n: the results for each weighting:\n\n\nweighting\n: the weighting used for that result (see parameter \nweighting\n above).\n\n\nstart\n: the coordinates of the start point as array of \n[lat, lng]\n.\n\n\ndestination\n: the coordinates of the destination as array of \n[lat, lng]\n.\n\n\ndistance\n: the length of the route in meter.\n\n\nduration\n: the walking time in milli seconds.\n\n\nroute_weights\n: the route weights of the selected weightings for the route.\n\n\npath\n: the geometry of the path found; an array of points, were each point is an array of [lat, lng]`.\n\n\n\n\n\n\n\n\nExample\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/routing?start=49.0118083,8.4251357\ndestination=49.0126868,8.4065707\ntime=2015-08-31T10:00:00\nweighting=shortest,heatindex,temperature\n\n\n\nSample Response:\n\n{\n\n  \nstatus\n:\n \nOK\n,\n\n  \nstatus_code\n:\n \n200\n,\n\n  \nresults\n:\n \n{\n\n    \nshortest\n:\n \n{\n\n      \nweighting\n:\n \nshortest\n,\n\n      \nstart\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \ndestination\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \ndistance\n:\n \n1698.2989202985977\n,\n\n      \nduration\n:\n \n1222740\n,\n\n      \nroute_weights\n:\n \n{\n\n        \ntemperature\n:\n \n50903.955833052285\n,\n\n        \nheatindex\n:\n \n50892.20496302502\n,\n\n        \nshortest\n:\n \n1698.2989202985977\n\n      \n},\n\n      \npath\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \nheatindex\n:\n \n{\n\n      \nweighting\n:\n \nheatindex\n,\n\n      \nstart\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \ndestination\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \ndistance\n:\n \n1901.8839202985973\n,\n\n      \nduration\n:\n \n1369323\n,\n\n      \nroute_weights\n:\n \n{\n\n        \ntemperature\n:\n \n51868.74807902536\n,\n\n        \nheatindex\n:\n \n51098.277424417196\n,\n\n        \nshortest\n:\n \n1901.8839202985978\n\n      \n},\n\n      \npath\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \ntemperature\n:\n \n{\n\n      \nweighting\n:\n \ntemperature\n,\n\n      \nstart\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \ndestination\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \ndistance\n:\n \n1901.8839202985973\n,\n\n      \nduration\n:\n \n1369323\n,\n\n      \nroute_weights\n:\n \n{\n\n        \ntemperature\n:\n \n51868.74807902536\n,\n\n        \nheatindex\n:\n \n51098.277424417196\n,\n\n        \nshortest\n:\n \n1901.8839202985978\n\n      \n},\n\n      \npath\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\nOptimal time\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime\n\n\nPerformce a nearby search for a given start point and computes for every place that fulfills\na specified criterion an optimal point in time, i.e. the time with the minimal heat stress.\n\n\nParameters\n\n\nThe \n/optimaltime\n api supports the following parameter (some are optional):\n\n\n\n\nstart\n: the start point as pair of a latitude value and longitude value (in that order) seperated by a comma, e.g. \nstart=49.0118083,8.4251357\n. \n\n\ntime\n: the date and time the optimal route should be searched for; a time stamp of the form \nYYYY-MM-DDTHH:MM:SS\n, e.g. \ntime=2015-08-31T10:00:00\n. The value must be in the time range returned by \n/info\n (see \nabove\n).\n\n\nplace_type\n: the place type to search for; a comma seperated list of supported place types, e.g. \nplace_type=supermarket,chemist\n; a complete list of supported place list can be queried using the \ninfo\n api (see \nabove\n). Currently the following place tyes are supported: \nbakery\n, \ntaxi\n, \npost_office\n, \nice_cream\n, \ndentist\n, \npost_box\n, \nsupermarket\n, \ntoilets\n, \nbank\n, \ncafe\n, \npolice\n, \ndoctors\n, \npharmacy\n, \ndrinking_water\n, \natm\n, \nclinic\n, \nkiosk\n, \nhospital\n, \nchemist\n, \nfast_food\n. The place types are mapped to the corresponding \nshop\n respectively \namenity\n tags.\n\n\nmax_results\n (optional): the maximum number of results to consider for the nearby search (an positive integer), e.g. \nmax_results=10\n; the default value is 5.\n\n\nmax_distance\n (optional): the maximum direct distance (as the crow flies) between the start point and the place in meter, e.g. \nmax_distance=500.0\n; the default value is 1000.0 meter.\n\n\ntime_buffer\n (optional): the minimum time needed at the place (in minutes), i.e. the optimal time is chossen so that the place is opened for a least \ntime_buffer\n when the user arrives, e.g. \ntime_buffer=30\n; the default value is 15 miniutes.\n\n\nearliest_time\n (optional): the earliest desired time, either a time stamp, e.g. \nearliest_time=2015-08-31T09:00\n or the string \nnull\n (case is ignored); the default value is \nnull\n. If both \nearliest_time\n and \nlatest_time\n are specified, \nearliest_time\n must be before \nlatest_time\n.\n\n\nlatest_time\n (optional): the latest desired time, either a time stamp, e.g. \nlatest_time=2015-08-31T17:00\n or the string \nnull\n (case is ignored); the default value is \nnull\n. If both \nearliest_time\n and \nlatest_time\n are specified, \nearliest_time\n must be before \nlatest_time\n; \nlatest_time\n must be after \ntime\n.\n\n\n\n\nReturns\n\n\nThe optimal point in time for each place found in the specified radius ranked by the optimal-value:\n\n\n\n\nstatus\n: the status of the request; \nOK\n if everthing is okay, \nNO_REULTS\n if not results were found, \nBAD_REQUEST\n if a invalid request was send to the server or \nINTERNAL_SERVER_ERROR\n if an internal server error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\n\n\nresults\n: the result for each place found during the nearby search:\n\n\n\n\nrank\n: the rank of the place according to the optimal value (were 1 is the best rank).\n\n\nname\n: the name of the place.\n\n\nosm_id\n: the \nOpenStreetMap Node ID\n of the place.\n\n\nlocation\n: the coordinates of the places as an array of \n[lat, lng]\n.\n\n\nopening_hours\n: the opening hours of the place; the format specification can be found \nhere\n.\n\n\noptimal_time\n: the optimal point in time found for that place, e.g. \n2015-08-31T20:00\n\n\noptimal_value\n: the optimal value found for the place; the value considering the heat stress acording to steadman's heatindex \n(Steadmean, 1979)\n as well as the distance between the start and the place.\n\n\ndistance\n: the length of the optimal path (see \nRouting\n above) from the start to the place in meter.\n\n\nduration\n: the time needed to walk from the start to the place (in milli seconds).\n\n\npath_optimal\n: the geometry of the optimal path (see \nRouting\n above).\n\n\ndistance_shortest\n: the length of the shortest path between the start and the place (in meter).\n\n\nduration_shortest\n: the time needed to walk the shortest path between the start and the place (in milli seconds).\n\n\npath_optimal\n: the geometry of the shortest path (see \nRouting\n above).\n\n\n\n\n\n\n\n\nExample\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime?start=49.0118083,8.4251357\ntime=2015-08-31T10:00:00\nplace_type=supermarket\nmax_distance=1000\nmax_results=5\ntime_buffer=15\nearliest_time=2015-08-31T09:00:00\nlatest_time=2015-08-31T20:00:00\n\n\n\nSample Response:\n\n{\n\n  \nstatus\n:\n \nOK\n,\n\n  \nstatus_code\n:\n \n200\n,\n\n  \nresults\n:\n \n[\n\n    \n{\n\n      \nrank\n:\n \n1\n,\n\n      \nname\n:\n \nRewe City\n,\n\n      \nosm_id\n:\n \n897615202\n,\n\n      \nlocation\n:\n \n[\n\n        \n49.0096613\n,\n\n        \n8.4237272\n\n      \n],\n\n      \nopening_hours\n:\n \nMo-Sa 07:00-22:00; Su,PH off\n,\n\n      \noptimal_time\n:\n \n2015-08-31T20:00\n,\n\n      \noptimal_value\n:\n \n12515.36230258099\n,\n\n      \ndistance\n:\n \n539.1839746027457\n,\n\n      \nduration\n:\n \n388207\n,\n\n      \npath_optimal\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00954480942009\n,\n\n          \n8.423681942364334\n\n        \n]\n\n      \n],\n\n      \ndistance_shortest\n:\n \n468.99728441805115\n,\n\n      \nduration_shortest\n:\n \n337669\n,\n\n      \npath_shortest\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00954480942009\n,\n\n          \n8.423681942364334\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \n{\n\n      \nrank\n:\n \n2\n,\n\n      \nname\n:\n \nOststadt Super-Bio-Markt\n,\n\n      \nosm_id\n:\n \n931682116\n,\n\n      \nlocation\n:\n \n[\n\n        \n49.009433\n,\n\n        \n8.4234214\n\n      \n],\n\n      \nopening_hours\n:\n \nMo-Fr 09:00-13:00,14:00-18:30; Sa 09:00-13:00\n,\n\n      \noptimal_time\n:\n \n2015-08-31T18:09:19.199\n,\n\n      \noptimal_value\n:\n \n14318.962937267655\n,\n\n      \ndistance\n:\n \n473.346750294328\n,\n\n      \nduration\n:\n \n340801\n,\n\n      \npath_optimal\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00944708743373\n,\n\n          \n8.4235711322383\n\n        \n]\n\n      \n],\n\n      \ndistance_shortest\n:\n \n473.346750294328\n,\n\n      \nduration_shortest\n:\n \n340801\n,\n\n      \npath_shortest\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00944708743373\n,\n\n          \n8.4235711322383\n\n        \n]\n\n      \n]\n\n    \n}\n\n  \n]\n\n\n}\n\n\n\n\nError messages\n\n\nIf an error occurs, e.g. because a bade request were send to the server or an internal server errors occurs, the server is sending a JSON response with the following content:\n\n\n\n\nstatus\n: the status of the request; \nOK\n if everthing is okay, \nNO_REULTS\n if not results were found, \nBAD_REQUEST\n if a invalid request was send to the server or \nINTERNAL_SERVER_ERROR\n if an internal server error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\nmessages\n: an array of human readable error messages.\n\n\n\n\nExample\n\n\nExample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime?start=Schloss,%20Karlsruhe\ntime=2015-08-31T10:00:00\nplace_type=supermarket\n\n\n\nExample Response:\n\n{\n\n  \nstatus\n:\n \nBAD_REQUEST\n,\n\n  \nstatus_code\n:\n \n400\n,\n\n  \nmessages\n:\n \n[\n\n    \nstart (Schloss, Karlsruhe) could not be parsed: failed to parse coordinate; \nstart\n must be a pair of latitude and longitude seperated by a comma (\n,\n), e.g. \n49.0118083,8.4251357\n)\n\n  \n]\n\n\n}\n\n\n\n\nReferences\n\n\n\n\nReference\n\n\nSteadman, R. G. \nThe Assessment of Sultriness. Part I: A Temperature-Humidity Index Based on Human Physiology and Clothing.\n\nScience Journal of Applied Meteorology, 1979, 18, 861-873, DOI: 10.1175/1520-0450(1979)018\n0861:TAOSPI\n2.0.CO;2", 
            "title": "Heatstress Routing App"
        }, 
        {
            "location": "/demos/heatstress/#heatstress-routing-app", 
            "text": "Note  Related repository is  https://github.com/biggis-project/path-optimizer   Related Scenarios:  Smart City  The back-end exposes a simple REST-API, that can be used for routing or to find\nthe optimal point in time. The API can be accessed on  http://localhost:8080/heatstressrouting/api/v1 .\nJSON is supported as the only output format.  The following sections describe the API in detail.", 
            "title": "Heatstress Routing App"
        }, 
        {
            "location": "/demos/heatstress/#server-information", 
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/info  Returns some information about the running service, e.g. the supported area and time range", 
            "title": "Server information"
        }, 
        {
            "location": "/demos/heatstress/#parameters", 
            "text": "The  /info  site takes no parameters.", 
            "title": "Parameters"
        }, 
        {
            "location": "/demos/heatstress/#returns", 
            "text": "Returns some information about the running service (see sample response below):   bbox : the bounding box of the area supported by the service as an array of  [min_lat, min_lng, max_lat, max_lng] .  time_range : the time range supported by the service, given as time stamps of the form  2014-08-23T00:00 .  place_types : a list of place types supported by the optimal time api", 
            "title": "Returns"
        }, 
        {
            "location": "/demos/heatstress/#example", 
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/info  Sample Response: { \n   service :   heat stress routing , \n   version :   0.0.1-SNAPSHOT , \n   build_time :   2016-09-27T07:50:42Z , \n   bbox :   [ \n     48.99 , \n     8.385 , \n     49.025 , \n     8.435 \n   ], \n   time_range :   { \n     from :   2014-08-23T00:00 , \n     to :   2016-02-23T23:00 \n   }, \n   place_types :   [ \n     bakery , \n     taxi , \n     post_office , \n     ice_cream , \n     dentist , \n     post_box , \n     supermarket , \n     toilets , \n     bank , \n     cafe , \n     police , \n     doctors , \n     pharmacy , \n     drinking_water , \n     atm , \n     clinic , \n     kiosk , \n     hospital , \n     chemist , \n     fast_food \n   ]  }", 
            "title": "Example"
        }, 
        {
            "location": "/demos/heatstress/#routing", 
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/routing  Computes the optimal route (regarding heat stress) between a start and a destination at a given time.", 
            "title": "Routing"
        }, 
        {
            "location": "/demos/heatstress/#parameters_1", 
            "text": "The  /routing  api supports the following parameter (some are optional):   start : the start point as pair of a latitude value and longitude value (in that order)\n  seperated by a comma, e.g.  start=49.0118083,8.4251357 .  destination : the destination as pair of a latitude value and longitude value (in that order)\n  separated by a comma, e.g.  destination=49.0126868,8.4065707 .   time : the date and time the optimal route should be searched for; a time stamp of the form\n   YYYY-MM-DDTHH:MM:SS , e.g.  time=2015-08-31T10:00:00 . The value must be in the time range\n  returned by  /info  (see  above ).  weighting  (optional): the weightings to be used; a comma seperated list of the supported\n  weightings ( shortest ,  heatindex  and  temperature ), e.g.  weighting=shortest,heatindex,temperature ;\n  the default is  weighting=shortest,heatindex ; the results for the  shortest  weighting are always\n  returned, even if the value is omited in the weighings list.", 
            "title": "Parameters"
        }, 
        {
            "location": "/demos/heatstress/#returns_1", 
            "text": "The path and some other information for each of the weightings:   status : the status of the request;  OK  is everthing is okay,  BAD_REQUEST  if a invalid request was send or  INTERNAL_SERVER_ERROR  if an internal error occoured.  status_code : the HTTP status code returned.  results : the results for each weighting:  weighting : the weighting used for that result (see parameter  weighting  above).  start : the coordinates of the start point as array of  [lat, lng] .  destination : the coordinates of the destination as array of  [lat, lng] .  distance : the length of the route in meter.  duration : the walking time in milli seconds.  route_weights : the route weights of the selected weightings for the route.  path : the geometry of the path found; an array of points, were each point is an array of [lat, lng]`.", 
            "title": "Returns"
        }, 
        {
            "location": "/demos/heatstress/#example_1", 
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/routing?start=49.0118083,8.4251357 destination=49.0126868,8.4065707 time=2015-08-31T10:00:00 weighting=shortest,heatindex,temperature  Sample Response: { \n   status :   OK , \n   status_code :   200 , \n   results :   { \n     shortest :   { \n       weighting :   shortest , \n       start :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       destination :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       distance :   1698.2989202985977 , \n       duration :   1222740 , \n       route_weights :   { \n         temperature :   50903.955833052285 , \n         heatindex :   50892.20496302502 , \n         shortest :   1698.2989202985977 \n       }, \n       path :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     }, \n     heatindex :   { \n       weighting :   heatindex , \n       start :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       destination :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       distance :   1901.8839202985973 , \n       duration :   1369323 , \n       route_weights :   { \n         temperature :   51868.74807902536 , \n         heatindex :   51098.277424417196 , \n         shortest :   1901.8839202985978 \n       }, \n       path :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     }, \n     temperature :   { \n       weighting :   temperature , \n       start :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       destination :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       distance :   1901.8839202985973 , \n       duration :   1369323 , \n       route_weights :   { \n         temperature :   51868.74807902536 , \n         heatindex :   51098.277424417196 , \n         shortest :   1901.8839202985978 \n       }, \n       path :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     } \n   }  }", 
            "title": "Example"
        }, 
        {
            "location": "/demos/heatstress/#optimal-time", 
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/optimaltime  Performce a nearby search for a given start point and computes for every place that fulfills\na specified criterion an optimal point in time, i.e. the time with the minimal heat stress.", 
            "title": "Optimal time"
        }, 
        {
            "location": "/demos/heatstress/#parameters_2", 
            "text": "The  /optimaltime  api supports the following parameter (some are optional):   start : the start point as pair of a latitude value and longitude value (in that order) seperated by a comma, e.g.  start=49.0118083,8.4251357 .   time : the date and time the optimal route should be searched for; a time stamp of the form  YYYY-MM-DDTHH:MM:SS , e.g.  time=2015-08-31T10:00:00 . The value must be in the time range returned by  /info  (see  above ).  place_type : the place type to search for; a comma seperated list of supported place types, e.g.  place_type=supermarket,chemist ; a complete list of supported place list can be queried using the  info  api (see  above ). Currently the following place tyes are supported:  bakery ,  taxi ,  post_office ,  ice_cream ,  dentist ,  post_box ,  supermarket ,  toilets ,  bank ,  cafe ,  police ,  doctors ,  pharmacy ,  drinking_water ,  atm ,  clinic ,  kiosk ,  hospital ,  chemist ,  fast_food . The place types are mapped to the corresponding  shop  respectively  amenity  tags.  max_results  (optional): the maximum number of results to consider for the nearby search (an positive integer), e.g.  max_results=10 ; the default value is 5.  max_distance  (optional): the maximum direct distance (as the crow flies) between the start point and the place in meter, e.g.  max_distance=500.0 ; the default value is 1000.0 meter.  time_buffer  (optional): the minimum time needed at the place (in minutes), i.e. the optimal time is chossen so that the place is opened for a least  time_buffer  when the user arrives, e.g.  time_buffer=30 ; the default value is 15 miniutes.  earliest_time  (optional): the earliest desired time, either a time stamp, e.g.  earliest_time=2015-08-31T09:00  or the string  null  (case is ignored); the default value is  null . If both  earliest_time  and  latest_time  are specified,  earliest_time  must be before  latest_time .  latest_time  (optional): the latest desired time, either a time stamp, e.g.  latest_time=2015-08-31T17:00  or the string  null  (case is ignored); the default value is  null . If both  earliest_time  and  latest_time  are specified,  earliest_time  must be before  latest_time ;  latest_time  must be after  time .", 
            "title": "Parameters"
        }, 
        {
            "location": "/demos/heatstress/#returns_2", 
            "text": "The optimal point in time for each place found in the specified radius ranked by the optimal-value:   status : the status of the request;  OK  if everthing is okay,  NO_REULTS  if not results were found,  BAD_REQUEST  if a invalid request was send to the server or  INTERNAL_SERVER_ERROR  if an internal server error occoured.  status_code : the HTTP status code returned.   results : the result for each place found during the nearby search:   rank : the rank of the place according to the optimal value (were 1 is the best rank).  name : the name of the place.  osm_id : the  OpenStreetMap Node ID  of the place.  location : the coordinates of the places as an array of  [lat, lng] .  opening_hours : the opening hours of the place; the format specification can be found  here .  optimal_time : the optimal point in time found for that place, e.g.  2015-08-31T20:00  optimal_value : the optimal value found for the place; the value considering the heat stress acording to steadman's heatindex  (Steadmean, 1979)  as well as the distance between the start and the place.  distance : the length of the optimal path (see  Routing  above) from the start to the place in meter.  duration : the time needed to walk from the start to the place (in milli seconds).  path_optimal : the geometry of the optimal path (see  Routing  above).  distance_shortest : the length of the shortest path between the start and the place (in meter).  duration_shortest : the time needed to walk the shortest path between the start and the place (in milli seconds).  path_optimal : the geometry of the shortest path (see  Routing  above).", 
            "title": "Returns"
        }, 
        {
            "location": "/demos/heatstress/#example_2", 
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/optimaltime?start=49.0118083,8.4251357 time=2015-08-31T10:00:00 place_type=supermarket max_distance=1000 max_results=5 time_buffer=15 earliest_time=2015-08-31T09:00:00 latest_time=2015-08-31T20:00:00  Sample Response: { \n   status :   OK , \n   status_code :   200 , \n   results :   [ \n     { \n       rank :   1 , \n       name :   Rewe City , \n       osm_id :   897615202 , \n       location :   [ \n         49.0096613 , \n         8.4237272 \n       ], \n       opening_hours :   Mo-Sa 07:00-22:00; Su,PH off , \n       optimal_time :   2015-08-31T20:00 , \n       optimal_value :   12515.36230258099 , \n       distance :   539.1839746027457 , \n       duration :   388207 , \n       path_optimal :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00954480942009 , \n           8.423681942364334 \n         ] \n       ], \n       distance_shortest :   468.99728441805115 , \n       duration_shortest :   337669 , \n       path_shortest :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00954480942009 , \n           8.423681942364334 \n         ] \n       ] \n     }, \n     { \n       rank :   2 , \n       name :   Oststadt Super-Bio-Markt , \n       osm_id :   931682116 , \n       location :   [ \n         49.009433 , \n         8.4234214 \n       ], \n       opening_hours :   Mo-Fr 09:00-13:00,14:00-18:30; Sa 09:00-13:00 , \n       optimal_time :   2015-08-31T18:09:19.199 , \n       optimal_value :   14318.962937267655 , \n       distance :   473.346750294328 , \n       duration :   340801 , \n       path_optimal :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00944708743373 , \n           8.4235711322383 \n         ] \n       ], \n       distance_shortest :   473.346750294328 , \n       duration_shortest :   340801 , \n       path_shortest :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00944708743373 , \n           8.4235711322383 \n         ] \n       ] \n     } \n   ]  }", 
            "title": "Example"
        }, 
        {
            "location": "/demos/heatstress/#error-messages", 
            "text": "If an error occurs, e.g. because a bade request were send to the server or an internal server errors occurs, the server is sending a JSON response with the following content:   status : the status of the request;  OK  if everthing is okay,  NO_REULTS  if not results were found,  BAD_REQUEST  if a invalid request was send to the server or  INTERNAL_SERVER_ERROR  if an internal server error occoured.  status_code : the HTTP status code returned.  messages : an array of human readable error messages.", 
            "title": "Error messages"
        }, 
        {
            "location": "/demos/heatstress/#example_3", 
            "text": "Example Request: http://localhost:8080/heatstressrouting/api/v1/optimaltime?start=Schloss,%20Karlsruhe time=2015-08-31T10:00:00 place_type=supermarket  Example Response: { \n   status :   BAD_REQUEST , \n   status_code :   400 , \n   messages :   [ \n     start (Schloss, Karlsruhe) could not be parsed: failed to parse coordinate;  start  must be a pair of latitude and longitude seperated by a comma ( , ), e.g.  49.0118083,8.4251357 ) \n   ]  }", 
            "title": "Example"
        }, 
        {
            "location": "/demos/heatstress/#references", 
            "text": "Reference  Steadman, R. G.  The Assessment of Sultriness. Part I: A Temperature-Humidity Index Based on Human Physiology and Clothing. \nScience Journal of Applied Meteorology, 1979, 18, 861-873, DOI: 10.1175/1520-0450(1979)018 0861:TAOSPI 2.0.CO;2", 
            "title": "References"
        }, 
        {
            "location": "/demos/hotspot_analysis/", 
            "text": "Hotspot analysis (using G*)\n\n\nProblem definition\n\n\n\n\nWe have a map, in this case a map of land surface temperatures\n\n\nWe want to find hotspots, i.e., areas on map that are \nsignificantly different from their surrounding area\n.\n\n\nWe want to use Getis-Ord G* statistic as the tool for finding the hotspots\n\n\nsee section \nStandards Getis-ord\n\n\n\n\n\n\nWe want to parallelize the computation in our Spark cluster.\n\n\nsee section \nRasterized Getis-ord\n\n\n\n\n\n\n\n\n\n\nHotspot analysis using geotrellis\n\n\nIn this section we show a simplified version of the hotspot analysis.\nWe use the \nGeotrellis\n library to achieve the paralleization.\nSome assumptions are:\n\n\n\n\nwe use 2-dimenational data (only the spatial part without the time component)\n\n\nwe store our data as a distributed raster (in geotrellis catalog)\n\n\nour hotspot analysis uses the standard G* with variable window\n\n\n\n\nFirst, we need to express the G* formula in terms of the map algebra operations.\n\n\nScala code snippets\n\n\n// typical type definition used by geotrellis\n\n\ntype\n \nSpatialRDD\n \n=\n \nRDD\n[(\nSpatialKey\n, \nTile\n)]\n\n                  \nwith\n \nMetadata\n[\nTileLayerMetadata\n[\nSpatialKey\n]]\n\n\n\ndef\n \ngetisord\n(\nrdd\n:\n \nSpatialRDD\n,\n \nweightMatrix\n:\n \nKernel\n,\n\n             \nglobMean\n:\nDouble\n,\n \nglobStdev\n:\nDouble\n,\n \nnumPixels\n:\nLong\n)\n:\n \nSpatialRDD\n \n=\n \n{\n\n\n  \nval\n \nwcells\n \n=\n \nweightMatrix\n.\ntile\n.\ntoArrayDouble\n\n  \nval\n \nsumW\n \n=\n \nwcells\n.\nsum\n\n  \nval\n \nsumW2\n \n=\n \nwcells\n.\nmap\n(\nx\n \n=\n \nx\n*\nx\n).\nsum\n\n\n  \nval\n \nA\n \n=\n \nglobMean\n \n*\n \nsumW\n\n  \nval\n \nB\n \n=\n \nglobStdev\n \n*\n \nMath\n.\nsqrt\n((\nnumPixels\n*\nsumW2\n \n-\n \nsumW\n*\nsumW\n)\n \n/\n \n(\nnumPixels\n \n-\n \n1\n))\n\n\n  \nrdd\n.\nwithContext\n \n{\n\n    \n_\n.\nbufferTiles\n(\nweightMatrix\n.\nextent\n)\n\n      \n.\nmapValues\n \n{\n \ntileWithCtx\n \n=\n\n        \ntileWithCtx\n.\ntile\n\n          \n.\nfocalSum\n(\nweightMatrix\n,\n \nSome\n(\ntileWithCtx\n.\ntargetArea\n))\n \n// focal op.\n\n          \n.\nmapDouble\n \n{\n \nx\n \n=\n \n(\nx\n \n-\n \nA\n)\n \n/\n \nB\n \n}\n \n// local op.\n\n      \n}\n\n  \n}\n\n\n}\n\n\n\n\n\nLet's assume, we already have \nlayerReader\n, \nsrcLayerId\n, \nkernelRadius\n, \nglobMean\n, \nglobStdev\n and \nnumPixels\n:\n\n\nval\n \nqueryResult\n:\n \nSpatialRDD\n \n=\n\n  \nlayerReader\n.\nread\n[\nSpatialKey\n, \nTile\n, \nTileLayerMetadata\n[\nSpatialKey\n]](\nsrcLayerId\n)\n\n\n\n// here, we use a circular kernel as a weight matrix\n\n\nval\n \nweightMatrix\n \n=\n \nKernel\n.\ncircle\n(\nkernelRadius\n,\n\n                                 \nqueryResult\n.\nmetadata\n.\ncellwidth\n,\n\n                                 \nkernelRadius\n)\n\n\n\nval\n \noutRdd\n \n=\n \ngetisord\n(\nqueryResult\n,\n \nweightMatrix\n,\n \nglobMean\n,\n \nglobStdev\n,\n \nnumPixels\n)\n\n\n\nYou can not further process \noutRdd\n or store it as a new layer.", 
            "title": "Hotspot analysis (using G*)"
        }, 
        {
            "location": "/demos/hotspot_analysis/#hotspot-analysis-using-g", 
            "text": "", 
            "title": "Hotspot analysis (using G*)"
        }, 
        {
            "location": "/demos/hotspot_analysis/#problem-definition", 
            "text": "We have a map, in this case a map of land surface temperatures  We want to find hotspots, i.e., areas on map that are  significantly different from their surrounding area .  We want to use Getis-Ord G* statistic as the tool for finding the hotspots  see section  Standards Getis-ord    We want to parallelize the computation in our Spark cluster.  see section  Rasterized Getis-ord", 
            "title": "Problem definition"
        }, 
        {
            "location": "/demos/hotspot_analysis/#hotspot-analysis-using-geotrellis", 
            "text": "In this section we show a simplified version of the hotspot analysis.\nWe use the  Geotrellis  library to achieve the paralleization.\nSome assumptions are:   we use 2-dimenational data (only the spatial part without the time component)  we store our data as a distributed raster (in geotrellis catalog)  our hotspot analysis uses the standard G* with variable window   First, we need to express the G* formula in terms of the map algebra operations.", 
            "title": "Hotspot analysis using geotrellis"
        }, 
        {
            "location": "/demos/hotspot_analysis/#scala-code-snippets", 
            "text": "// typical type definition used by geotrellis  type   SpatialRDD   =   RDD [( SpatialKey ,  Tile )] \n                   with   Metadata [ TileLayerMetadata [ SpatialKey ]]  def   getisord ( rdd :   SpatialRDD ,   weightMatrix :   Kernel , \n              globMean : Double ,   globStdev : Double ,   numPixels : Long ) :   SpatialRDD   =   { \n\n   val   wcells   =   weightMatrix . tile . toArrayDouble \n   val   sumW   =   wcells . sum \n   val   sumW2   =   wcells . map ( x   =   x * x ). sum \n\n   val   A   =   globMean   *   sumW \n   val   B   =   globStdev   *   Math . sqrt (( numPixels * sumW2   -   sumW * sumW )   /   ( numPixels   -   1 )) \n\n   rdd . withContext   { \n     _ . bufferTiles ( weightMatrix . extent ) \n       . mapValues   {   tileWithCtx   = \n         tileWithCtx . tile \n           . focalSum ( weightMatrix ,   Some ( tileWithCtx . targetArea ))   // focal op. \n           . mapDouble   {   x   =   ( x   -   A )   /   B   }   // local op. \n       } \n   }  }   Let's assume, we already have  layerReader ,  srcLayerId ,  kernelRadius ,  globMean ,  globStdev  and  numPixels :  val   queryResult :   SpatialRDD   = \n   layerReader . read [ SpatialKey ,  Tile ,  TileLayerMetadata [ SpatialKey ]]( srcLayerId )  // here, we use a circular kernel as a weight matrix  val   weightMatrix   =   Kernel . circle ( kernelRadius , \n                                  queryResult . metadata . cellwidth , \n                                  kernelRadius )  val   outRdd   =   getisord ( queryResult ,   weightMatrix ,   globMean ,   globStdev ,   numPixels )  \nYou can not further process  outRdd  or store it as a new layer.", 
            "title": "Scala code snippets"
        }, 
        {
            "location": "/demos/invasive-spec/", 
            "text": "Responsible person for this section\n\n\n\n\nHannes M\u00fcller (LUBW)\n\n\nJohannes Kutterer (Disy)\n\n\nDaniel Seebacher (Uni Konstanz)\n\n\n\n\n\n\nInvasive species\n\n\nDrosophila Suzuki\n\n\n\n\n\u00dcberwachung des invasiven Sch\u00e4dlings Kirschessigfliege (KEF)\n\n\nDatenerhebung durch Weinbauinstitut\n\n\nProblem:\n Datenerhebung teilweise unsystematisch\n\n\n\n\nHypothesenentwicklung\n\n\nHypothesenentwicklung zur Vermehrung der KEF aufgrund biologischer Erkenntnisse\n(v.a. abh\u00e4ngig von Umgebungstemperatur und Vegetation)\n\n\nEntwicklung einer Vektordatenpipeline in BigGIS\n\n\n\n\nDatenquelle \nwww.vitimeteo.de\n\n\nDatensammlung (kafka)\n\n\nProzessierung (flink)\n\n\nVisualisierung (Uni-Konstanz)\n\n\n\n\nDrosophigator Prototype for the Visual Analysis of Spatio-Temporal Event Predictions\n\n\n\n\nInvestigating the Spread Dynamics of invasive species\n\n\nVDS and now journal extension\n\n\nEnsemble-Based Classification of Infested Areas\n\n\nVisual Analysis \n\n\nDemonstration and Evaluation at the 6\nth\n workshop of the working group D Suzukii in Bad Kreuznach\n\n\n\n\nResults\n\n\n\n\nH\u00e4ufiges Auftreten der KEF\n\n\nN\u00e4he zu Wald -\n Refugium f\u00fcr Kirschessigfliege zum \u00dcberleben bei widrigen Wetterbedingungen\n\n\n\n\nRelated Scenarios\n\n\n\n\nEnvironment", 
            "title": "Invasive species"
        }, 
        {
            "location": "/demos/invasive-spec/#invasive-species", 
            "text": "", 
            "title": "Invasive species"
        }, 
        {
            "location": "/demos/invasive-spec/#drosophila-suzuki", 
            "text": "\u00dcberwachung des invasiven Sch\u00e4dlings Kirschessigfliege (KEF)  Datenerhebung durch Weinbauinstitut  Problem:  Datenerhebung teilweise unsystematisch", 
            "title": "Drosophila Suzuki"
        }, 
        {
            "location": "/demos/invasive-spec/#hypothesenentwicklung", 
            "text": "Hypothesenentwicklung zur Vermehrung der KEF aufgrund biologischer Erkenntnisse\n(v.a. abh\u00e4ngig von Umgebungstemperatur und Vegetation)", 
            "title": "Hypothesenentwicklung"
        }, 
        {
            "location": "/demos/invasive-spec/#entwicklung-einer-vektordatenpipeline-in-biggis", 
            "text": "Datenquelle  www.vitimeteo.de  Datensammlung (kafka)  Prozessierung (flink)  Visualisierung (Uni-Konstanz)", 
            "title": "Entwicklung einer Vektordatenpipeline in BigGIS"
        }, 
        {
            "location": "/demos/invasive-spec/#drosophigator-prototype-for-the-visual-analysis-of-spatio-temporal-event-predictions", 
            "text": "Investigating the Spread Dynamics of invasive species  VDS and now journal extension  Ensemble-Based Classification of Infested Areas  Visual Analysis   Demonstration and Evaluation at the 6 th  workshop of the working group D Suzukii in Bad Kreuznach", 
            "title": "Drosophigator Prototype for the Visual Analysis of Spatio-Temporal Event Predictions"
        }, 
        {
            "location": "/demos/invasive-spec/#results", 
            "text": "H\u00e4ufiges Auftreten der KEF  N\u00e4he zu Wald -  Refugium f\u00fcr Kirschessigfliege zum \u00dcberleben bei widrigen Wetterbedingungen", 
            "title": "Results"
        }, 
        {
            "location": "/demos/invasive-spec/#related-scenarios", 
            "text": "Environment", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/demos/landuse/", 
            "text": "Landuse classification\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/biggis-landuse\n\n\n\n\nProblem definition\n\n\n\n\nWe have the following datasets:\n\n\nAn existing Landuse vector dataset\n\n\nOrthorectified Aerial images (Digital Ortho Photos = DOP) \n\n\nSatellite images (SAT), e.g. Sentinel2 10m\n\n\n\n\n\n\nWe want to select / extract landcover classes from landuse classes.\n\n\nWe want to use Support Vector Machines (or other Machine Learning Classifiers) for classifying landcover classes.\n\n\nsee section \nSupport Vector Machines\n\n\n\n\n\n\nWe want to support Multiple classes using a One versus All (OvA) strategy or One vs. Rest.\n\n\nsee section \nOne vs. Rest\n\n\n\n\n\n\nWe want to parallelize the computation in our Spark cluster using \nGeotrellis\n for data loading and export.\n\n\n\n\n\n\nResponsible person for this section\n\n\nAdrian Klink (EFTAS)\n\n\n\n\n\n\nTodo\n\n\n\n\nDescribe the idea\n\n\nMaybe add some geotrellis examples\n\n\n\n\n\n\nClassification of Aerial Images according to Land Use Classes (using Land Cover Classes as intermediate)\n\n\nTools\n\n\n\n\nMachine Learning\n\n\nTraining: Multiclass SVM\n\n\nGeotrellis\n\n\n\n\nScala code snippets\n\n\ntype\n \nSpatialMultibandRDD\n \n=\n \nRDD\n[(\nSpatialKey\n, \nMultibandTile\n)]\n \nwith\n \nMetadata\n[\nTileLayerMetadata\n[\nSpatialKey\n]]\n\n\n\n// reading from Hadoop Layer (HDFS)\n\n\nval\n \nrdd\n \n:\n \nSpatialMultibandRDD\n \n=\n \nbiggis\n.\nlanduse\n.\napi\n.\nreadRddFromLayer\n(\nLayerId\n(\nlayerName\n,\n \nzoom\n))\n\n\n\n// writing to Hadoop Layer (HDFS)\n\n\nbiggis\n.\nlanduse\n.\napi\n.\nwriteRddToLayer\n(\nrdd\n,\n \nLayerId\n(\nlayerName\n,\n \nzoom\n))\n\n\n\n\n\nExample\n\n\n\n\nClassification of Aerial Images May-Aug 2016\n\n\nLayerstacking: Aerial Images + Satellite images (IR, Resolution 2m)\n\n\nTraining of a Multiclass SVM with manually selected training data (classified image tiles)\n\n\n\n\nFurther Steps\n\n\n\n\nAdding additional Layers, e.g.\n\n\nTerrain Height\n\n\nHomogeneity of Texture\n\n\nUsing Other Classififiers\n\n\n\n\nRelated Scenarios\n\n\n\n\nEnvironment\n\n\nSmart City", 
            "title": "Landuse classification"
        }, 
        {
            "location": "/demos/landuse/#landuse-classification", 
            "text": "Note  Related repository is  https://github.com/biggis-project/biggis-landuse", 
            "title": "Landuse classification"
        }, 
        {
            "location": "/demos/landuse/#problem-definition", 
            "text": "We have the following datasets:  An existing Landuse vector dataset  Orthorectified Aerial images (Digital Ortho Photos = DOP)   Satellite images (SAT), e.g. Sentinel2 10m    We want to select / extract landcover classes from landuse classes.  We want to use Support Vector Machines (or other Machine Learning Classifiers) for classifying landcover classes.  see section  Support Vector Machines    We want to support Multiple classes using a One versus All (OvA) strategy or One vs. Rest.  see section  One vs. Rest    We want to parallelize the computation in our Spark cluster using  Geotrellis  for data loading and export.    Responsible person for this section  Adrian Klink (EFTAS)    Todo   Describe the idea  Maybe add some geotrellis examples", 
            "title": "Problem definition"
        }, 
        {
            "location": "/demos/landuse/#classification-of-aerial-images-according-to-land-use-classes-using-land-cover-classes-as-intermediate", 
            "text": "", 
            "title": "Classification of Aerial Images according to Land Use Classes (using Land Cover Classes as intermediate)"
        }, 
        {
            "location": "/demos/landuse/#tools", 
            "text": "Machine Learning  Training: Multiclass SVM  Geotrellis", 
            "title": "Tools"
        }, 
        {
            "location": "/demos/landuse/#scala-code-snippets", 
            "text": "type   SpatialMultibandRDD   =   RDD [( SpatialKey ,  MultibandTile )]   with   Metadata [ TileLayerMetadata [ SpatialKey ]]  // reading from Hadoop Layer (HDFS)  val   rdd   :   SpatialMultibandRDD   =   biggis . landuse . api . readRddFromLayer ( LayerId ( layerName ,   zoom ))  // writing to Hadoop Layer (HDFS)  biggis . landuse . api . writeRddToLayer ( rdd ,   LayerId ( layerName ,   zoom ))", 
            "title": "Scala code snippets"
        }, 
        {
            "location": "/demos/landuse/#example", 
            "text": "Classification of Aerial Images May-Aug 2016  Layerstacking: Aerial Images + Satellite images (IR, Resolution 2m)  Training of a Multiclass SVM with manually selected training data (classified image tiles)", 
            "title": "Example"
        }, 
        {
            "location": "/demos/landuse/#further-steps", 
            "text": "Adding additional Layers, e.g.  Terrain Height  Homogeneity of Texture  Using Other Classififiers", 
            "title": "Further Steps"
        }, 
        {
            "location": "/demos/landuse/#related-scenarios", 
            "text": "Environment  Smart City", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/demos/optimize-drones/", 
            "text": "Responsible person for this section\n\n\nKatharina Glock\n\n\n\n\nOptimal flight plan for drones\n\n\n\n\nTodo\n\n\n\n\ndescribe the idea\n\n\nadd some images\n\n\nadd some links to related papers\n\n\nadd links to related github repos\n\n\nadd links to related scenarios", 
            "title": "Optimal flight plan for drones"
        }, 
        {
            "location": "/demos/optimize-drones/#optimal-flight-plan-for-drones", 
            "text": "Todo   describe the idea  add some images  add some links to related papers  add links to related github repos  add links to related scenarios", 
            "title": "Optimal flight plan for drones"
        }, 
        {
            "location": "/demos/optimize-sensors/", 
            "text": "Responsible person for this section\n\n\nKatharina Glock\n\n\n\n\nPlacement of sensors under uncertainty\n\n\n\n\nTodo\n\n\n\n\ndescribe the idea\n\n\nadd some images\n\n\nadd some links to related papers\n\n\nadd links to related github repos\n\n\nadd links to related scenarios", 
            "title": "Placement of sensors under uncertainty"
        }, 
        {
            "location": "/demos/optimize-sensors/#placement-of-sensors-under-uncertainty", 
            "text": "Todo   describe the idea  add some images  add some links to related papers  add links to related github repos  add links to related scenarios", 
            "title": "Placement of sensors under uncertainty"
        }, 
        {
            "location": "/demos/Stability_of_hotspots/", 
            "text": "Responsible person for this section\n\n\nMarc Gassenschmidt\n\n\n\n\nStability of hotspots\n\n\n\n\nTodo\n\n\n\n\ncomparison of different metrics (soh, jaccard, ...)\n\n\ncomparison of different methods (gstar, focalgstar, ...)", 
            "title": "Stability of hotspots"
        }, 
        {
            "location": "/demos/Stability_of_hotspots/#stability-of-hotspots", 
            "text": "Todo   comparison of different metrics (soh, jaccard, ...)  comparison of different methods (gstar, focalgstar, ...)", 
            "title": "Stability of hotspots"
        }, 
        {
            "location": "/demos/urban-heat-islands/", 
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nUrban Heat Islands\n\n\n\n\nTodo\n\n\n\n\nTranslate to English\n\n\nadd links to related github repos\n\n\nadd some images (but not too many)\n\n\nadd links to related papers\n\n\ndescribe APIs especially from the end-users' point of view\n\n\n\n\n\n\n\n\nTemperaturinseln in Karlsruhe und anderen St\u00e4dten\n\n\nTemperaturdaten: Volunteered geographic data (z.B. wunderground.com)\n\n\n\n\nKorrelation zwischen Bereichen in verschiedenen St\u00e4dten mit \u00e4hnlichem Temperaturverlauf -\n Zugang zu Ursachen f\u00fcr Temperaturentwicklung\n\n\n\n\n\n\nVorstellung der Heat-Islands-Analyse\n\n\n\n\nWetterstationen\n\n\nTechnik: Sensebox (\nhttps://sensebox.de\n)\n\n\nBeispielstation (\nhttps://opensensemap.org/explore/58b4354fe53e0b001251119d\n)\n\n\nHotspotanalyse (SoH, Stability of Hotspots):\n\n\nAbh\u00e4ngigkeit des Auftretens von Hotspots von der Aggregationsstufe\n\n\nAusblick:\n\n\nSensorfusion in Kooperation mit SDIL (smart data innovation lab)\n\n\n\n\nRelated Scenarios\n\n\n\n\nSmart City", 
            "title": "Urban Heat Islands"
        }, 
        {
            "location": "/demos/urban-heat-islands/#urban-heat-islands", 
            "text": "Todo   Translate to English  add links to related github repos  add some images (but not too many)  add links to related papers  describe APIs especially from the end-users' point of view     Temperaturinseln in Karlsruhe und anderen St\u00e4dten  Temperaturdaten: Volunteered geographic data (z.B. wunderground.com)   Korrelation zwischen Bereichen in verschiedenen St\u00e4dten mit \u00e4hnlichem Temperaturverlauf -  Zugang zu Ursachen f\u00fcr Temperaturentwicklung    Vorstellung der Heat-Islands-Analyse   Wetterstationen  Technik: Sensebox ( https://sensebox.de )  Beispielstation ( https://opensensemap.org/explore/58b4354fe53e0b001251119d )  Hotspotanalyse (SoH, Stability of Hotspots):  Abh\u00e4ngigkeit des Auftretens von Hotspots von der Aggregationsstufe  Ausblick:  Sensorfusion in Kooperation mit SDIL (smart data innovation lab)", 
            "title": "Urban Heat Islands"
        }, 
        {
            "location": "/demos/urban-heat-islands/#related-scenarios", 
            "text": "Smart City", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/docs-howto/", 
            "text": "Documentation Howto\n\n\nWe use \nmkdocs\n for documenting the project.\nThe goal is to make the documentation process as simple as possible,\nto allow versioning and to use pull requests for text reviews.\nI should edit the docs locally, preview it in a browser and then suggest a pull request.\n\n\nThe amount of formatting elements is deliberately small.\nApart from wiki, we try to keep each page self contained and to minimize\ninterlinking between pages because it only complicates reading of the docs.\n\n\nThe documentation is written as a set of Markdown files within the \ndocs/\n directory and after deployment\navailable as a static website: \nDocs Website\n.\n\n\nBefore building the docs\n\n\nFirst of all, you need \npython\n and \npip\n to be installed.\nUsing pip, you need to install the following packages:\n\n\n\n\nmkdocs\n : Provides the executable command \nmkdocs\n.\n\n\nmkdocs-material\n : A material design theme, see also \nthis page\n.\n\n\n\n\nYou can install the packages either locally as a user into \n~/.local/\n or system-wide\n(when omitting the \n--user\n parameter).\n\n\n1\n2\npip install --user mkdocs\npip install --user mkdocs-material\n\n\n\n\n\n\n\n\nNote\n\n\nMake sure you are using \nmkdocs version 0.17.2+\n.\n\n# to upgrade use this instead\n\npip install -U --user mkdocs\npip install -U --user mkdocs-material\n\n\n\n\n\nRecommended editor\n\n\nSince we use the markdown format, you can use any plain text editor.\nHowever, we suggest to use \nIntelliJ IDEA\n\nwith the \nMarkdown Support plugin\n (both are free) which gives you:\n\n\n\n\nsyntax highlighting\n\n\npath completion of links such as image file names\n\n\nrefactoring, which is handy when renaming markdown files which are liked from other files\n\n\nfancy search\n\n\noutline of the document structure\n\n\nautomated simplified preview (which is not that important due to the mkdocs hotreload)\n\n\n\n\nHow to edit\n\n\nBefore editing the documentation, start the live-reloading docs server\nusing \nmkdocs serve\n within the project root directory.\nThen, open the page \nhttp://127.0.0.1:8000\n in your\nbrowser and watch your edits being reloaded automatically.\n\n\n1\nmkdocs serve\n\n\n\n\n\n\nINFO    -  Building documentation... \nINFO    -  Cleaning site directory \n[I 171024 15:03:51 server:283] Serving on http://127.0.0.1:8000\n[I 171024 15:03:51 handlers:60] Start watching changes\n\n\n\n\nYou can now edit the markdown documents with the \ndocs/\n directory.\n\n\n\n\nWarning\n\n\nIf you are renaming or creating new markdown files (\n*.md\n extension), you should also update the\n\npages\n configuration section in \nmkdocs.yml\n. Otherwise the update will not be reloaded in your browser.\nThis is a known limitation of mkdocs that will be addressed in future releases. Until then, there is\na small shell script that updates the \npages\n section automatically. However, it only works on Unix systems.\n\n# refreshes the pages section\n\n./fix-mkdocs-pages.sh mkdocs.yml\n\n\n\n\n\nDeployment\n\n\nUsing the command \nmkdocs gh-deploy\n we can generate a static \nDocs Website\n\nand deploy it automatically as a github page (served from \ngh-pages\n branch).\n\n\n\n\nNote\n\n\nThe newly deployed version appears after few seconds.\n\n\n\n\n\n\nWarning\n\n\nThis operation is relevant only to the administrators of the \nbiggis-docs\n repository.\nAs a regular contributor, you should only preview your local changes and create\na pull request instead of directly deploying the built docs to the gh-pages branch.\n\n\n\n\nDocumentation layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n  index.md    # The documentation homepage.\n  ...         # Other markdown pages, images and other files.\n\n\n\n\n\nFor the sake of simplicity, we use two-level hierarchy inside \ndocs/\n:\n\n\n\n\nLevel 1\n : areas (directories) that appear in the main menu.\n\n\nLevel 2\n : pages (markdown files) that appear in the left side bar.\n\n\nLevel 3\n : headings (H1, H2, ...) that appear in the table of contents on the right\n\n\n\n\n\n\nNode\n\n\nDo not use spaces in file names. Replace them with underscores \n_\n.\nThis allows for easier refactoring because spaces are transformed to \n%20\n in markdown.\n\n\n\n\nFormatting examples\n\n\nSectioning\n\n\n# Chapter\n\n## Section\n\n### Subsection\n\n\n\n\n\nFootnotes\n\n\nSee also \nhttps://squidfunk.github.io/mkdocs-material/extensions/footnotes/\n\n\nSome text with a footnote[^1]\n\n[^1]: Text of the footnote\n\n\n\n\n\nCitations, Notes\n\n\n!!! cite\n    Here comes the citation including authors, title, year, doi, url ...\n\n\n\n\n\n\n\nCite\n\n\nHere comes the citation including authors, title, year, doi, url ...\n\n\n\n\n\n\n!!! note\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n\n\n\n\n\n\n\nNote\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa.\n\n\n\n\n\n\nFor more options see \nhttps://squidfunk.github.io/mkdocs-material/extensions/admonition/\n\n\nImages and Figures\n\n\nYou can include images into the documentation in the following format:\n\n\n\n\nSVG\n (scalable vectors).\n\n\nJPG\n (photos)\n\n\nPNG\n (raster graphics)\n\n\n\n\nIn contrast to scientific papers, it is not possible to create references to numbered figures in markdown.\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#images-and-media\n\n\n![Image \nalt\n description](path/to/image.svg)\n\n\n\n\n\n\n\n\n\nNote\n\n\nWhen editing a file \npath/to/ABC.md\n, store all related images in folder \npath/to/ABC\n.\nThis way, different topics are better encapsulated.\n\n\n\n\nTables\n\n\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#tables\n\n\nFirst Header | Second Header | Third Header\n------------ | ------------- | ------------\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell\n\n\n\n\n\n\n\n\n\n\n\nFirst Header\n\n\nSecond Header\n\n\nThird Header\n\n\n\n\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\n\n\nFormulas\n\n\nFormula are generated using \nMathJax\n, which is similar to LaTeX.\nSee also this \nquick reference\n.\n\n\n$$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$\n\n\n\n\n\n\\[\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\\]\nSource Code\n\n\nCode can be displayed inline like this:\n\n\n`print 1+{variable}`\n\n\n\n\n\nOr it can be displayed in a code block with optional syntax highlighting if the language is specified.\n\n\n```python\ndef my_function():\n    \njust a test\n\n    print 8/2 \n```\n\n\n\n\n\ndef\n \nmy_function\n():\n\n    \njust a test\n\n    \nprint\n \n8\n/\n2\n \n\n\n\n\nSmart Symbols\n\n\nSee also \nhttps://facelessuser.github.io/pymdown-extensions/extensions/smartsymbols/\n\n\nSome smart symbols: --\n,  \n--, 1st, 2nd, 1/4\n\n\n\n\n\nSome smart symbols: \n,  \n, 1\nst\n, 2\nnd\n, \n\n\nSequence diagrams\n\n\n```sequence\nTitle: Example sequence diagram\nA-\nB: Sync call\nB--\nA: Sync return\nA-\nC: Another sync call\nC-\nD: Async call\nD--\nC: Async return\n```\n\n\n\n\nTitle: Example sequence diagram\nA-\nB: Sync call\nB--\nA: Sync return\nA-\nC: Another sync call\nC-\nD: Async call\nD--\nC: Async return", 
            "title": "Documentation Howto"
        }, 
        {
            "location": "/docs-howto/#documentation-howto", 
            "text": "We use  mkdocs  for documenting the project.\nThe goal is to make the documentation process as simple as possible,\nto allow versioning and to use pull requests for text reviews.\nI should edit the docs locally, preview it in a browser and then suggest a pull request.  The amount of formatting elements is deliberately small.\nApart from wiki, we try to keep each page self contained and to minimize\ninterlinking between pages because it only complicates reading of the docs.  The documentation is written as a set of Markdown files within the  docs/  directory and after deployment\navailable as a static website:  Docs Website .", 
            "title": "Documentation Howto"
        }, 
        {
            "location": "/docs-howto/#before-building-the-docs", 
            "text": "First of all, you need  python  and  pip  to be installed.\nUsing pip, you need to install the following packages:   mkdocs  : Provides the executable command  mkdocs .  mkdocs-material  : A material design theme, see also  this page .   You can install the packages either locally as a user into  ~/.local/  or system-wide\n(when omitting the  --user  parameter).  1\n2 pip install --user mkdocs\npip install --user mkdocs-material    Note  Make sure you are using  mkdocs version 0.17.2+ . # to upgrade use this instead \npip install -U --user mkdocs\npip install -U --user mkdocs-material", 
            "title": "Before building the docs"
        }, 
        {
            "location": "/docs-howto/#recommended-editor", 
            "text": "Since we use the markdown format, you can use any plain text editor.\nHowever, we suggest to use  IntelliJ IDEA \nwith the  Markdown Support plugin  (both are free) which gives you:   syntax highlighting  path completion of links such as image file names  refactoring, which is handy when renaming markdown files which are liked from other files  fancy search  outline of the document structure  automated simplified preview (which is not that important due to the mkdocs hotreload)", 
            "title": "Recommended editor"
        }, 
        {
            "location": "/docs-howto/#how-to-edit", 
            "text": "Before editing the documentation, start the live-reloading docs server\nusing  mkdocs serve  within the project root directory.\nThen, open the page  http://127.0.0.1:8000  in your\nbrowser and watch your edits being reloaded automatically.  1 mkdocs serve   INFO    -  Building documentation... \nINFO    -  Cleaning site directory \n[I 171024 15:03:51 server:283] Serving on http://127.0.0.1:8000\n[I 171024 15:03:51 handlers:60] Start watching changes  You can now edit the markdown documents with the  docs/  directory.   Warning  If you are renaming or creating new markdown files ( *.md  extension), you should also update the pages  configuration section in  mkdocs.yml . Otherwise the update will not be reloaded in your browser.\nThis is a known limitation of mkdocs that will be addressed in future releases. Until then, there is\na small shell script that updates the  pages  section automatically. However, it only works on Unix systems. # refreshes the pages section \n./fix-mkdocs-pages.sh mkdocs.yml", 
            "title": "How to edit"
        }, 
        {
            "location": "/docs-howto/#deployment", 
            "text": "Using the command  mkdocs gh-deploy  we can generate a static  Docs Website \nand deploy it automatically as a github page (served from  gh-pages  branch).   Note  The newly deployed version appears after few seconds.    Warning  This operation is relevant only to the administrators of the  biggis-docs  repository.\nAs a regular contributor, you should only preview your local changes and create\na pull request instead of directly deploying the built docs to the gh-pages branch.", 
            "title": "Deployment"
        }, 
        {
            "location": "/docs-howto/#documentation-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n  index.md    # The documentation homepage.\n  ...         # Other markdown pages, images and other files.  For the sake of simplicity, we use two-level hierarchy inside  docs/ :   Level 1  : areas (directories) that appear in the main menu.  Level 2  : pages (markdown files) that appear in the left side bar.  Level 3  : headings (H1, H2, ...) that appear in the table of contents on the right    Node  Do not use spaces in file names. Replace them with underscores  _ .\nThis allows for easier refactoring because spaces are transformed to  %20  in markdown.", 
            "title": "Documentation layout"
        }, 
        {
            "location": "/docs-howto/#formatting-examples", 
            "text": "", 
            "title": "Formatting examples"
        }, 
        {
            "location": "/docs-howto/#sectioning", 
            "text": "# Chapter\n\n## Section\n\n### Subsection", 
            "title": "Sectioning"
        }, 
        {
            "location": "/docs-howto/#footnotes", 
            "text": "See also  https://squidfunk.github.io/mkdocs-material/extensions/footnotes/  Some text with a footnote[^1]\n\n[^1]: Text of the footnote", 
            "title": "Footnotes"
        }, 
        {
            "location": "/docs-howto/#citations-notes", 
            "text": "!!! cite\n    Here comes the citation including authors, title, year, doi, url ...   Cite  Here comes the citation including authors, title, year, doi, url ...    !!! note\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.   Note  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa.    For more options see  https://squidfunk.github.io/mkdocs-material/extensions/admonition/", 
            "title": "Citations, Notes"
        }, 
        {
            "location": "/docs-howto/#images-and-figures", 
            "text": "You can include images into the documentation in the following format:   SVG  (scalable vectors).  JPG  (photos)  PNG  (raster graphics)   In contrast to scientific papers, it is not possible to create references to numbered figures in markdown.\nSee also  http://www.mkdocs.org/user-guide/writing-your-docs/#images-and-media  ![Image  alt  description](path/to/image.svg)    Note  When editing a file  path/to/ABC.md , store all related images in folder  path/to/ABC .\nThis way, different topics are better encapsulated.", 
            "title": "Images and Figures"
        }, 
        {
            "location": "/docs-howto/#tables", 
            "text": "See also  http://www.mkdocs.org/user-guide/writing-your-docs/#tables  First Header | Second Header | Third Header\n------------ | ------------- | ------------\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell     First Header  Second Header  Third Header      Content Cell  Content Cell  Content Cell    Content Cell  Content Cell  Content Cell", 
            "title": "Tables"
        }, 
        {
            "location": "/docs-howto/#formulas", 
            "text": "Formula are generated using  MathJax , which is similar to LaTeX.\nSee also this  quick reference .  $$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$  \\[\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\\]", 
            "title": "Formulas"
        }, 
        {
            "location": "/docs-howto/#source-code", 
            "text": "Code can be displayed inline like this:  `print 1+{variable}`  Or it can be displayed in a code block with optional syntax highlighting if the language is specified.  ```python\ndef my_function():\n     just a test \n    print 8/2 \n```  def   my_function (): \n     just a test \n     print   8 / 2", 
            "title": "Source Code"
        }, 
        {
            "location": "/docs-howto/#smart-symbols", 
            "text": "See also  https://facelessuser.github.io/pymdown-extensions/extensions/smartsymbols/  Some smart symbols: -- ,   --, 1st, 2nd, 1/4  Some smart symbols:  ,   , 1 st , 2 nd ,", 
            "title": "Smart Symbols"
        }, 
        {
            "location": "/docs-howto/#sequence-diagrams", 
            "text": "```sequence\nTitle: Example sequence diagram\nA- B: Sync call\nB-- A: Sync return\nA- C: Another sync call\nC- D: Async call\nD-- C: Async return\n```  Title: Example sequence diagram\nA- B: Sync call\nB-- A: Sync return\nA- C: Another sync call\nC- D: Async call\nD-- C: Async return", 
            "title": "Sequence diagrams"
        }, 
        {
            "location": "/methods/", 
            "text": "Methods\n\n\nThis section contains a \nlist of methods\n that serve as a \ntheoretical background\n\nfor the other sections inside the documentation, especially for the \nDemos\n.\n\n\nSome methods depend on each other which is reflected in their ordering.\n\n\n\n\nNote\n\n\nYou can use the \nNext\n and \nPrevious\n buttons on the bottom of each page to navigate.\n\n\n\n\nMethods not yet included into the list\n\n\n\n\nlanduse classification -\n add new Support Vector Machines, One versus All\n\n\n...\n\n\n...", 
            "title": "Methods"
        }, 
        {
            "location": "/methods/#methods", 
            "text": "This section contains a  list of methods  that serve as a  theoretical background \nfor the other sections inside the documentation, especially for the  Demos .  Some methods depend on each other which is reflected in their ordering.   Note  You can use the  Next  and  Previous  buttons on the bottom of each page to navigate.", 
            "title": "Methods"
        }, 
        {
            "location": "/methods/#methods-not-yet-included-into-the-list", 
            "text": "landuse classification -  add new Support Vector Machines, One versus All  ...  ...", 
            "title": "Methods not yet included into the list"
        }, 
        {
            "location": "/methods/focal_getis_ord/", 
            "text": "Focal Getis-ord\n\n\n\n\nTodo\n\n\nJulian Bruns : definition of Focal G* using points \n\n\n\n\nThe Focal Getis-Ord \n\\(G^*_i\\)\n statistic differs from the \nStandard Getis-ord\n ...\n\n\n\\[\n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\\]\nwhere:\n\n\n\n\nTODO\n\n\nTODO", 
            "title": "Focal Getis-ord"
        }, 
        {
            "location": "/methods/focal_getis_ord/#focal-getis-ord", 
            "text": "Todo  Julian Bruns : definition of Focal G* using points    The Focal Getis-Ord  \\(G^*_i\\)  statistic differs from the  Standard Getis-ord  ...  \\[\n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\\] where:   TODO  TODO", 
            "title": "Focal Getis-ord"
        }, 
        {
            "location": "/methods/focal_getis_ord_raster/", 
            "text": "Foal Getis-ord on rasters\n\n\nThe rasterized Focal Getis-Ord formula looks as follows:\n\n\n\\[\nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\\]\nwhere:\n\n\n\n\n\\(R\\)\n is the input raster.\n\n\n\\(W\\)\n is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g. \n\\(5 \\times 5\\)\n, \n\\(31 \\times 31\\)\n ...\n\n\n\\(N\\)\n represents the focal count of pixels TODO (there can be NA values)\n\n\n\\(M\\)\n represents the focal mean TODO.\n\n\n\\(S\\)\n represents the focal standard deviation TODO.\n\n\n\n\nIt can be seen that the formula can be nicely refactored into:\n\n\n\n\nTODO", 
            "title": "Foal Getis-ord on rasters"
        }, 
        {
            "location": "/methods/focal_getis_ord_raster/#foal-getis-ord-on-rasters", 
            "text": "The rasterized Focal Getis-Ord formula looks as follows:  \\[\nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\\] where:   \\(R\\)  is the input raster.  \\(W\\)  is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g.  \\(5 \\times 5\\) ,  \\(31 \\times 31\\)  ...  \\(N\\)  represents the focal count of pixels TODO (there can be NA values)  \\(M\\)  represents the focal mean TODO.  \\(S\\)  represents the focal standard deviation TODO.   It can be seen that the formula can be nicely refactored into:   TODO", 
            "title": "Foal Getis-ord on rasters"
        }, 
        {
            "location": "/methods/getis_ord/", 
            "text": "Standards Getis-Ord G*\n\n\nThe standard definition of Getis-Ord \n\\(G^*_i\\)\n statistic assumes a study area with \n\\(n\\)\n points with measurements\n\n\\(X = [x_1, \\ldots, x_n]\\)\n. Moreover, it assumes weights \n\\(w_{i,j}\\)\n to be defined between all pairs of points \n\\(i\\)\n\nand \n\\(j\\)\n (for all \n\\(i,j \\in \\{ 1, \\ldots, n\\}\\)\n). The formula to compute \n\\(G^*_i\\)\n at a given point \n\\(i\\)\n is then:\n\n\n\\[\n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\\]\nwhere:\n\n\n\n\n\\(\\bar{X}\\)\n is the mean of all measurements,\n\n\n\\(S\\)\n is the standard deviation of all measurements.\n\n\n\n\nAs it is known, this statistic creates a z-score, which denotes the significance of an area in relation\nto its surrounding areas.\n\n\n\n\nNote\n\n\nFor \n\\(x \\in X\\)\n, the \n\\(zscore(x) = \\frac{x - mean(X)}{stdev(X)}\\)\n\n\n\n\n\n\nTodo\n\n\nJulian Bruns: Add references to papers", 
            "title": "Standards Getis-Ord G*"
        }, 
        {
            "location": "/methods/getis_ord/#standards-getis-ord-g", 
            "text": "The standard definition of Getis-Ord  \\(G^*_i\\)  statistic assumes a study area with  \\(n\\)  points with measurements \\(X = [x_1, \\ldots, x_n]\\) . Moreover, it assumes weights  \\(w_{i,j}\\)  to be defined between all pairs of points  \\(i\\) \nand  \\(j\\)  (for all  \\(i,j \\in \\{ 1, \\ldots, n\\}\\) ). The formula to compute  \\(G^*_i\\)  at a given point  \\(i\\)  is then:  \\[\n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\\] where:   \\(\\bar{X}\\)  is the mean of all measurements,  \\(S\\)  is the standard deviation of all measurements.   As it is known, this statistic creates a z-score, which denotes the significance of an area in relation\nto its surrounding areas.   Note  For  \\(x \\in X\\) , the  \\(zscore(x) = \\frac{x - mean(X)}{stdev(X)}\\)    Todo  Julian Bruns: Add references to papers", 
            "title": "Standards Getis-Ord G*"
        }, 
        {
            "location": "/methods/getis_ord_raster/", 
            "text": "Getis-ord G* on rasters\n\n\nThe \nStandard Getis-ord\n is defined on individual points (vector data).\nIn many situations, we want to compute \n\\(G^*_i\\)\n in a raster rather than in a point cloud.\nThis way, the computation can be expressed using map algebra operations and nicely parallelized in frameworks \nsuch as \nGeotrellis\n.\n\n\nThe rasterized Getis-Ord formula looks as follows:\n\n\n\\[\nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\\]\nwhere:\n\n\n\n\n\\(R\\)\n is the input raster.\n\n\n\\(W\\)\n is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g. \n\\(5 \\times 5\\)\n, \n\\(31 \\times 31\\)\n ...\n\n\n\\(N\\)\n represents the number of all pixels in \n\\(R\\)\n (because there can be NA values)\n\n\n\\(M\\)\n represents the global mean of \n\\(R\\)\n.\n\n\n\\(S\\)\n represents the global standard deviation of all pixels in \n\\(R\\)\n.\n\n\n\n\nIt can be seen that the formula can be nicely refactored into:\n\n\n\n\nOne \nglobal operation\n that computes \n\\(N\\)\n, \n\\(M\\)\n, \n\\(S\\)\n. These values are usually available because they\n  were pre-computed when the raster layer has been ingested into the catalog.\n\n\nOne \nfocal operation\n \n\\(R{\\stackrel{\\mathtt{sum}}{\\circ}}W\\)\n - the convolution of raster \n\\(R\\)\n with\n  the weight matrix \n\\(W\\)\n.\n\n\nOne \nlocal operation\n that puts all components toghether for each pixel in \n\\(R\\)\n.", 
            "title": "Getis-ord G* on rasters"
        }, 
        {
            "location": "/methods/getis_ord_raster/#getis-ord-g-on-rasters", 
            "text": "The  Standard Getis-ord  is defined on individual points (vector data).\nIn many situations, we want to compute  \\(G^*_i\\)  in a raster rather than in a point cloud.\nThis way, the computation can be expressed using map algebra operations and nicely parallelized in frameworks \nsuch as  Geotrellis .  The rasterized Getis-Ord formula looks as follows:  \\[\nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\\] where:   \\(R\\)  is the input raster.  \\(W\\)  is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g.  \\(5 \\times 5\\) ,  \\(31 \\times 31\\)  ...  \\(N\\)  represents the number of all pixels in  \\(R\\)  (because there can be NA values)  \\(M\\)  represents the global mean of  \\(R\\) .  \\(S\\)  represents the global standard deviation of all pixels in  \\(R\\) .   It can be seen that the formula can be nicely refactored into:   One  global operation  that computes  \\(N\\) ,  \\(M\\) ,  \\(S\\) . These values are usually available because they\n  were pre-computed when the raster layer has been ingested into the catalog.  One  focal operation   \\(R{\\stackrel{\\mathtt{sum}}{\\circ}}W\\)  - the convolution of raster  \\(R\\)  with\n  the weight matrix  \\(W\\) .  One  local operation  that puts all components toghether for each pixel in  \\(R\\) .", 
            "title": "Getis-ord G* on rasters"
        }, 
        {
            "location": "/methods/one_vs_rest/", 
            "text": "One vs. Rest\n\n\nThe One-versus-Rest (or One-versus-All = OvA) Strategy is a Method to use Binary (Two-Class) Classifiers (such as \nSupport Vector Machines\n ) for classifying multiple (more than two) Classes.\n\n\n\n\nNote\n\n\n\n\nhttps://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest\n\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#one-vs-rest-classifier-aka-one-vs-all\n\n\n\n\n\n\nOne-vs.-rest\n\n\nInputs:\n\n\n    L, a learner (training algorithm for binary classifiers)\n    samples X\n    labels y where yi \u2208 {1, \u2026 K} is the label for the sample Xi\n\n\n\n\n\nOutput:\n\n\n    a list of classifiers fk for k \u2208 {1, \u2026, K}\n\n\n\n\n\nProcedure:\n\n\n    For each k in {1, \u2026, K}\n        Construct a new label vector z where zi = 1 if yi = k and zi = 0 otherwise\n        Apply L to X, z to obtain fk\n\n\n\n\n\nMaking decisions means applying all classifiers to an unseen sample and\npredicting the label for which the corresponding classifier reports the\nhighest confidence score:\n\n\n\\[\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\arg\\!\\max}\\; f_k(x)\\]\n\n\nTodo\n\n\nAdrian Klink: Add references, optimize description", 
            "title": "One vs. Rest"
        }, 
        {
            "location": "/methods/one_vs_rest/#one-vs-rest", 
            "text": "The One-versus-Rest (or One-versus-All = OvA) Strategy is a Method to use Binary (Two-Class) Classifiers (such as  Support Vector Machines  ) for classifying multiple (more than two) Classes.   Note   https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest  https://spark.apache.org/docs/latest/ml-classification-regression.html#one-vs-rest-classifier-aka-one-vs-all", 
            "title": "One vs. Rest"
        }, 
        {
            "location": "/methods/one_vs_rest/#one-vs-rest_1", 
            "text": "", 
            "title": "One-vs.-rest"
        }, 
        {
            "location": "/methods/one_vs_rest/#inputs", 
            "text": "L, a learner (training algorithm for binary classifiers)\n    samples X\n    labels y where yi \u2208 {1, \u2026 K} is the label for the sample Xi", 
            "title": "Inputs:"
        }, 
        {
            "location": "/methods/one_vs_rest/#output", 
            "text": "a list of classifiers fk for k \u2208 {1, \u2026, K}", 
            "title": "Output:"
        }, 
        {
            "location": "/methods/one_vs_rest/#procedure", 
            "text": "For each k in {1, \u2026, K}\n        Construct a new label vector z where zi = 1 if yi = k and zi = 0 otherwise\n        Apply L to X, z to obtain fk  Making decisions means applying all classifiers to an unseen sample and\npredicting the label for which the corresponding classifier reports the\nhighest confidence score:  \\[\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\arg\\!\\max}\\; f_k(x)\\]  Todo  Adrian Klink: Add references, optimize description", 
            "title": "Procedure:"
        }, 
        {
            "location": "/methods/soh/", 
            "text": "Stability of hotspots\n\n\n\n\nTodo\n\n\nIntro text\n\n\n\n\nRelated demos:\n- \nstability of hotspots", 
            "title": "Stability of hotspots"
        }, 
        {
            "location": "/methods/soh/#stability-of-hotspots", 
            "text": "Todo  Intro text   Related demos:\n-  stability of hotspots", 
            "title": "Stability of hotspots"
        }, 
        {
            "location": "/methods/support_vector_machine/", 
            "text": "Support Vector Machine\n\n\nA Support Vector Machine (SVM) is a supervised learning model (machine learning) which can be used to classify image data. \nFor Parallelization we use a Linear SVM from \nApache Spark\n. Since we have more than two classes the One versus All (or \nOne vs. Rest\n ) Strategy is used.\n\n\n\n\nNote\n\n\n\n\nhttps://en.wikipedia.org/wiki/Support_vector_machine\n\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine\n\n\n\n\n\n\nLinear SVM\n\n\nWe are given a training dataset of \n\\(n\\)\n points of the form\n\n\n:   \n\\((\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)\\)\n\n\nwhere the \n\\(y_i\\)\n are either 1 or \u22121, each indicating the class to which\nthe point \n\\(\\vec{x}_i\\)\n belongs. Each \n\\(\\vec{x}_i\\)\n is a \n\\(p\\)\n-dimensional\n\nreal\n vector. We want to find the\n\"maximum-margin hyperplane\" that divides the group of points\n\n\\(\\vec{x}_i\\)\n for which \n\\(y_i=1\\)\n from the group of points for which\n\n\\(y_i=-1\\)\n, which is defined so that the distance between the hyperplane\nand the nearest point \n\\(\\vec{x}_i\\)\n from either group is maximized.\n\n\nAny \nhyperplane\n can be written as the set of\npoints \n\\(\\vec{x}\\)\n satisfying\n\n\n:   \n\\(\\vec{w}\\cdot\\vec{x} - b=0,\\,\\)\n \n\n\nwhere \n\\({\\vec{w}}\\)\n is the (not necessarily normalized) \nnormal\nvector\n to the hyperplane. This is much\nlike \nHesse normal form\n, except that\n\n\\({\\vec{w}}\\)\n is not necessarily a unit vector. The parameter\n\n\\(\\tfrac{b}{\\|\\vec{w}\\|}\\)\n determines the offset of the hyperplane from\nthe origin along the normal vector \n\\({\\vec{w}}\\)\n.\n\n\n\n\nHard-margin\n\n\nIf the training data are \nlinearly\nseparable\n, we can select two parallel\nhyperplanes that separate the two classes of data, so that the distance\nbetween them is as large as possible. The region bounded by these two\nhyperplanes is called the \\\"margin\\\", and the maximum-margin hyperplane\nis the hyperplane that lies halfway between them. These hyperplanes can\nbe described by the equations\n\n\n:   \n\\(\\vec{w}\\cdot\\vec{x} - b=1\\,\\)\n\n\nand\n\n\n:   \n\\(\\vec{w}\\cdot\\vec{x} - b=-1.\\,\\)\n\n\nGeometrically, the distance between these two hyperplanes is\n\n\\(\\tfrac{2}{\\|\\vec{w}\\|}\\)\n, so to maximize the distance between the planes\nwe want to minimize \n\\(\\|\\vec{w}\\|\\)\n. As we also have to prevent data\npoints from falling into the margin, we add the following constraint:\nfor each \n\\(i\\)\n either\n\n\n:   \n\\(\\vec{w}\\cdot\\vec{x}_i - b \\ge 1,\\)\n if \n\\(y_i = 1\\)\n\n\nor\n\n\n:   \n\\(\\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\)\n if \n\\(y_i = -1.\\)\n\n\nThese constraints state that each data point must lie on the correct\nside of the margin.\n\n\nThis can be rewritten as:\n\n\n:   \n\\(y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)\\)\n\n\nWe can put this together to get the optimization problem:\n\n\n:   \\\"Minimize \n\\(\\|\\vec{w}\\|\\)\n subject to\n    \n\\(y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1,\\)\n for \n\\(i = 1,\\,\\ldots,\\,n\\)\n\\\"\n\n\nThe \n\\(\\vec w\\)\n and \n\\(b\\)\n that solve this problem determine our classifier,\n\n\\(\\vec{x} \\mapsto \\sgn(\\vec{w} \\cdot \\vec{x} - b)\\)\n.\n\n\nAn easy-to-see but important consequence of this geometric description\nis that the max-margin hyperplane is completely determined by those\n\n\\(\\vec{x}_i\\)\n which lie nearest to it. These \n\\(\\vec{x}_i\\)\n are called\n\nsupport vectors.\n\n\nSoft-margin\n\n\nTo extend SVM to cases in which the data are not linearly separable, we\nintroduce the \nhinge loss\n function,\n\n\n:   \n\\(\\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right).\\)\n\n\nThis function is zero if the constraint in (1) is satisfied, in other\nwords, if \n\\(\\vec{x}_i\\)\n lies on the correct side of the margin. For data\non the wrong side of the margin, the function\\'s value is proportional\nto the distance from the margin.\n\n\nWe then wish to minimize\n\n\n:   \n\\(\\left[\\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right) \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,\\)\n\n\nwhere the parameter \n\\(\\lambda\\)\n determines the tradeoff between increasing\nthe margin-size and ensuring that the \n\\(\\vec{x}_i\\)\n lie on the correct\nside of the margin. Thus, for sufficiently small values of \n\\(\\lambda\\)\n,\nthe soft-margin SVM will behave identically to the hard-margin SVM if\nthe input data are linearly classifiable, but will still learn if a\nclassification rule is viable or not.\n\n\n\n\nTodo\n\n\nAdrian Klink: Add references, optimize description", 
            "title": "Support Vector Machine"
        }, 
        {
            "location": "/methods/support_vector_machine/#support-vector-machine", 
            "text": "A Support Vector Machine (SVM) is a supervised learning model (machine learning) which can be used to classify image data. \nFor Parallelization we use a Linear SVM from  Apache Spark . Since we have more than two classes the One versus All (or  One vs. Rest  ) Strategy is used.   Note   https://en.wikipedia.org/wiki/Support_vector_machine  https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine", 
            "title": "Support Vector Machine"
        }, 
        {
            "location": "/methods/support_vector_machine/#linear-svm", 
            "text": "We are given a training dataset of  \\(n\\)  points of the form  :    \\((\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)\\)  where the  \\(y_i\\)  are either 1 or \u22121, each indicating the class to which\nthe point  \\(\\vec{x}_i\\)  belongs. Each  \\(\\vec{x}_i\\)  is a  \\(p\\) -dimensional real  vector. We want to find the\n\"maximum-margin hyperplane\" that divides the group of points \\(\\vec{x}_i\\)  for which  \\(y_i=1\\)  from the group of points for which \\(y_i=-1\\) , which is defined so that the distance between the hyperplane\nand the nearest point  \\(\\vec{x}_i\\)  from either group is maximized.  Any  hyperplane  can be written as the set of\npoints  \\(\\vec{x}\\)  satisfying  :    \\(\\vec{w}\\cdot\\vec{x} - b=0,\\,\\)    where  \\({\\vec{w}}\\)  is the (not necessarily normalized)  normal\nvector  to the hyperplane. This is much\nlike  Hesse normal form , except that \\({\\vec{w}}\\)  is not necessarily a unit vector. The parameter \\(\\tfrac{b}{\\|\\vec{w}\\|}\\)  determines the offset of the hyperplane from\nthe origin along the normal vector  \\({\\vec{w}}\\) .", 
            "title": "Linear SVM"
        }, 
        {
            "location": "/methods/support_vector_machine/#hard-margin", 
            "text": "If the training data are  linearly\nseparable , we can select two parallel\nhyperplanes that separate the two classes of data, so that the distance\nbetween them is as large as possible. The region bounded by these two\nhyperplanes is called the \\\"margin\\\", and the maximum-margin hyperplane\nis the hyperplane that lies halfway between them. These hyperplanes can\nbe described by the equations  :    \\(\\vec{w}\\cdot\\vec{x} - b=1\\,\\)  and  :    \\(\\vec{w}\\cdot\\vec{x} - b=-1.\\,\\)  Geometrically, the distance between these two hyperplanes is \\(\\tfrac{2}{\\|\\vec{w}\\|}\\) , so to maximize the distance between the planes\nwe want to minimize  \\(\\|\\vec{w}\\|\\) . As we also have to prevent data\npoints from falling into the margin, we add the following constraint:\nfor each  \\(i\\)  either  :    \\(\\vec{w}\\cdot\\vec{x}_i - b \\ge 1,\\)  if  \\(y_i = 1\\)  or  :    \\(\\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\)  if  \\(y_i = -1.\\)  These constraints state that each data point must lie on the correct\nside of the margin.  This can be rewritten as:  :    \\(y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)\\)  We can put this together to get the optimization problem:  :   \\\"Minimize  \\(\\|\\vec{w}\\|\\)  subject to\n     \\(y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1,\\)  for  \\(i = 1,\\,\\ldots,\\,n\\) \\\"  The  \\(\\vec w\\)  and  \\(b\\)  that solve this problem determine our classifier, \\(\\vec{x} \\mapsto \\sgn(\\vec{w} \\cdot \\vec{x} - b)\\) .  An easy-to-see but important consequence of this geometric description\nis that the max-margin hyperplane is completely determined by those \\(\\vec{x}_i\\)  which lie nearest to it. These  \\(\\vec{x}_i\\)  are called support vectors.", 
            "title": "Hard-margin"
        }, 
        {
            "location": "/methods/support_vector_machine/#soft-margin", 
            "text": "To extend SVM to cases in which the data are not linearly separable, we\nintroduce the  hinge loss  function,  :    \\(\\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right).\\)  This function is zero if the constraint in (1) is satisfied, in other\nwords, if  \\(\\vec{x}_i\\)  lies on the correct side of the margin. For data\non the wrong side of the margin, the function\\'s value is proportional\nto the distance from the margin.  We then wish to minimize  :    \\(\\left[\\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right) \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,\\)  where the parameter  \\(\\lambda\\)  determines the tradeoff between increasing\nthe margin-size and ensuring that the  \\(\\vec{x}_i\\)  lie on the correct\nside of the margin. Thus, for sufficiently small values of  \\(\\lambda\\) ,\nthe soft-margin SVM will behave identically to the hard-margin SVM if\nthe input data are linearly classifiable, but will still learn if a\nclassification rule is viable or not.   Todo  Adrian Klink: Add references, optimize description", 
            "title": "Soft-margin"
        }, 
        {
            "location": "/scenarios/01_city/", 
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nSmart City", 
            "title": "Smart City"
        }, 
        {
            "location": "/scenarios/01_city/#smart-city", 
            "text": "", 
            "title": "Smart City"
        }, 
        {
            "location": "/scenarios/02_bos/", 
            "text": "Responsible person for this section\n\n\nAlexander Groeschel, Bodo Bernsdorf\n\n\n\n\nDisaster Management\n\n\n\n\nGoals\n\n\n\n\nOptimal support of the command and control team\n\n\nInformation is needed quickly (real time)\n\n\nRequirements / challenges:\n\n\nFast\n\n\nHow can data be delivered to the consumers through slow connection lines?\n\n\nDecision makers often without IT background", 
            "title": "Disaster Management"
        }, 
        {
            "location": "/scenarios/02_bos/#disaster-management", 
            "text": "", 
            "title": "Disaster Management"
        }, 
        {
            "location": "/scenarios/02_bos/#goals", 
            "text": "Optimal support of the command and control team  Information is needed quickly (real time)  Requirements / challenges:  Fast  How can data be delivered to the consumers through slow connection lines?  Decision makers often without IT background", 
            "title": "Goals"
        }, 
        {
            "location": "/scenarios/03_env/", 
            "text": "Responsible person for this section\n\n\nJohannes Kutterer\n\n\n\n\nEnvironment", 
            "title": "Environment"
        }, 
        {
            "location": "/scenarios/03_env/#environment", 
            "text": "", 
            "title": "Environment"
        }
    ]
}