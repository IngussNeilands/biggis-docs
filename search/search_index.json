{
    "docs": [
        {
            "location": "/", 
            "text": "About BigGIS\n\n\nBigGIS is a new generation of GIS that supports decision making in multiple\nscenarios which require processing of large and heterogeneous data sets.\n\n\nThe novelty lies in an integrated analytical approach to spatio-temporal\ndata, that are unstructured and from unreliable sources. The system provides\npredictive, prescriptive and visual tool integrated in a common analytical\npipeline.\n\n\n\n\n\n\n\n\nSmart City\n\n\nEnvironmental Management\n\n\nDisaster Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  #scenlist img {\n    height:32px;\n    vertical-align:middle\n  }\n\n\n\n\nThe project is evaluated on three scenarios:\n\n\n  \n Smart City\n\n  : urban heat islands, particulate matter\n  \n\n  \n Environmental management\n\n  : health threatening animals and plants\n  \n\n  \n Disaster control, civil protection\n\n  : air pollution, toxic chemicals\n\n\n\n\n\n\n\nWhy BigGIS?\n\n\nCurrent GIS solution are mostly tackling big data related requirements in\nterms of data volume or data velocity. In the era of cloud computing,\nleveraging cloud-based resources is a widely adopted pattern. In addition,\nwith the advent of big data analytics, performing massively parallel\nanalytical tasks on large-scale data at rest or data in motion is as well\nbecoming a feasible approach shaping the design of today\u2019s GIS. Although\nscaling out enables GIS to tackle the aforementioned big data induced\nrequirements, there are still two major open issues. Firstly, dealing with\nvarying data types across multiple data sources (variety) lead to data and\nschema heterogeneity, e.g., to describe locations such as addresses, relative\nspatial relationships or different coordinates reference systems. Secondly,\nmodeling the inherent uncertainties in data (veracity), e.g., real-world\nnoise and erroneous values due to the nature of the data collecting process.\nBoth being crucial tasks in data management and analytics that directly\naffect the information retrieval and decision-making quality and moreover\nthe generated knowledge on human-side (value). By leveraging the the\ncontinuous refinement model, we present a holistic approach that explicitly\ndeals with all big data dimensions. By integrating the user in the\nprocess, computers can learn from the cognitive and perceptive skills of\nhuman analysis to create hidden connections between data and the problem\ndomain. This helps to decrease the noise and uncertainty and allows to\nbuild up trust in the analysis results on user side which will eventually\nlead to an increasing likelihood of relevant findings and generated\nknowledge.\n\n\nContact and Support\n\n\n\n\n\n\n\n\nRole\n\n\nName\n\n\nE-mail\n\n\n\n\n\n\n\n\n\n\nContact person\n\n\nProf. Dr. Thomas Setzer\n\n\n\n\n\n\n\n\nProject coordination\n\n\nDr. Viliam Simko", 
            "title": "About BigGIS"
        }, 
        {
            "location": "/#about-biggis", 
            "text": "BigGIS is a new generation of GIS that supports decision making in multiple\nscenarios which require processing of large and heterogeneous data sets.  The novelty lies in an integrated analytical approach to spatio-temporal\ndata, that are unstructured and from unreliable sources. The system provides\npredictive, prescriptive and visual tool integrated in a common analytical\npipeline.     Smart City  Environmental Management  Disaster Control            \n  #scenlist img {\n    height:32px;\n    vertical-align:middle\n  }  The project is evaluated on three scenarios: \n    Smart City \n  : urban heat islands, particulate matter\n   \n    Environmental management \n  : health threatening animals and plants\n   \n    Disaster control, civil protection \n  : air pollution, toxic chemicals", 
            "title": "About BigGIS"
        }, 
        {
            "location": "/#why-biggis", 
            "text": "Current GIS solution are mostly tackling big data related requirements in\nterms of data volume or data velocity. In the era of cloud computing,\nleveraging cloud-based resources is a widely adopted pattern. In addition,\nwith the advent of big data analytics, performing massively parallel\nanalytical tasks on large-scale data at rest or data in motion is as well\nbecoming a feasible approach shaping the design of today\u2019s GIS. Although\nscaling out enables GIS to tackle the aforementioned big data induced\nrequirements, there are still two major open issues. Firstly, dealing with\nvarying data types across multiple data sources (variety) lead to data and\nschema heterogeneity, e.g., to describe locations such as addresses, relative\nspatial relationships or different coordinates reference systems. Secondly,\nmodeling the inherent uncertainties in data (veracity), e.g., real-world\nnoise and erroneous values due to the nature of the data collecting process.\nBoth being crucial tasks in data management and analytics that directly\naffect the information retrieval and decision-making quality and moreover\nthe generated knowledge on human-side (value). By leveraging the the\ncontinuous refinement model, we present a holistic approach that explicitly\ndeals with all big data dimensions. By integrating the user in the\nprocess, computers can learn from the cognitive and perceptive skills of\nhuman analysis to create hidden connections between data and the problem\ndomain. This helps to decrease the noise and uncertainty and allows to\nbuild up trust in the analysis results on user side which will eventually\nlead to an increasing likelihood of relevant findings and generated\nknowledge.", 
            "title": "Why BigGIS?"
        }, 
        {
            "location": "/#contact-and-support", 
            "text": "Role  Name  E-mail      Contact person  Prof. Dr. Thomas Setzer     Project coordination  Dr. Viliam Simko", 
            "title": "Contact and Support"
        }, 
        {
            "location": "/architecture/", 
            "text": "Overview\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/biggis-infrastructure\n\n\n\n\n\n\nThe BigGIS architecture, as depicted in the picture above, consists of several layers that are briefly discussed in the following.\n\n\nModelling\n\n\nStreamPipes\n\ntbd.\n\n\nAnalytics\n\n\nAnalytics and processing\n\n\n\n\nApache Flink\n\n\nApache Spark\n featuring \nGeoTrellis\n\n\nOther Languages and Notebooks: R, Java, ...\ntbd.\n\n\n\n\nMiddleware \n Connectors\n\n\nData and control flow, connectors are nodes in StreamPipes that enable the user to load various data sources in Apache Kafka or to\n\ntbd.\n\n\nStorage Backends\n\n\nInternally, BigGIS uses a variety of different storage backends for designated purposes.\n\n\n\n\nHDFS\n for GeoTrellis catalog\n\n\nExasol\n for xx\n\n\nCouchDB\n for StreamPipes user and pipeline configurations\n\n\nSesame\n for semantic framework of StreamPipes\ntbd.\n\n\n\n\nContainer Management\n\n\nRunning these containers in a distributed manner requires a wide variety of technologies, that must be integrated and managed throughout their lifecycle. To easily deploy our containers, our infrastructure is designed to run on \nRancher\n as our container management platform. \ntbd.\n\n\nInfrastructure\n\n\nBigGIS infrastructure leverages \nbwCloud\n Infrastructure-as-a-Service (IaaS) offer powered by Openstack.", 
            "title": "Overview"
        }, 
        {
            "location": "/architecture/#overview", 
            "text": "Note  Related repository is  https://github.com/biggis-project/biggis-infrastructure    The BigGIS architecture, as depicted in the picture above, consists of several layers that are briefly discussed in the following.", 
            "title": "Overview"
        }, 
        {
            "location": "/architecture/#modelling", 
            "text": "StreamPipes \ntbd.", 
            "title": "Modelling"
        }, 
        {
            "location": "/architecture/#analytics", 
            "text": "Analytics and processing   Apache Flink  Apache Spark  featuring  GeoTrellis  Other Languages and Notebooks: R, Java, ...\ntbd.", 
            "title": "Analytics"
        }, 
        {
            "location": "/architecture/#middleware-connectors", 
            "text": "Data and control flow, connectors are nodes in StreamPipes that enable the user to load various data sources in Apache Kafka or to \ntbd.", 
            "title": "Middleware &amp; Connectors"
        }, 
        {
            "location": "/architecture/#storage-backends", 
            "text": "Internally, BigGIS uses a variety of different storage backends for designated purposes.   HDFS  for GeoTrellis catalog  Exasol  for xx  CouchDB  for StreamPipes user and pipeline configurations  Sesame  for semantic framework of StreamPipes\ntbd.", 
            "title": "Storage Backends"
        }, 
        {
            "location": "/architecture/#container-management", 
            "text": "Running these containers in a distributed manner requires a wide variety of technologies, that must be integrated and managed throughout their lifecycle. To easily deploy our containers, our infrastructure is designed to run on  Rancher  as our container management platform. \ntbd.", 
            "title": "Container Management"
        }, 
        {
            "location": "/architecture/#infrastructure", 
            "text": "BigGIS infrastructure leverages  bwCloud  Infrastructure-as-a-Service (IaaS) offer powered by Openstack.", 
            "title": "Infrastructure"
        }, 
        {
            "location": "/architecture/Docker_Containers/", 
            "text": "Responsible person for this section\n\n\nPatrick Wiener\n\n\n\n\nComponents\n\n\ntbd. some infos about the structure and hierarchical composition of our Docker images.\n\n\n\n\nall sources should be on github\n\n\nimages should be hosted on dockerhub\n\n\nlist of docker images that should be available:\n\n\nHDFS (should use all bwCloud resources available to BigGIS)\n\n\nKafka with Zookeeper (overview of queues needed)\n\n\nFlink\n\n\nSpark\n\n\nGeotrellis libraries (part of the Spark container?)\n\n\nAccumulo with Geomesa (or Geowave)\n\n\nStreamPipes\n\n\nExasolution\n\n\nExasolution should support Accumulo(Geomesa/Geowave) through virtual schema\n\n\nExasolution should support indexing using 2D, 3D, 4D space filling curves (lat,lon,time,elevation)\n\n\n\n\n\n\nGeo-Server\n\n\nmit Plugin f\u00fcr Accumulo\n\n\nzur Transformation von Formaten\n\n\nauch als Datenquelle f\u00fcr Cadenza", 
            "title": "Components"
        }, 
        {
            "location": "/architecture/Docker_Containers/#components", 
            "text": "tbd. some infos about the structure and hierarchical composition of our Docker images.   all sources should be on github  images should be hosted on dockerhub  list of docker images that should be available:  HDFS (should use all bwCloud resources available to BigGIS)  Kafka with Zookeeper (overview of queues needed)  Flink  Spark  Geotrellis libraries (part of the Spark container?)  Accumulo with Geomesa (or Geowave)  StreamPipes  Exasolution  Exasolution should support Accumulo(Geomesa/Geowave) through virtual schema  Exasolution should support indexing using 2D, 3D, 4D space filling curves (lat,lon,time,elevation)    Geo-Server  mit Plugin f\u00fcr Accumulo  zur Transformation von Formaten  auch als Datenquelle f\u00fcr Cadenza", 
            "title": "Components"
        }, 
        {
            "location": "/architecture/Platform_bwCloud/", 
            "text": "Responsible person for this section\n\n\nPatrick Wiener\n\n\n\n\nPlatform bwCloud\n\n\n\n\n\n\nbwCloud\n VMs should be available, latest status here: bwCloud Status\n\n\nWeb-based admin. interface - dashboard - based on Ambari", 
            "title": "Platform bwCloud"
        }, 
        {
            "location": "/architecture/Platform_bwCloud/#platform-bwcloud", 
            "text": "bwCloud  VMs should be available, latest status here: bwCloud Status  Web-based admin. interface - dashboard - based on Ambari", 
            "title": "Platform bwCloud"
        }, 
        {
            "location": "/architecture/StreamPipes/", 
            "text": "StreamPipes\n\n\n\n\nResponsible person for this section\n\n\nMatthias Frank\n\n\n\n\n\n\n\n\nOfficial Documentation\n\n\nFeatures Overview\n\n\n\n\nBigGIS extensions\n\n\nWithin the BigGIS project, we added several GIS-specific extensions to the StreamPipes platform.\n\n\n\n\n\n\nClimate data\n\n\n\n\n\n\nSenseBoxAdapter\n\n\nprovides the semantic description for the externally created SenseBox data stream\nfor the integration into StreamPipes\n\n\n\n\n\n\nSenseboxMetadataEnricher\n\n\nenriches each message in the SenseBox measurements data stream with meta-data (location, OpenSenseMap-Id)\n\n\n\n\n\n\n\n\n\n\nRaster processing using geotrellis.\n\n\n\n\n\n\nRasterDataEndlessSource\n\n\ngenerates an endless Kafka stream of rasterdata messages to easily test and debug\nother rasterdata processing components\n\n\n\n\n\n\nRasterDataAdapter\n\n\nprovides the semantic description for the RasterDataEndlessSource", 
            "title": "StreamPipes"
        }, 
        {
            "location": "/architecture/StreamPipes/#streampipes", 
            "text": "Responsible person for this section  Matthias Frank     Official Documentation  Features Overview", 
            "title": "StreamPipes"
        }, 
        {
            "location": "/architecture/StreamPipes/#biggis-extensions", 
            "text": "Within the BigGIS project, we added several GIS-specific extensions to the StreamPipes platform.    Climate data    SenseBoxAdapter  provides the semantic description for the externally created SenseBox data stream\nfor the integration into StreamPipes    SenseboxMetadataEnricher  enriches each message in the SenseBox measurements data stream with meta-data (location, OpenSenseMap-Id)      Raster processing using geotrellis.    RasterDataEndlessSource  generates an endless Kafka stream of rasterdata messages to easily test and debug\nother rasterdata processing components    RasterDataAdapter  provides the semantic description for the RasterDataEndlessSource", 
            "title": "BigGIS extensions"
        }, 
        {
            "location": "/biggis-github-repos/", 
            "text": "List of Github Repositories\n\n\n\nth a * { float:right; color: white }\n\n\n\n\n\n  \n\n    \n\n      \nName\nDescription\n\n      \n\n        \n{{item.name}}\n\n        \n{{item.description}}\n\n      \n\n    \n\n  \n\n  \n{{items}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: 'Loading list ...',\n    items_url: 'https://api.github.com/orgs/biggis-project/repos',\n  },\n  methods: {\n    async loadItems() {\n      try {\n        const json = await axios(this.items_url).then(_ => _.json())\n        this.items = json.sort(sortByDate)\n      } catch(e) {\n        this.items = e.response.data.message\n      }\n    }\n  }\n})\nvueapp.loadItems() // async load", 
            "title": "List of Github Repositories"
        }, 
        {
            "location": "/biggis-github-repos/#list-of-github-repositories", 
            "text": "th a * { float:right; color: white }  \n   \n     \n       Name Description \n       \n         {{item.name}} \n         {{item.description}} \n       \n     \n   \n   {{items}}      \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: 'Loading list ...',\n    items_url: 'https://api.github.com/orgs/biggis-project/repos',\n  },\n  methods: {\n    async loadItems() {\n      try {\n        const json = await axios(this.items_url).then(_ => _.json())\n        this.items = json.sort(sortByDate)\n      } catch(e) {\n        this.items = e.response.data.message\n      }\n    }\n  }\n})\nvueapp.loadItems() // async load", 
            "title": "List of Github Repositories"
        }, 
        {
            "location": "/biggis-papers/", 
            "text": "List of Papers\n\n\n\nth a * { float:right; color: white }\n\n\n\n\n\n  \n\n    \n\n      \n\n        \nDate\n\n        \nAuthor(s)\n\n        \n\n          Title / Link\n          \n\n            \nmode_edit\n\n          \n\n        \n\n      \n\n      \n\n        \n{{item.date}}\n\n        \n\n          \n\n            {{author}}\n,\n\n          \n\n        \n\n        \n\n          \n{{item.title}}.\n\n          \n({{item.note}})\n\n          \n\n            {{item.event.title}},\n            {{item.event.place}},\n            {{item.event.info}}\n          \n\n        \n\n        \n\n          \nopen_in_new\n\n          \ninsert_drive_file\n\n          \nopen_in_new\n\n        \n\n      \n\n    \n\n  \n\n  \nLoading list ...\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/papers.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/papers.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json());\n      this.items = json.sort(sortByDate);\n    }\n  }\n});\nvueapp.loadItems() // async load", 
            "title": "List of Papers"
        }, 
        {
            "location": "/biggis-papers/#list-of-papers", 
            "text": "th a * { float:right; color: white }  \n   \n     \n       \n         Date \n         Author(s) \n         \n          Title / Link\n           \n             mode_edit \n           \n         \n       \n       \n         {{item.date}} \n         \n           \n            {{author}} , \n           \n         \n         \n           {{item.title}}. \n           ({{item.note}}) \n           \n            {{item.event.title}},\n            {{item.event.place}},\n            {{item.event.info}}\n           \n         \n         \n           open_in_new \n           insert_drive_file \n           open_in_new \n         \n       \n     \n   \n   Loading list ...     \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/papers.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/papers.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json());\n      this.items = json.sort(sortByDate);\n    }\n  }\n});\nvueapp.loadItems() // async load", 
            "title": "List of Papers"
        }, 
        {
            "location": "/biggis-presentations/", 
            "text": "List of Presentations\n\n\n\nth a * { float:right; color: white }\n\n\n\n\n\n  \n\n    \n\n      \n\n        \nDate\n\n        \nAuthor(s)\n\n        \n\n          Title / Link\n          \n\n            \nmode_edit\n\n          \n\n        \n\n      \n\n      \n\n        \n{{item.date}}\n\n        \n\n          \n\n            {{author}}\n,\n\n          \n\n        \n\n        \n\n          \n{{item.title}}.\n\n          \n({{item.note}})\n\n        \n\n        \n\n          \nopen_in_new\n\n          \ninsert_drive_file\n\n        \n\n      \n\n    \n\n  \n\n  \nLoading list ...\n\n\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/presentations.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/presentations.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json())\n      this.items = json.sort(sortByDate)\n    }\n  }\n})\nvueapp.loadItems() // async load", 
            "title": "List of Presentations"
        }, 
        {
            "location": "/biggis-presentations/#list-of-presentations", 
            "text": "th a * { float:right; color: white }  \n   \n     \n       \n         Date \n         Author(s) \n         \n          Title / Link\n           \n             mode_edit \n           \n         \n       \n       \n         {{item.date}} \n         \n           \n            {{author}} , \n           \n         \n         \n           {{item.title}}. \n           ({{item.note}}) \n         \n         \n           open_in_new \n           insert_drive_file \n         \n       \n     \n   \n   Loading list ...     \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/presentations.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/presentations.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json())\n      this.items = json.sort(sortByDate)\n    }\n  }\n})\nvueapp.loadItems() // async load", 
            "title": "List of Presentations"
        }, 
        {
            "location": "/biggis-press/", 
            "text": "List of Press Releases\n\n\n\nth a * { float:right; color: white }\n\n\n\n\n\n  \n\n    \n\n      \n\n        \n{{item.title}},\n\n        \n{{item.title}},\n\n        {{item.date}}\n      \n\n    \n\n  \n\n  \nLoading list ...\n\n\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/press.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/press.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json())\n      this.items = json.sort(sortByDate)\n    }\n  }\n})\nvueapp.loadItems() // async load", 
            "title": "List of Press Releases"
        }, 
        {
            "location": "/biggis-press/#list-of-press-releases", 
            "text": "th a * { float:right; color: white }  \n   \n     \n       \n         {{item.title}}, \n         {{item.title}}, \n        {{item.date}}\n       \n     \n   \n   Loading list ...     \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/press.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/press.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json())\n      this.items = json.sort(sortByDate)\n    }\n  }\n})\nvueapp.loadItems() // async load", 
            "title": "List of Press Releases"
        }, 
        {
            "location": "/consortium/", 
            "text": "Project Consortium\n\n\n\n\n\n\n\nProject Partners\n\n\n\n\nFZI Forschungszentrum Informatik am KIT\n\n\nUniversit\u00e4t Konstanz\n\n\nHochschule Karlsruhe\n\n\nDisy Informationssysteme GmbH\n\n\nEXASOL AG\n\n\nEFTAS Fernerkundung Technologietransfer GmbH\n\n\nLandesanstalt f\u00fcr Umwelt Messungen und Naturschutz\n\n\n\n\nAssociated Partners\n\n\n\n\nTHW Karlsruhe\n\n\nStadt Karlsruhe", 
            "title": "Project Consortium"
        }, 
        {
            "location": "/consortium/#project-consortium", 
            "text": "", 
            "title": "Project Consortium"
        }, 
        {
            "location": "/consortium/#project-partners", 
            "text": "FZI Forschungszentrum Informatik am KIT  Universit\u00e4t Konstanz  Hochschule Karlsruhe  Disy Informationssysteme GmbH  EXASOL AG  EFTAS Fernerkundung Technologietransfer GmbH  Landesanstalt f\u00fcr Umwelt Messungen und Naturschutz", 
            "title": "Project Partners"
        }, 
        {
            "location": "/consortium/#associated-partners", 
            "text": "THW Karlsruhe  Stadt Karlsruhe", 
            "title": "Associated Partners"
        }, 
        {
            "location": "/contributing/", 
            "text": "Contributing\n\n\nWe value all kinds of contributions from the community, not just actual\ncode. If you do like to contribute actual code in the form of bug fixes, new\nfeatures or other patches this page gives you more info on how to do it.\n\n\nGit Branching Model\n\n\nThe BigGIS team follows the standard practice of using the\n\nmaster\n branch as main integration branch.\n\n\nGit Commit Messages\n\n\nWe follow the 'imperative present tense' style for commit messages.\n(e.g. \"Add new EnterpriseWidgetLoader instance\")\n\n\nIssue Tracking\n\n\nIf you find a bug and would like to report it please go to the github\nissue tracker of a given sub-project and file an issue.\n\n\nPull Requests\n\n\nIf you'd like to submit a code contribution please fork BigGIS and\nsend us pull request against the \nmaster\n branch. Like any other open\nsource project, we might ask you to go through some iterations of\ndiscussion and refinement before merging.\n\n\nContributing documentation\n\n\nsee \nDocumentation Howto", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#contributing", 
            "text": "We value all kinds of contributions from the community, not just actual\ncode. If you do like to contribute actual code in the form of bug fixes, new\nfeatures or other patches this page gives you more info on how to do it.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#git-branching-model", 
            "text": "The BigGIS team follows the standard practice of using the master  branch as main integration branch.", 
            "title": "Git Branching Model"
        }, 
        {
            "location": "/contributing/#git-commit-messages", 
            "text": "We follow the 'imperative present tense' style for commit messages.\n(e.g. \"Add new EnterpriseWidgetLoader instance\")", 
            "title": "Git Commit Messages"
        }, 
        {
            "location": "/contributing/#issue-tracking", 
            "text": "If you find a bug and would like to report it please go to the github\nissue tracker of a given sub-project and file an issue.", 
            "title": "Issue Tracking"
        }, 
        {
            "location": "/contributing/#pull-requests", 
            "text": "If you'd like to submit a code contribution please fork BigGIS and\nsend us pull request against the  master  branch. Like any other open\nsource project, we might ask you to go through some iterations of\ndiscussion and refinement before merging.", 
            "title": "Pull Requests"
        }, 
        {
            "location": "/contributing/#contributing-documentation", 
            "text": "see  Documentation Howto", 
            "title": "Contributing documentation"
        }, 
        {
            "location": "/data-sources/", 
            "text": "Datasets in BigGIS\n\n\nData in bwCloud\n\n\nPre-cached / downloaded to bwCloud as a part of BigGIS\n\n\n\n\nATKIS land use data (multiple options possible - TODO:Matthias) TODO:how big\n\n\nshapefiles in a directory\n\n\ndata in Accumulo/Exasolution\n\n\n\n\n\n\nNew York taxi drives\n\n\n2GB/month -\n for years 2009-2015 potentially ~160GB of storage space\n\n\nmultiple options possible - TODO:Matthias\n\n\nbunch of CSV files in a directories organized per year\n\n\npoints stored in Accumulo\n\n\npoints stored in Exasolution\n\n\n\n\n\n\nLarge historical data sets (HDFS/Accumulo - TODO:Matthias):\n\n\nLUBW (REST API for pulling)\n\n\nDWD (REST API for pulling)\n\n\nVolunteered Meteorological Information (Wunderground) (TODO:UKON)\n\n\nAir Pollution Data (Umweltbundesamt) \n\n\nDrosophila Suzukii Observations (Vitimeteo)\n\n\nSatellite Land-Surface-Temperature (MODIS Satellite)\n\n\nTraffic Incidents (Bing Traffic API)\n\n\nVolunteered Geographic Information (enviroCar)\n\n\n\n\nConnectors to external services\n\n\nThe following connectors to external data sources should be available (for pulling data as a stream).\n\n\n\n\nPulling data:\n\n\nEnvisat raster data\n\n\nLandsat raster data\n\n\nDWD weather stations\n\n\nLUBW weather stations\n\n\n\n\nWunderground weather stations\n\n\n\n\n\n\nPushing data (REST API needs to be developed from our side):\n\n\n\n\nSensor data from LoRa weather stations\n\n\nVGI data -\n compare KA Feedback (TODO)\n\n\nTODO: hyperspectral images from drones (TODO)\n\n\nTODO: other sensor data from drones (TODO)", 
            "title": "Datasets in BigGIS"
        }, 
        {
            "location": "/data-sources/#datasets-in-biggis", 
            "text": "", 
            "title": "Datasets in BigGIS"
        }, 
        {
            "location": "/data-sources/#data-in-bwcloud", 
            "text": "Pre-cached / downloaded to bwCloud as a part of BigGIS   ATKIS land use data (multiple options possible - TODO:Matthias) TODO:how big  shapefiles in a directory  data in Accumulo/Exasolution    New York taxi drives  2GB/month -  for years 2009-2015 potentially ~160GB of storage space  multiple options possible - TODO:Matthias  bunch of CSV files in a directories organized per year  points stored in Accumulo  points stored in Exasolution    Large historical data sets (HDFS/Accumulo - TODO:Matthias):  LUBW (REST API for pulling)  DWD (REST API for pulling)  Volunteered Meteorological Information (Wunderground) (TODO:UKON)  Air Pollution Data (Umweltbundesamt)   Drosophila Suzukii Observations (Vitimeteo)  Satellite Land-Surface-Temperature (MODIS Satellite)  Traffic Incidents (Bing Traffic API)  Volunteered Geographic Information (enviroCar)", 
            "title": "Data in bwCloud"
        }, 
        {
            "location": "/data-sources/#connectors-to-external-services", 
            "text": "The following connectors to external data sources should be available (for pulling data as a stream).   Pulling data:  Envisat raster data  Landsat raster data  DWD weather stations  LUBW weather stations   Wunderground weather stations    Pushing data (REST API needs to be developed from our side):   Sensor data from LoRa weather stations  VGI data -  compare KA Feedback (TODO)  TODO: hyperspectral images from drones (TODO)  TODO: other sensor data from drones (TODO)", 
            "title": "Connectors to external services"
        }, 
        {
            "location": "/data-sources/weather-stations/", 
            "text": "Weather stations\n\n\n\n\nTodo\n\n\nJulian, Hannes, Jochen\n\n\n\n\nLUBW weather station at FZI\n\n\nLabels:\n Stream Processing, Batch Processing\n\n\n\n\nthe device was mounted at FZI in January 2017 for measuring air pressure, temperature, wind speed, precipitation (was bedeuten die Parameter \"strg_1\" und \"rlf_1\").\n\n\nmeasurement results (*.mw1.xml) are pushed via xml-files on BigGIS SFTP Server at the University Konstanz \n  -measurement devices will be dismounted at April 2018\n\n\n\n\nSenseBox-based weather stations\n\n\n\n\n\n\nsupport two different modes of data transmission: WLAN and LoRa\n\n\n\n\nLoRa is preferred for longer range and easier deployment (no site-specific configuration)\n\n\n\n\n\n\n\n\nLoRa gateways\n\n\n\n\nTwo LoRa gateways should be deployed, one at FZI, the other at LUBW-KA (TODO:clarify)\n\n\nGateways are a \nKickstarter project\n,\n    and have not yet been delivered as of 2017-12-15. \nLast status\n\n    on Kickstarter is that the gateways are ready to ship (2017-11-06)\n\n\nJulian handles Kickstarter and deployment\n\n\n\n\n\n\n\n\nData handling\n\n\n\n\na common Kafka queue for events from both transports\n\n\none Kafka message per transmitted event, containing all measurements (temperature, humidity, air pressure, internal temperature, light, UV radiation)\n\n\ntransport specific adapters for \nWLAN\n\n    and \nLoRa\n\n\nshould be handled in a stream-processing way (pipeline modeled using StreamPipes)\n\n\nData source and metadata enrichment exist as \nStreamPipes components\n\n\n\n\n\n\nshould be stored within BigGIS database i.e. Accumulo/Exasolution (sensor,lat,lon,ts)\n\n\na \nFlink job\n\n    that persists the events from the Kafka queue into MySQL or PostgreSQL exists,\n    could be extended to support other JDBC databases\n\n\n\n\n\n\nshould be sent to \nhttps://opensensemap.org/\n (TODO:Jochen)\n\n\nOutlier filtering node (kafka-\nflink-\nkafka)\n\n\n\n\n\n\n\n\nDeployment\n\n\n\n\n34 LoRa sensor units should be deployed, Julian handles locations, external organizations etc.\n\n\n\n\n\n\n\n\nWeb-based mobile-friendly app\n\n\n\n\nQR code contains stations id and URL that leads to public web\n\n\nthe web page contains info about the station and the project\n\n\nadmin can click and change station information (or register a new station)\n\n\nlat/lon is taken from the phone (HTML5 geolocation api)\n\n\nadmin can add additional parameters (placement details)", 
            "title": "Weather stations"
        }, 
        {
            "location": "/data-sources/weather-stations/#weather-stations", 
            "text": "Todo  Julian, Hannes, Jochen", 
            "title": "Weather stations"
        }, 
        {
            "location": "/data-sources/weather-stations/#lubw-weather-station-at-fzi", 
            "text": "Labels:  Stream Processing, Batch Processing   the device was mounted at FZI in January 2017 for measuring air pressure, temperature, wind speed, precipitation (was bedeuten die Parameter \"strg_1\" und \"rlf_1\").  measurement results (*.mw1.xml) are pushed via xml-files on BigGIS SFTP Server at the University Konstanz \n  -measurement devices will be dismounted at April 2018", 
            "title": "LUBW weather station at FZI"
        }, 
        {
            "location": "/data-sources/weather-stations/#sensebox-based-weather-stations", 
            "text": "support two different modes of data transmission: WLAN and LoRa   LoRa is preferred for longer range and easier deployment (no site-specific configuration)     LoRa gateways   Two LoRa gateways should be deployed, one at FZI, the other at LUBW-KA (TODO:clarify)  Gateways are a  Kickstarter project ,\n    and have not yet been delivered as of 2017-12-15.  Last status \n    on Kickstarter is that the gateways are ready to ship (2017-11-06)  Julian handles Kickstarter and deployment     Data handling   a common Kafka queue for events from both transports  one Kafka message per transmitted event, containing all measurements (temperature, humidity, air pressure, internal temperature, light, UV radiation)  transport specific adapters for  WLAN \n    and  LoRa  should be handled in a stream-processing way (pipeline modeled using StreamPipes)  Data source and metadata enrichment exist as  StreamPipes components    should be stored within BigGIS database i.e. Accumulo/Exasolution (sensor,lat,lon,ts)  a  Flink job \n    that persists the events from the Kafka queue into MySQL or PostgreSQL exists,\n    could be extended to support other JDBC databases    should be sent to  https://opensensemap.org/  (TODO:Jochen)  Outlier filtering node (kafka- flink- kafka)     Deployment   34 LoRa sensor units should be deployed, Julian handles locations, external organizations etc.", 
            "title": "SenseBox-based weather stations"
        }, 
        {
            "location": "/data-sources/weather-stations/#web-based-mobile-friendly-app", 
            "text": "QR code contains stations id and URL that leads to public web  the web page contains info about the station and the project  admin can click and change station information (or register a new station)  lat/lon is taken from the phone (HTML5 geolocation api)  admin can add additional parameters (placement details)", 
            "title": "Web-based mobile-friendly app"
        }, 
        {
            "location": "/demos/", 
            "text": "About demos\n\n\nIn this section you can find demos of various BigGIS components.\n\n\nEach demo consists of:\n\n\n\n\nA short introduction of the problem being solved.\n\n\nReferences to related methods, algorithms and models (from section \nMethods\n)\n\n\nExplanation of related code samples.\n\n\nStep-by-step tutorial, how the demo can be executed.\n\n\n\n\nDemos to be included later\n\n\n\n\nExasol: demo of using a spatial index\n\n\nExasol: demo of using a virtual schema\n\n\nExasol: demo of using R-connector", 
            "title": "About demos"
        }, 
        {
            "location": "/demos/#about-demos", 
            "text": "In this section you can find demos of various BigGIS components.  Each demo consists of:   A short introduction of the problem being solved.  References to related methods, algorithms and models (from section  Methods )  Explanation of related code samples.  Step-by-step tutorial, how the demo can be executed.", 
            "title": "About demos"
        }, 
        {
            "location": "/demos/#demos-to-be-included-later", 
            "text": "Exasol: demo of using a spatial index  Exasol: demo of using a virtual schema  Exasol: demo of using R-connector", 
            "title": "Demos to be included later"
        }, 
        {
            "location": "/demos/enviro-car/", 
            "text": "Responsible person for this section\n\n\nManuel Stein\n\n\n\n\nEnviroCar (Smart City)\n\n\nVisualisierung von Verkehrs- und Umweltdaten basierend auf mobilen Sensoren in Fahrzeugen\n\n\nMotivation\n\n\nShort section about available data\n\n\nVisual Analysis of Traffic Data\n\n\n\n\nHighdimensional\n\n\nspatial \n\n\ntemporal\n\n\nspatio temporal \n\n\n\n\nVisual Interactive Logging and Provenance\n\n\n\n\nAbstraction\n\n\nVisualization and Interaction\n\n\n\n\nRelated Scenarios\n\n\n\n\nSmart City\n\n\nEnvironment", 
            "title": "EnviroCar (Smart City)"
        }, 
        {
            "location": "/demos/enviro-car/#envirocar-smart-city", 
            "text": "Visualisierung von Verkehrs- und Umweltdaten basierend auf mobilen Sensoren in Fahrzeugen", 
            "title": "EnviroCar (Smart City)"
        }, 
        {
            "location": "/demos/enviro-car/#motivation", 
            "text": "", 
            "title": "Motivation"
        }, 
        {
            "location": "/demos/enviro-car/#short-section-about-available-data", 
            "text": "", 
            "title": "Short section about available data"
        }, 
        {
            "location": "/demos/enviro-car/#visual-analysis-of-traffic-data", 
            "text": "Highdimensional  spatial   temporal  spatio temporal", 
            "title": "Visual Analysis of Traffic Data"
        }, 
        {
            "location": "/demos/enviro-car/#visual-interactive-logging-and-provenance", 
            "text": "Abstraction  Visualization and Interaction", 
            "title": "Visual Interactive Logging and Provenance"
        }, 
        {
            "location": "/demos/enviro-car/#related-scenarios", 
            "text": "Smart City  Environment", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/demos/gas-detect/", 
            "text": "Responsible person for this section\n\n\nAlexander Groeschel\n\n\n\n\nGas Cloud Detection\n\n\n\n\nChlorophyll-Erkennung im Befliegungsexperiment\n\n\nUnsichtbare Schadgaswolke (IR-Bereich)\n\n\nHei\u00dfe oder kalte Gase/Gegenst\u00e4nde leicht zu erkennen\n\n\nGase mit Umgebungstemperatur im IR-Bild nicht zu erkennen.\n\n\nSubtraktion von georeferenzierten und zus\u00e4tzlich positionskorrigierten Bildern aus Zeitreihen macht kleine \n  Abweichungen in Bildern sichtbar -\n Wolken unsichtbarer Gase im Bild sichtbar\n\n\n\n\nBefliegungskampagne am 15./16.07.17\n\n\n\n\nAnalyse von Gaswolken aus der Luft\n\n\nTools:\n\n\nIR/RGB-Kamera\n\n\nHyperspektralkamera\n\n\n\n\n\n\n\n\n\n\nEtablierung einer Funkstrecke\n\n\n\u00dcbertragung von Flugplan-Daten/Bildergebnissen\n\n\n\n\n\n\n\n\nRelated Scenarios\n\n\n\n\nDisaster Management\n\n\nSmart City", 
            "title": "Gas Cloud Detection"
        }, 
        {
            "location": "/demos/gas-detect/#gas-cloud-detection", 
            "text": "Chlorophyll-Erkennung im Befliegungsexperiment  Unsichtbare Schadgaswolke (IR-Bereich)  Hei\u00dfe oder kalte Gase/Gegenst\u00e4nde leicht zu erkennen  Gase mit Umgebungstemperatur im IR-Bild nicht zu erkennen.  Subtraktion von georeferenzierten und zus\u00e4tzlich positionskorrigierten Bildern aus Zeitreihen macht kleine \n  Abweichungen in Bildern sichtbar -  Wolken unsichtbarer Gase im Bild sichtbar", 
            "title": "Gas Cloud Detection"
        }, 
        {
            "location": "/demos/gas-detect/#befliegungskampagne-am-15160717", 
            "text": "Analyse von Gaswolken aus der Luft  Tools:  IR/RGB-Kamera  Hyperspektralkamera      Etablierung einer Funkstrecke  \u00dcbertragung von Flugplan-Daten/Bildergebnissen", 
            "title": "Befliegungskampagne am 15./16.07.17"
        }, 
        {
            "location": "/demos/gas-detect/#related-scenarios", 
            "text": "Disaster Management  Smart City", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/demos/gas-predict/", 
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nGas Cloud Prediction\n\n\n\n\nModeling of gas clouds and their dispersion over time.\n\n\nJulian's bachelor student (maybe text from his bc-thesis?)", 
            "title": "Gas Cloud Prediction"
        }, 
        {
            "location": "/demos/gas-predict/#gas-cloud-prediction", 
            "text": "Modeling of gas clouds and their dispersion over time.  Julian's bachelor student (maybe text from his bc-thesis?)", 
            "title": "Gas Cloud Prediction"
        }, 
        {
            "location": "/demos/heatstress/", 
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nHeatstress Routing App\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/path-optimizer\n\n\n\n\nRelated Scenarios: \nSmart City\n\n\nThe back-end exposes a simple REST-API, that can be used for routing or to find\nthe optimal point in time. The API can be accessed on \nhttp://localhost:8080/heatstressrouting/api/v1\n.\nJSON is supported as the only output format.\n\n\nThe following sections describe the API in detail.\n\n\nServer information\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/info\n\n\nReturns some information about the running service, e.g. the supported area and time range\n\n\nParameters\n\n\nThe \n/info\n site takes no parameters.\n\n\nReturns\n\n\nReturns some information about the running service (see sample response below):\n\n\n\n\nbbox\n: the bounding box of the area supported by the service as an array of \n[min_lat, min_lng, max_lat, max_lng]\n.\n\n\ntime_range\n: the time range supported by the service, given as time stamps of the form \n2014-08-23T00:00\n.\n\n\nplace_types\n: a list of place types supported by the optimal time api\n\n\n\n\nExample\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/info\n\n\n\nSample Response:\n\n{\n\n  \nservice\n:\n \nheat stress routing\n,\n\n  \nversion\n:\n \n0.0.1-SNAPSHOT\n,\n\n  \nbuild_time\n:\n \n2016-09-27T07:50:42Z\n,\n\n  \nbbox\n:\n \n[\n\n    \n48.99\n,\n\n    \n8.385\n,\n\n    \n49.025\n,\n\n    \n8.435\n\n  \n],\n\n  \ntime_range\n:\n \n{\n\n    \nfrom\n:\n \n2014-08-23T00:00\n,\n\n    \nto\n:\n \n2016-02-23T23:00\n\n  \n},\n\n  \nplace_types\n:\n \n[\n\n    \nbakery\n,\n\n    \ntaxi\n,\n\n    \npost_office\n,\n\n    \nice_cream\n,\n\n    \ndentist\n,\n\n    \npost_box\n,\n\n    \nsupermarket\n,\n\n    \ntoilets\n,\n\n    \nbank\n,\n\n    \ncafe\n,\n\n    \npolice\n,\n\n    \ndoctors\n,\n\n    \npharmacy\n,\n\n    \ndrinking_water\n,\n\n    \natm\n,\n\n    \nclinic\n,\n\n    \nkiosk\n,\n\n    \nhospital\n,\n\n    \nchemist\n,\n\n    \nfast_food\n\n  \n]\n\n\n}\n\n\n\n\nRouting\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/routing\n\n\nComputes the optimal route (regarding heat stress) between a start and a destination at a given time.\n\n\nParameters\n\n\nThe \n/routing\n api supports the following parameter (some are optional):\n\n\n\n\nstart\n: the start point as pair of a latitude value and longitude value (in that order)\n  seperated by a comma, e.g. \nstart=49.0118083,8.4251357\n.\n\n\ndestination\n: the destination as pair of a latitude value and longitude value (in that order)\n  separated by a comma, e.g. \ndestination=49.0126868,8.4065707\n. \n\n\ntime\n: the date and time the optimal route should be searched for; a time stamp of the form\n  \nYYYY-MM-DDTHH:MM:SS\n, e.g. \ntime=2015-08-31T10:00:00\n. The value must be in the time range\n  returned by \n/info\n (see \nabove\n).\n\n\nweighting\n (optional): the weightings to be used; a comma seperated list of the supported\n  weightings (\nshortest\n, \nheatindex\n and \ntemperature\n), e.g. \nweighting=shortest,heatindex,temperature\n;\n  the default is \nweighting=shortest,heatindex\n; the results for the \nshortest\n weighting are always\n  returned, even if the value is omited in the weighings list.\n\n\n\n\nReturns\n\n\nThe path and some other information for each of the weightings:\n\n\n\n\nstatus\n: the status of the request; \nOK\n is everthing is okay, \nBAD_REQUEST\n if a invalid request was send or \nINTERNAL_SERVER_ERROR\n if an internal error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\nresults\n: the results for each weighting:\n\n\nweighting\n: the weighting used for that result (see parameter \nweighting\n above).\n\n\nstart\n: the coordinates of the start point as array of \n[lat, lng]\n.\n\n\ndestination\n: the coordinates of the destination as array of \n[lat, lng]\n.\n\n\ndistance\n: the length of the route in meter.\n\n\nduration\n: the walking time in milli seconds.\n\n\nroute_weights\n: the route weights of the selected weightings for the route.\n\n\npath\n: the geometry of the path found; an array of points, were each point is an array of [lat, lng]`.\n\n\n\n\n\n\n\n\nExample\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/routing?start=49.0118083,8.4251357\ndestination=49.0126868,8.4065707\ntime=2015-08-31T10:00:00\nweighting=shortest,heatindex,temperature\n\n\n\nSample Response:\n\n{\n\n  \nstatus\n:\n \nOK\n,\n\n  \nstatus_code\n:\n \n200\n,\n\n  \nresults\n:\n \n{\n\n    \nshortest\n:\n \n{\n\n      \nweighting\n:\n \nshortest\n,\n\n      \nstart\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \ndestination\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \ndistance\n:\n \n1698.2989202985977\n,\n\n      \nduration\n:\n \n1222740\n,\n\n      \nroute_weights\n:\n \n{\n\n        \ntemperature\n:\n \n50903.955833052285\n,\n\n        \nheatindex\n:\n \n50892.20496302502\n,\n\n        \nshortest\n:\n \n1698.2989202985977\n\n      \n},\n\n      \npath\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \nheatindex\n:\n \n{\n\n      \nweighting\n:\n \nheatindex\n,\n\n      \nstart\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \ndestination\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \ndistance\n:\n \n1901.8839202985973\n,\n\n      \nduration\n:\n \n1369323\n,\n\n      \nroute_weights\n:\n \n{\n\n        \ntemperature\n:\n \n51868.74807902536\n,\n\n        \nheatindex\n:\n \n51098.277424417196\n,\n\n        \nshortest\n:\n \n1901.8839202985978\n\n      \n},\n\n      \npath\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \ntemperature\n:\n \n{\n\n      \nweighting\n:\n \ntemperature\n,\n\n      \nstart\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \ndestination\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \ndistance\n:\n \n1901.8839202985973\n,\n\n      \nduration\n:\n \n1369323\n,\n\n      \nroute_weights\n:\n \n{\n\n        \ntemperature\n:\n \n51868.74807902536\n,\n\n        \nheatindex\n:\n \n51098.277424417196\n,\n\n        \nshortest\n:\n \n1901.8839202985978\n\n      \n},\n\n      \npath\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\nOptimal time\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime\n\n\nPerformce a nearby search for a given start point and computes for every place that fulfills\na specified criterion an optimal point in time, i.e. the time with the minimal heat stress.\n\n\nParameters\n\n\nThe \n/optimaltime\n api supports the following parameter (some are optional):\n\n\n\n\nstart\n: the start point as pair of a latitude value and longitude value (in that order) seperated by a comma, e.g. \nstart=49.0118083,8.4251357\n. \n\n\ntime\n: the date and time the optimal route should be searched for; a time stamp of the form \nYYYY-MM-DDTHH:MM:SS\n, e.g. \ntime=2015-08-31T10:00:00\n. The value must be in the time range returned by \n/info\n (see \nabove\n).\n\n\nplace_type\n: the place type to search for; a comma seperated list of supported place types, e.g. \nplace_type=supermarket,chemist\n; a complete list of supported place list can be queried using the \ninfo\n api (see \nabove\n). Currently the following place tyes are supported: \nbakery\n, \ntaxi\n, \npost_office\n, \nice_cream\n, \ndentist\n, \npost_box\n, \nsupermarket\n, \ntoilets\n, \nbank\n, \ncafe\n, \npolice\n, \ndoctors\n, \npharmacy\n, \ndrinking_water\n, \natm\n, \nclinic\n, \nkiosk\n, \nhospital\n, \nchemist\n, \nfast_food\n. The place types are mapped to the corresponding \nshop\n respectively \namenity\n tags.\n\n\nmax_results\n (optional): the maximum number of results to consider for the nearby search (an positive integer), e.g. \nmax_results=10\n; the default value is 5.\n\n\nmax_distance\n (optional): the maximum direct distance (as the crow flies) between the start point and the place in meter, e.g. \nmax_distance=500.0\n; the default value is 1000.0 meter.\n\n\ntime_buffer\n (optional): the minimum time needed at the place (in minutes), i.e. the optimal time is chossen so that the place is opened for a least \ntime_buffer\n when the user arrives, e.g. \ntime_buffer=30\n; the default value is 15 miniutes.\n\n\nearliest_time\n (optional): the earliest desired time, either a time stamp, e.g. \nearliest_time=2015-08-31T09:00\n or the string \nnull\n (case is ignored); the default value is \nnull\n. If both \nearliest_time\n and \nlatest_time\n are specified, \nearliest_time\n must be before \nlatest_time\n.\n\n\nlatest_time\n (optional): the latest desired time, either a time stamp, e.g. \nlatest_time=2015-08-31T17:00\n or the string \nnull\n (case is ignored); the default value is \nnull\n. If both \nearliest_time\n and \nlatest_time\n are specified, \nearliest_time\n must be before \nlatest_time\n; \nlatest_time\n must be after \ntime\n.\n\n\n\n\nReturns\n\n\nThe optimal point in time for each place found in the specified radius ranked by the optimal-value:\n\n\n\n\nstatus\n: the status of the request; \nOK\n if everthing is okay, \nNO_REULTS\n if not results were found, \nBAD_REQUEST\n if a invalid request was send to the server or \nINTERNAL_SERVER_ERROR\n if an internal server error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\n\n\nresults\n: the result for each place found during the nearby search:\n\n\n\n\nrank\n: the rank of the place according to the optimal value (were 1 is the best rank).\n\n\nname\n: the name of the place.\n\n\nosm_id\n: the \nOpenStreetMap Node ID\n of the place.\n\n\nlocation\n: the coordinates of the places as an array of \n[lat, lng]\n.\n\n\nopening_hours\n: the opening hours of the place; the format specification can be found \nhere\n.\n\n\noptimal_time\n: the optimal point in time found for that place, e.g. \n2015-08-31T20:00\n\n\noptimal_value\n: the optimal value found for the place; the value considering the heat stress acording to steadman's heatindex \n(Steadmean, 1979)\n as well as the distance between the start and the place.\n\n\ndistance\n: the length of the optimal path (see \nRouting\n above) from the start to the place in meter.\n\n\nduration\n: the time needed to walk from the start to the place (in milli seconds).\n\n\npath_optimal\n: the geometry of the optimal path (see \nRouting\n above).\n\n\ndistance_shortest\n: the length of the shortest path between the start and the place (in meter).\n\n\nduration_shortest\n: the time needed to walk the shortest path between the start and the place (in milli seconds).\n\n\npath_optimal\n: the geometry of the shortest path (see \nRouting\n above).\n\n\n\n\n\n\n\n\nExample\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime?start=49.0118083,8.4251357\ntime=2015-08-31T10:00:00\nplace_type=supermarket\nmax_distance=1000\nmax_results=5\ntime_buffer=15\nearliest_time=2015-08-31T09:00:00\nlatest_time=2015-08-31T20:00:00\n\n\n\nSample Response:\n\n{\n\n  \nstatus\n:\n \nOK\n,\n\n  \nstatus_code\n:\n \n200\n,\n\n  \nresults\n:\n \n[\n\n    \n{\n\n      \nrank\n:\n \n1\n,\n\n      \nname\n:\n \nRewe City\n,\n\n      \nosm_id\n:\n \n897615202\n,\n\n      \nlocation\n:\n \n[\n\n        \n49.0096613\n,\n\n        \n8.4237272\n\n      \n],\n\n      \nopening_hours\n:\n \nMo-Sa 07:00-22:00; Su,PH off\n,\n\n      \noptimal_time\n:\n \n2015-08-31T20:00\n,\n\n      \noptimal_value\n:\n \n12515.36230258099\n,\n\n      \ndistance\n:\n \n539.1839746027457\n,\n\n      \nduration\n:\n \n388207\n,\n\n      \npath_optimal\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00954480942009\n,\n\n          \n8.423681942364334\n\n        \n]\n\n      \n],\n\n      \ndistance_shortest\n:\n \n468.99728441805115\n,\n\n      \nduration_shortest\n:\n \n337669\n,\n\n      \npath_shortest\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00954480942009\n,\n\n          \n8.423681942364334\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \n{\n\n      \nrank\n:\n \n2\n,\n\n      \nname\n:\n \nOststadt Super-Bio-Markt\n,\n\n      \nosm_id\n:\n \n931682116\n,\n\n      \nlocation\n:\n \n[\n\n        \n49.009433\n,\n\n        \n8.4234214\n\n      \n],\n\n      \nopening_hours\n:\n \nMo-Fr 09:00-13:00,14:00-18:30; Sa 09:00-13:00\n,\n\n      \noptimal_time\n:\n \n2015-08-31T18:09:19.199\n,\n\n      \noptimal_value\n:\n \n14318.962937267655\n,\n\n      \ndistance\n:\n \n473.346750294328\n,\n\n      \nduration\n:\n \n340801\n,\n\n      \npath_optimal\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00944708743373\n,\n\n          \n8.4235711322383\n\n        \n]\n\n      \n],\n\n      \ndistance_shortest\n:\n \n473.346750294328\n,\n\n      \nduration_shortest\n:\n \n340801\n,\n\n      \npath_shortest\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00944708743373\n,\n\n          \n8.4235711322383\n\n        \n]\n\n      \n]\n\n    \n}\n\n  \n]\n\n\n}\n\n\n\n\nError messages\n\n\nIf an error occurs, e.g. because a bade request were send to the server or an internal server errors occurs, the server is sending a JSON response with the following content:\n\n\n\n\nstatus\n: the status of the request; \nOK\n if everthing is okay, \nNO_REULTS\n if not results were found, \nBAD_REQUEST\n if a invalid request was send to the server or \nINTERNAL_SERVER_ERROR\n if an internal server error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\nmessages\n: an array of human readable error messages.\n\n\n\n\nExample\n\n\nExample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime?start=Schloss,%20Karlsruhe\ntime=2015-08-31T10:00:00\nplace_type=supermarket\n\n\n\nExample Response:\n\n{\n\n  \nstatus\n:\n \nBAD_REQUEST\n,\n\n  \nstatus_code\n:\n \n400\n,\n\n  \nmessages\n:\n \n[\n\n    \nstart (Schloss, Karlsruhe) could not be parsed: failed to parse coordinate; \nstart\n must be a pair of latitude and longitude seperated by a comma (\n,\n), e.g. \n49.0118083,8.4251357\n)\n\n  \n]\n\n\n}\n\n\n\n\nReferences\n\n\n\n\nReference\n\n\nSteadman, R. G. \nThe Assessment of Sultriness. Part I: A Temperature-Humidity Index Based on Human Physiology and Clothing.\n\nScience Journal of Applied Meteorology, 1979, 18, 861-873, DOI: 10.1175/1520-0450(1979)018\n0861:TAOSPI\n2.0.CO;2", 
            "title": "Heatstress Routing App"
        }, 
        {
            "location": "/demos/heatstress/#heatstress-routing-app", 
            "text": "Note  Related repository is  https://github.com/biggis-project/path-optimizer   Related Scenarios:  Smart City  The back-end exposes a simple REST-API, that can be used for routing or to find\nthe optimal point in time. The API can be accessed on  http://localhost:8080/heatstressrouting/api/v1 .\nJSON is supported as the only output format.  The following sections describe the API in detail.", 
            "title": "Heatstress Routing App"
        }, 
        {
            "location": "/demos/heatstress/#server-information", 
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/info  Returns some information about the running service, e.g. the supported area and time range", 
            "title": "Server information"
        }, 
        {
            "location": "/demos/heatstress/#parameters", 
            "text": "The  /info  site takes no parameters.", 
            "title": "Parameters"
        }, 
        {
            "location": "/demos/heatstress/#returns", 
            "text": "Returns some information about the running service (see sample response below):   bbox : the bounding box of the area supported by the service as an array of  [min_lat, min_lng, max_lat, max_lng] .  time_range : the time range supported by the service, given as time stamps of the form  2014-08-23T00:00 .  place_types : a list of place types supported by the optimal time api", 
            "title": "Returns"
        }, 
        {
            "location": "/demos/heatstress/#example", 
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/info  Sample Response: { \n   service :   heat stress routing , \n   version :   0.0.1-SNAPSHOT , \n   build_time :   2016-09-27T07:50:42Z , \n   bbox :   [ \n     48.99 , \n     8.385 , \n     49.025 , \n     8.435 \n   ], \n   time_range :   { \n     from :   2014-08-23T00:00 , \n     to :   2016-02-23T23:00 \n   }, \n   place_types :   [ \n     bakery , \n     taxi , \n     post_office , \n     ice_cream , \n     dentist , \n     post_box , \n     supermarket , \n     toilets , \n     bank , \n     cafe , \n     police , \n     doctors , \n     pharmacy , \n     drinking_water , \n     atm , \n     clinic , \n     kiosk , \n     hospital , \n     chemist , \n     fast_food \n   ]  }", 
            "title": "Example"
        }, 
        {
            "location": "/demos/heatstress/#routing", 
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/routing  Computes the optimal route (regarding heat stress) between a start and a destination at a given time.", 
            "title": "Routing"
        }, 
        {
            "location": "/demos/heatstress/#parameters_1", 
            "text": "The  /routing  api supports the following parameter (some are optional):   start : the start point as pair of a latitude value and longitude value (in that order)\n  seperated by a comma, e.g.  start=49.0118083,8.4251357 .  destination : the destination as pair of a latitude value and longitude value (in that order)\n  separated by a comma, e.g.  destination=49.0126868,8.4065707 .   time : the date and time the optimal route should be searched for; a time stamp of the form\n   YYYY-MM-DDTHH:MM:SS , e.g.  time=2015-08-31T10:00:00 . The value must be in the time range\n  returned by  /info  (see  above ).  weighting  (optional): the weightings to be used; a comma seperated list of the supported\n  weightings ( shortest ,  heatindex  and  temperature ), e.g.  weighting=shortest,heatindex,temperature ;\n  the default is  weighting=shortest,heatindex ; the results for the  shortest  weighting are always\n  returned, even if the value is omited in the weighings list.", 
            "title": "Parameters"
        }, 
        {
            "location": "/demos/heatstress/#returns_1", 
            "text": "The path and some other information for each of the weightings:   status : the status of the request;  OK  is everthing is okay,  BAD_REQUEST  if a invalid request was send or  INTERNAL_SERVER_ERROR  if an internal error occoured.  status_code : the HTTP status code returned.  results : the results for each weighting:  weighting : the weighting used for that result (see parameter  weighting  above).  start : the coordinates of the start point as array of  [lat, lng] .  destination : the coordinates of the destination as array of  [lat, lng] .  distance : the length of the route in meter.  duration : the walking time in milli seconds.  route_weights : the route weights of the selected weightings for the route.  path : the geometry of the path found; an array of points, were each point is an array of [lat, lng]`.", 
            "title": "Returns"
        }, 
        {
            "location": "/demos/heatstress/#example_1", 
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/routing?start=49.0118083,8.4251357 destination=49.0126868,8.4065707 time=2015-08-31T10:00:00 weighting=shortest,heatindex,temperature  Sample Response: { \n   status :   OK , \n   status_code :   200 , \n   results :   { \n     shortest :   { \n       weighting :   shortest , \n       start :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       destination :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       distance :   1698.2989202985977 , \n       duration :   1222740 , \n       route_weights :   { \n         temperature :   50903.955833052285 , \n         heatindex :   50892.20496302502 , \n         shortest :   1698.2989202985977 \n       }, \n       path :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     }, \n     heatindex :   { \n       weighting :   heatindex , \n       start :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       destination :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       distance :   1901.8839202985973 , \n       duration :   1369323 , \n       route_weights :   { \n         temperature :   51868.74807902536 , \n         heatindex :   51098.277424417196 , \n         shortest :   1901.8839202985978 \n       }, \n       path :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     }, \n     temperature :   { \n       weighting :   temperature , \n       start :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       destination :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       distance :   1901.8839202985973 , \n       duration :   1369323 , \n       route_weights :   { \n         temperature :   51868.74807902536 , \n         heatindex :   51098.277424417196 , \n         shortest :   1901.8839202985978 \n       }, \n       path :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     } \n   }  }", 
            "title": "Example"
        }, 
        {
            "location": "/demos/heatstress/#optimal-time", 
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/optimaltime  Performce a nearby search for a given start point and computes for every place that fulfills\na specified criterion an optimal point in time, i.e. the time with the minimal heat stress.", 
            "title": "Optimal time"
        }, 
        {
            "location": "/demos/heatstress/#parameters_2", 
            "text": "The  /optimaltime  api supports the following parameter (some are optional):   start : the start point as pair of a latitude value and longitude value (in that order) seperated by a comma, e.g.  start=49.0118083,8.4251357 .   time : the date and time the optimal route should be searched for; a time stamp of the form  YYYY-MM-DDTHH:MM:SS , e.g.  time=2015-08-31T10:00:00 . The value must be in the time range returned by  /info  (see  above ).  place_type : the place type to search for; a comma seperated list of supported place types, e.g.  place_type=supermarket,chemist ; a complete list of supported place list can be queried using the  info  api (see  above ). Currently the following place tyes are supported:  bakery ,  taxi ,  post_office ,  ice_cream ,  dentist ,  post_box ,  supermarket ,  toilets ,  bank ,  cafe ,  police ,  doctors ,  pharmacy ,  drinking_water ,  atm ,  clinic ,  kiosk ,  hospital ,  chemist ,  fast_food . The place types are mapped to the corresponding  shop  respectively  amenity  tags.  max_results  (optional): the maximum number of results to consider for the nearby search (an positive integer), e.g.  max_results=10 ; the default value is 5.  max_distance  (optional): the maximum direct distance (as the crow flies) between the start point and the place in meter, e.g.  max_distance=500.0 ; the default value is 1000.0 meter.  time_buffer  (optional): the minimum time needed at the place (in minutes), i.e. the optimal time is chossen so that the place is opened for a least  time_buffer  when the user arrives, e.g.  time_buffer=30 ; the default value is 15 miniutes.  earliest_time  (optional): the earliest desired time, either a time stamp, e.g.  earliest_time=2015-08-31T09:00  or the string  null  (case is ignored); the default value is  null . If both  earliest_time  and  latest_time  are specified,  earliest_time  must be before  latest_time .  latest_time  (optional): the latest desired time, either a time stamp, e.g.  latest_time=2015-08-31T17:00  or the string  null  (case is ignored); the default value is  null . If both  earliest_time  and  latest_time  are specified,  earliest_time  must be before  latest_time ;  latest_time  must be after  time .", 
            "title": "Parameters"
        }, 
        {
            "location": "/demos/heatstress/#returns_2", 
            "text": "The optimal point in time for each place found in the specified radius ranked by the optimal-value:   status : the status of the request;  OK  if everthing is okay,  NO_REULTS  if not results were found,  BAD_REQUEST  if a invalid request was send to the server or  INTERNAL_SERVER_ERROR  if an internal server error occoured.  status_code : the HTTP status code returned.   results : the result for each place found during the nearby search:   rank : the rank of the place according to the optimal value (were 1 is the best rank).  name : the name of the place.  osm_id : the  OpenStreetMap Node ID  of the place.  location : the coordinates of the places as an array of  [lat, lng] .  opening_hours : the opening hours of the place; the format specification can be found  here .  optimal_time : the optimal point in time found for that place, e.g.  2015-08-31T20:00  optimal_value : the optimal value found for the place; the value considering the heat stress acording to steadman's heatindex  (Steadmean, 1979)  as well as the distance between the start and the place.  distance : the length of the optimal path (see  Routing  above) from the start to the place in meter.  duration : the time needed to walk from the start to the place (in milli seconds).  path_optimal : the geometry of the optimal path (see  Routing  above).  distance_shortest : the length of the shortest path between the start and the place (in meter).  duration_shortest : the time needed to walk the shortest path between the start and the place (in milli seconds).  path_optimal : the geometry of the shortest path (see  Routing  above).", 
            "title": "Returns"
        }, 
        {
            "location": "/demos/heatstress/#example_2", 
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/optimaltime?start=49.0118083,8.4251357 time=2015-08-31T10:00:00 place_type=supermarket max_distance=1000 max_results=5 time_buffer=15 earliest_time=2015-08-31T09:00:00 latest_time=2015-08-31T20:00:00  Sample Response: { \n   status :   OK , \n   status_code :   200 , \n   results :   [ \n     { \n       rank :   1 , \n       name :   Rewe City , \n       osm_id :   897615202 , \n       location :   [ \n         49.0096613 , \n         8.4237272 \n       ], \n       opening_hours :   Mo-Sa 07:00-22:00; Su,PH off , \n       optimal_time :   2015-08-31T20:00 , \n       optimal_value :   12515.36230258099 , \n       distance :   539.1839746027457 , \n       duration :   388207 , \n       path_optimal :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00954480942009 , \n           8.423681942364334 \n         ] \n       ], \n       distance_shortest :   468.99728441805115 , \n       duration_shortest :   337669 , \n       path_shortest :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00954480942009 , \n           8.423681942364334 \n         ] \n       ] \n     }, \n     { \n       rank :   2 , \n       name :   Oststadt Super-Bio-Markt , \n       osm_id :   931682116 , \n       location :   [ \n         49.009433 , \n         8.4234214 \n       ], \n       opening_hours :   Mo-Fr 09:00-13:00,14:00-18:30; Sa 09:00-13:00 , \n       optimal_time :   2015-08-31T18:09:19.199 , \n       optimal_value :   14318.962937267655 , \n       distance :   473.346750294328 , \n       duration :   340801 , \n       path_optimal :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00944708743373 , \n           8.4235711322383 \n         ] \n       ], \n       distance_shortest :   473.346750294328 , \n       duration_shortest :   340801 , \n       path_shortest :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00944708743373 , \n           8.4235711322383 \n         ] \n       ] \n     } \n   ]  }", 
            "title": "Example"
        }, 
        {
            "location": "/demos/heatstress/#error-messages", 
            "text": "If an error occurs, e.g. because a bade request were send to the server or an internal server errors occurs, the server is sending a JSON response with the following content:   status : the status of the request;  OK  if everthing is okay,  NO_REULTS  if not results were found,  BAD_REQUEST  if a invalid request was send to the server or  INTERNAL_SERVER_ERROR  if an internal server error occoured.  status_code : the HTTP status code returned.  messages : an array of human readable error messages.", 
            "title": "Error messages"
        }, 
        {
            "location": "/demos/heatstress/#example_3", 
            "text": "Example Request: http://localhost:8080/heatstressrouting/api/v1/optimaltime?start=Schloss,%20Karlsruhe time=2015-08-31T10:00:00 place_type=supermarket  Example Response: { \n   status :   BAD_REQUEST , \n   status_code :   400 , \n   messages :   [ \n     start (Schloss, Karlsruhe) could not be parsed: failed to parse coordinate;  start  must be a pair of latitude and longitude seperated by a comma ( , ), e.g.  49.0118083,8.4251357 ) \n   ]  }", 
            "title": "Example"
        }, 
        {
            "location": "/demos/heatstress/#references", 
            "text": "Reference  Steadman, R. G.  The Assessment of Sultriness. Part I: A Temperature-Humidity Index Based on Human Physiology and Clothing. \nScience Journal of Applied Meteorology, 1979, 18, 861-873, DOI: 10.1175/1520-0450(1979)018 0861:TAOSPI 2.0.CO;2", 
            "title": "References"
        }, 
        {
            "location": "/demos/hotspot_analysis/", 
            "text": "Hotspot analysis (using G*)\n\n\nProblem definition\n\n\n\n\nWe have a map, in this case a map of land surface temperatures\n\n\nWe want to find hotspots, i.e., areas on map that are \nsignificantly different from their surrounding area\n.\n\n\nWe want to use Getis-Ord G* statistic as the tool for finding the hotspots\n\n\nsee section \nStandards Getis-ord\n\n\n\n\n\n\nWe want to parallelize the computation in our Spark cluster.\n\n\nsee section \nRasterized Getis-ord\n\n\n\n\n\n\n\n\n\n\nHotspot analysis using geotrellis\n\n\nIn this section we show a simplified version of the hotspot analysis.\nWe use the \nGeotrellis\n library to achieve the parallelization.\nSome assumptions are:\n\n\n\n\nwe use 2-dimenational data (only the spatial part without the time component)\n\n\nwe store our data as a layer of tiles in geotrellis catalog (distributed raster)\n\n\nour hotspot analysis uses the standard G* with variable window\n\n\n\n\nFirst, we need to express the G* formula in terms of the map algebra operations.\n\n\nScala code\n\n\nFull source code can be found in our github repository: \n\nbiggis-project/biggis-landuse\n\n\n// typical type definition used by geotrellis\n\n\ntype\n \nSpatialRDD\n \n=\n \nRDD\n[(\nSpatialKey\n, \nTile\n)]\n\n                  \nwith\n \nMetadata\n[\nTileLayerMetadata\n[\nSpatialKey\n]]\n\n\n\ndef\n \ngetisord\n(\nrdd\n:\n \nSpatialRDD\n,\n \nweightMatrix\n:\n \nKernel\n,\n\n             \nglobMean\n:\nDouble\n,\n \nglobStdev\n:\nDouble\n,\n \nnumPixels\n:\nLong\n)\n:\n \nSpatialRDD\n \n=\n \n{\n\n\n  \nval\n \nwcells\n:\n \nArray\n[\nDouble\n]\n \n=\n \nweightMatrix\n.\ntile\n.\ntoArrayDouble\n\n  \nval\n \nsumW\n:\n \nDouble\n \n=\n \nwcells\n.\nsum\n\n  \nval\n \nsumW2\n:\n \nDouble\n \n=\n \nwcells\n.\nmap\n(\nx\n \n=\n \nx\n \n*\n \nx\n).\nsum\n\n\n  \n// variables used in the getis-ord formula\n\n  \nval\n \nA\n:\n \nDouble\n \n=\n \nglobalMean\n \n*\n \nsumW\n\n  \nval\n \nB\n:\n \nDouble\n \n=\n \nglobalStdev\n \n*\n \nMath\n.\nsqrt\n((\nnumPixels\n \n*\n \nsumW2\n \n-\n \nsumW\n \n*\n \nsumW\n)\n \n/\n \n(\nnumPixels\n \n-\n \n1\n))\n\n\n  \nrdd\n.\nwithContext\n \n{\n\n    \n_\n.\nbufferTiles\n(\nweightMatrix\n.\nextent\n)\n\n      \n.\nmapValues\n \n{\n \ntileWithCtx\n \n=\n\n        \ntileWithCtx\n.\ntile\n\n          \n.\nfocalSum\n(\nweightMatrix\n,\n \nSome\n(\ntileWithCtx\n.\ntargetArea\n))\n \n// focal op.\n\n          \n.\nmapDouble\n \n{\n \nx\n \n=\n \n(\nx\n \n-\n \nA\n)\n \n/\n \nB\n \n}\n \n// local op.\n\n      \n}\n\n  \n}\n\n\n}\n\n\n\n\n\nLet's assume, we already have the following variables:\n\n\n\n\nlayerReader\n: helper class to query tiles from geotrellis catalog/layer,\n\n\nlayerId\n: ID of the raster layer used as input raster,\n\n\nkernelRadius\n: size of the weight matrix (how many pixels)\n\n\n\n\n// RDD (distributed dataset from Apache Spark) representing all tiles in the layer \n\n\nval\n \nqueryResult\n:\n \nSpatialRDD\n \n=\n\n  \nlayerReader\n.\nread\n[\nSpatialKey\n, \nTile\n, \nTileLayerMetadata\n[\nSpatialKey\n]](\nlayerId\n)\n\n\n\n// here, we use a circular kernel as a weight matrix\n\n\nval\n \nweightMatrix\n \n=\n \nKernel\n.\ncircle\n(\nkernelRadius\n,\n\n                                 \nqueryResult\n.\nmetadata\n.\ncellwidth\n,\n\n                                 \nkernelRadius\n)\n\n\n\n// use precomputed histogram metadata (stored in zoom level 0 inside the layer)\n\n\nval\n \nstats\n \n=\n \nqueryResult\n.\nhistogram\n.\nstatistics\n\n\nrequire\n(\nstats\n.\nnonEmpty\n)\n\n\n\nval\n \nStatistics\n(\n_\n,\n \nglobMean\n,\n \n_\n,\n \n_\n,\n \nglobStdev\n,\n \n_\n,\n \n_\n)\n \n=\n \nstats\n.\nget\n\n\nval\n \nnumPixels\n \n=\n \nqueryResult\n.\nhistogram\n.\ntotalCount\n\n\n\n// apply the parallelized getis ord\n\n\nval\n \noutRdd\n \n=\n \ngetisord\n(\nqueryResult\n,\n \nweightMatrix\n,\n \nglobMean\n,\n \nglobStdev\n,\n \nnumPixels\n)\n\n\n\n\n\nThe result \noutRdd\n is an RDD (distributed dataset from Apache Spark) that can be further processed\nor stored as a new layer in geotrellis catalog.", 
            "title": "Hotspot analysis (using G*)"
        }, 
        {
            "location": "/demos/hotspot_analysis/#hotspot-analysis-using-g", 
            "text": "", 
            "title": "Hotspot analysis (using G*)"
        }, 
        {
            "location": "/demos/hotspot_analysis/#problem-definition", 
            "text": "We have a map, in this case a map of land surface temperatures  We want to find hotspots, i.e., areas on map that are  significantly different from their surrounding area .  We want to use Getis-Ord G* statistic as the tool for finding the hotspots  see section  Standards Getis-ord    We want to parallelize the computation in our Spark cluster.  see section  Rasterized Getis-ord", 
            "title": "Problem definition"
        }, 
        {
            "location": "/demos/hotspot_analysis/#hotspot-analysis-using-geotrellis", 
            "text": "In this section we show a simplified version of the hotspot analysis.\nWe use the  Geotrellis  library to achieve the parallelization.\nSome assumptions are:   we use 2-dimenational data (only the spatial part without the time component)  we store our data as a layer of tiles in geotrellis catalog (distributed raster)  our hotspot analysis uses the standard G* with variable window   First, we need to express the G* formula in terms of the map algebra operations.", 
            "title": "Hotspot analysis using geotrellis"
        }, 
        {
            "location": "/demos/hotspot_analysis/#scala-code", 
            "text": "Full source code can be found in our github repository:  biggis-project/biggis-landuse  // typical type definition used by geotrellis  type   SpatialRDD   =   RDD [( SpatialKey ,  Tile )] \n                   with   Metadata [ TileLayerMetadata [ SpatialKey ]]  def   getisord ( rdd :   SpatialRDD ,   weightMatrix :   Kernel , \n              globMean : Double ,   globStdev : Double ,   numPixels : Long ) :   SpatialRDD   =   { \n\n   val   wcells :   Array [ Double ]   =   weightMatrix . tile . toArrayDouble \n   val   sumW :   Double   =   wcells . sum \n   val   sumW2 :   Double   =   wcells . map ( x   =   x   *   x ). sum \n\n   // variables used in the getis-ord formula \n   val   A :   Double   =   globalMean   *   sumW \n   val   B :   Double   =   globalStdev   *   Math . sqrt (( numPixels   *   sumW2   -   sumW   *   sumW )   /   ( numPixels   -   1 )) \n\n   rdd . withContext   { \n     _ . bufferTiles ( weightMatrix . extent ) \n       . mapValues   {   tileWithCtx   = \n         tileWithCtx . tile \n           . focalSum ( weightMatrix ,   Some ( tileWithCtx . targetArea ))   // focal op. \n           . mapDouble   {   x   =   ( x   -   A )   /   B   }   // local op. \n       } \n   }  }   Let's assume, we already have the following variables:   layerReader : helper class to query tiles from geotrellis catalog/layer,  layerId : ID of the raster layer used as input raster,  kernelRadius : size of the weight matrix (how many pixels)   // RDD (distributed dataset from Apache Spark) representing all tiles in the layer   val   queryResult :   SpatialRDD   = \n   layerReader . read [ SpatialKey ,  Tile ,  TileLayerMetadata [ SpatialKey ]]( layerId )  // here, we use a circular kernel as a weight matrix  val   weightMatrix   =   Kernel . circle ( kernelRadius , \n                                  queryResult . metadata . cellwidth , \n                                  kernelRadius )  // use precomputed histogram metadata (stored in zoom level 0 inside the layer)  val   stats   =   queryResult . histogram . statistics  require ( stats . nonEmpty )  val   Statistics ( _ ,   globMean ,   _ ,   _ ,   globStdev ,   _ ,   _ )   =   stats . get  val   numPixels   =   queryResult . histogram . totalCount  // apply the parallelized getis ord  val   outRdd   =   getisord ( queryResult ,   weightMatrix ,   globMean ,   globStdev ,   numPixels )   The result  outRdd  is an RDD (distributed dataset from Apache Spark) that can be further processed\nor stored as a new layer in geotrellis catalog.", 
            "title": "Scala code"
        }, 
        {
            "location": "/demos/invasive-spec/", 
            "text": "Responsible person for this section\n\n\n\n\nHannes M\u00fcller (LUBW)\n\n\nJohannes Kutterer (Disy)\n\n\nDaniel Seebacher (Uni Konstanz)\n\n\n\n\n\n\nInvasive species\n\n\nMotiviation\n\n\nInvasive species are a major cause of ecological damage and commercial losses. A current problem spreading in North America and Europe is the vinegar fly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and healthy fruits and is therefore of concern to fruit growers, such as vintners.  Consequently, large amounts of data about the occurrence of D. suzukii have been collected in recent years. However, there is a lack of interactive methods to investigate this data. \n\n\nUsed Data Sources\n\n\n\n\nATKIS/ALKIS\n\n\nVitimeteo\n\n\nASTER Elevation Map\n\n\n\n\nData Description\n\n\nIn the data provided by VitiMeteo are, among other things, observations of the spread of D. suzukii. This data consists of trap findings of D. suzukii as well as percentage information about how many berries were infested in a sample taken at the station. Additionally, there is percentage information about how many eggs were found in a sample. This percentage can be over 100 %, if there are more egg findings than berries in a sample. These observations are collected from 867 stations non-uniformly spread over Baden-Wuerttemberg. Some of them only report observations for one day, others report multiple observations over a time period of up to 1641 days. The observations are rather sparse and irregularly sampled, which makes the use of standard time series analysis techniques challenging, if not impossible. Consequently, an interactive visual analysis should enable researchers to interactively analyze this complex data source. \n\n\nPrediction of Infested Areas\n\n\nWe enriched the data provided by VitiMeteo, by adding information about the environmental surroundings of each station. First, we added the height information, which we extracted from ASTER. Second, we added the surrounding land usage information. Since a local spread is possible by D. suzukii itself, we extracted the land usage information in a 5~km radius around each station. Finally, we have an 85 dimensional feature vector for each instance, consisting of the month of the year, the station height, and the surrounding land usage.\n\n\nWe end up with a rather imbalanced data set with four times as many negative examples as positive ones. This can cause problems since many machine learning algorithms depend on the assumption that the given data set is balanced. Although machine learning techniques exist which can deal with imbalanced data sets, such as the Robust Decision Trees, we want to employ ensemble-based classification, which is a combination of different classifiers. This allows us to improve the classification performance and also to model the uncertainty of our classification, which aids people in making more informed decisions. This requires the creation of a balanced data set, which we can achieve by either using undersampling of the majority class or oversampling of the minority class. Undersampling can be achieved by stratified sampling using the occurrence class as strata. However, this would remove instances from our already small data set. To avoid this, we employ oversampling of the minority class using the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE picks pairs of nearest neighbors in the minority class and creates artificial instances by randomly placing a point on the line between the nearest neighbors until the data is balanced. Thus, allowing us to employ default machine learning algorithms. \n\n\nHypothesenentwicklung\n\n\nHypothesenentwicklung zur Vermehrung der KEF aufgrund biologischer Erkenntnisse\n(v.a. abh\u00e4ngig von Umgebungstemperatur und Vegetation)\n\n\nEntwicklung einer Vektordatenpipeline in BigGIS\n\n\n\n\nDatenquelle \nwww.vitimeteo.de\n\n\nDatensammlung (kafka)\n\n\nProzessierung (flink)\n\n\nVisualisierung (Uni-Konstanz)\n\n\n\n\nDrosophigator Prototype for the Visual Analysis of Spatio-Temporal Event Predictions\n\n\n\n\nInvestigating the Spread Dynamics of invasive species\n\n\nVDS and now journal extension\n\n\nEnsemble-Based Classification of Infested Areas\n\n\nVisual Analysis \n\n\nDemonstration and Evaluation at the 6\nth\n workshop of the working group D Suzukii in Bad Kreuznach\n\n\n\n\nResults\n\n\n\n\nH\u00e4ufiges Auftreten der KEF\n\n\nN\u00e4he zu Wald -\n Refugium f\u00fcr Kirschessigfliege zum \u00dcberleben bei widrigen Wetterbedingungen\n\n\n\n\nRelated Scenarios\n\n\n\n\nEnvironment", 
            "title": "Invasive species"
        }, 
        {
            "location": "/demos/invasive-spec/#invasive-species", 
            "text": "", 
            "title": "Invasive species"
        }, 
        {
            "location": "/demos/invasive-spec/#motiviation", 
            "text": "Invasive species are a major cause of ecological damage and commercial losses. A current problem spreading in North America and Europe is the vinegar fly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and healthy fruits and is therefore of concern to fruit growers, such as vintners.  Consequently, large amounts of data about the occurrence of D. suzukii have been collected in recent years. However, there is a lack of interactive methods to investigate this data.", 
            "title": "Motiviation"
        }, 
        {
            "location": "/demos/invasive-spec/#used-data-sources", 
            "text": "ATKIS/ALKIS  Vitimeteo  ASTER Elevation Map", 
            "title": "Used Data Sources"
        }, 
        {
            "location": "/demos/invasive-spec/#data-description", 
            "text": "In the data provided by VitiMeteo are, among other things, observations of the spread of D. suzukii. This data consists of trap findings of D. suzukii as well as percentage information about how many berries were infested in a sample taken at the station. Additionally, there is percentage information about how many eggs were found in a sample. This percentage can be over 100 %, if there are more egg findings than berries in a sample. These observations are collected from 867 stations non-uniformly spread over Baden-Wuerttemberg. Some of them only report observations for one day, others report multiple observations over a time period of up to 1641 days. The observations are rather sparse and irregularly sampled, which makes the use of standard time series analysis techniques challenging, if not impossible. Consequently, an interactive visual analysis should enable researchers to interactively analyze this complex data source.", 
            "title": "Data Description"
        }, 
        {
            "location": "/demos/invasive-spec/#prediction-of-infested-areas", 
            "text": "We enriched the data provided by VitiMeteo, by adding information about the environmental surroundings of each station. First, we added the height information, which we extracted from ASTER. Second, we added the surrounding land usage information. Since a local spread is possible by D. suzukii itself, we extracted the land usage information in a 5~km radius around each station. Finally, we have an 85 dimensional feature vector for each instance, consisting of the month of the year, the station height, and the surrounding land usage.  We end up with a rather imbalanced data set with four times as many negative examples as positive ones. This can cause problems since many machine learning algorithms depend on the assumption that the given data set is balanced. Although machine learning techniques exist which can deal with imbalanced data sets, such as the Robust Decision Trees, we want to employ ensemble-based classification, which is a combination of different classifiers. This allows us to improve the classification performance and also to model the uncertainty of our classification, which aids people in making more informed decisions. This requires the creation of a balanced data set, which we can achieve by either using undersampling of the majority class or oversampling of the minority class. Undersampling can be achieved by stratified sampling using the occurrence class as strata. However, this would remove instances from our already small data set. To avoid this, we employ oversampling of the minority class using the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE picks pairs of nearest neighbors in the minority class and creates artificial instances by randomly placing a point on the line between the nearest neighbors until the data is balanced. Thus, allowing us to employ default machine learning algorithms.", 
            "title": "Prediction of Infested Areas"
        }, 
        {
            "location": "/demos/invasive-spec/#hypothesenentwicklung", 
            "text": "Hypothesenentwicklung zur Vermehrung der KEF aufgrund biologischer Erkenntnisse\n(v.a. abh\u00e4ngig von Umgebungstemperatur und Vegetation)", 
            "title": "Hypothesenentwicklung"
        }, 
        {
            "location": "/demos/invasive-spec/#entwicklung-einer-vektordatenpipeline-in-biggis", 
            "text": "Datenquelle  www.vitimeteo.de  Datensammlung (kafka)  Prozessierung (flink)  Visualisierung (Uni-Konstanz)", 
            "title": "Entwicklung einer Vektordatenpipeline in BigGIS"
        }, 
        {
            "location": "/demos/invasive-spec/#drosophigator-prototype-for-the-visual-analysis-of-spatio-temporal-event-predictions", 
            "text": "Investigating the Spread Dynamics of invasive species  VDS and now journal extension  Ensemble-Based Classification of Infested Areas  Visual Analysis   Demonstration and Evaluation at the 6 th  workshop of the working group D Suzukii in Bad Kreuznach", 
            "title": "Drosophigator Prototype for the Visual Analysis of Spatio-Temporal Event Predictions"
        }, 
        {
            "location": "/demos/invasive-spec/#results", 
            "text": "H\u00e4ufiges Auftreten der KEF  N\u00e4he zu Wald -  Refugium f\u00fcr Kirschessigfliege zum \u00dcberleben bei widrigen Wetterbedingungen", 
            "title": "Results"
        }, 
        {
            "location": "/demos/invasive-spec/#related-scenarios", 
            "text": "Environment", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/demos/landuse/", 
            "text": "Landuse classification\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/biggis-landuse\n\n\n\n\nProblem definition\n\n\n\n\nWe have the following datasets:\n\n\nAn existing Landuse vector dataset\n\n\nOrthorectified Aerial images (Digital Ortho Photos = DOP) \n\n\nSatellite images (SAT), e.g. Sentinel2 10m\n\n\n\n\n\n\nWe want to select / extract landcover classes from landuse classes.\n\n\nWe want to use Support Vector Machines (or other Machine Learning Classifiers) for classifying landcover classes.\n\n\nsee section \nSupport Vector Machines\n\n\n\n\n\n\nWe want to support Multiple classes using a One versus All (OvA) strategy or One vs. Rest.\n\n\nsee section \nOne vs. Rest\n\n\n\n\n\n\nWe want to parallelize the computation in our Spark cluster using \nGeotrellis\n for data loading and export.\n\n\n\n\n\n\nResponsible person for this section\n\n\nAdrian Klink (EFTAS)\n\n\n\n\n\n\nTodo\n\n\n\n\nDescribe the idea\n\n\nMaybe add some geotrellis examples\n\n\n\n\n\n\nClassification of Aerial Images according to Land Use Classes (using Land Cover Classes as intermediate)\n\n\nTools\n\n\n\n\nMachine Learning\n\n\nTraining: Multiclass SVM\n\n\nGeotrellis\n\n\n\n\nScala code snippets\n\n\ntype\n \nSpatialMultibandRDD\n \n=\n \nRDD\n[(\nSpatialKey\n, \nMultibandTile\n)]\n \nwith\n \nMetadata\n[\nTileLayerMetadata\n[\nSpatialKey\n]]\n\n\n\n// reading from Hadoop Layer (HDFS)\n\n\nval\n \nrdd\n \n:\n \nSpatialMultibandRDD\n \n=\n \nbiggis\n.\nlanduse\n.\napi\n.\nreadRddFromLayer\n(\nLayerId\n(\nlayerName\n,\n \nzoom\n))\n\n\n\n// writing to Hadoop Layer (HDFS)\n\n\nbiggis\n.\nlanduse\n.\napi\n.\nwriteRddToLayer\n(\nrdd\n,\n \nLayerId\n(\nlayerName\n,\n \nzoom\n))\n\n\n\n\n\nExample\n\n\n\n\nClassification of Aerial Images May-Aug 2016\n\n\nLayerstacking: Aerial Images + Satellite images (IR, Resolution 2m)\n\n\nTraining of a Multiclass SVM with manually selected training data (classified image tiles)\n\n\n\n\nFurther Steps\n\n\n\n\nAdding additional Layers, e.g.\n\n\nTerrain Height\n\n\nHomogeneity of Texture\n\n\nUsing Other Classififiers\n\n\n\n\nRelated Scenarios\n\n\n\n\nEnvironment\n\n\nSmart City", 
            "title": "Landuse classification"
        }, 
        {
            "location": "/demos/landuse/#landuse-classification", 
            "text": "Note  Related repository is  https://github.com/biggis-project/biggis-landuse", 
            "title": "Landuse classification"
        }, 
        {
            "location": "/demos/landuse/#problem-definition", 
            "text": "We have the following datasets:  An existing Landuse vector dataset  Orthorectified Aerial images (Digital Ortho Photos = DOP)   Satellite images (SAT), e.g. Sentinel2 10m    We want to select / extract landcover classes from landuse classes.  We want to use Support Vector Machines (or other Machine Learning Classifiers) for classifying landcover classes.  see section  Support Vector Machines    We want to support Multiple classes using a One versus All (OvA) strategy or One vs. Rest.  see section  One vs. Rest    We want to parallelize the computation in our Spark cluster using  Geotrellis  for data loading and export.    Responsible person for this section  Adrian Klink (EFTAS)    Todo   Describe the idea  Maybe add some geotrellis examples", 
            "title": "Problem definition"
        }, 
        {
            "location": "/demos/landuse/#classification-of-aerial-images-according-to-land-use-classes-using-land-cover-classes-as-intermediate", 
            "text": "", 
            "title": "Classification of Aerial Images according to Land Use Classes (using Land Cover Classes as intermediate)"
        }, 
        {
            "location": "/demos/landuse/#tools", 
            "text": "Machine Learning  Training: Multiclass SVM  Geotrellis", 
            "title": "Tools"
        }, 
        {
            "location": "/demos/landuse/#scala-code-snippets", 
            "text": "type   SpatialMultibandRDD   =   RDD [( SpatialKey ,  MultibandTile )]   with   Metadata [ TileLayerMetadata [ SpatialKey ]]  // reading from Hadoop Layer (HDFS)  val   rdd   :   SpatialMultibandRDD   =   biggis . landuse . api . readRddFromLayer ( LayerId ( layerName ,   zoom ))  // writing to Hadoop Layer (HDFS)  biggis . landuse . api . writeRddToLayer ( rdd ,   LayerId ( layerName ,   zoom ))", 
            "title": "Scala code snippets"
        }, 
        {
            "location": "/demos/landuse/#example", 
            "text": "Classification of Aerial Images May-Aug 2016  Layerstacking: Aerial Images + Satellite images (IR, Resolution 2m)  Training of a Multiclass SVM with manually selected training data (classified image tiles)", 
            "title": "Example"
        }, 
        {
            "location": "/demos/landuse/#further-steps", 
            "text": "Adding additional Layers, e.g.  Terrain Height  Homogeneity of Texture  Using Other Classififiers", 
            "title": "Further Steps"
        }, 
        {
            "location": "/demos/landuse/#related-scenarios", 
            "text": "Environment  Smart City", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/", 
            "text": "Optical Remote Sensing\n\n\nThe starting point for the BOS scenario in the BigGIS frame is the satellite-based emergency management services of Copernicus or the charter \u201cSpace and major Disasters\u201d. The idea was to implement similar sensors on an unmanned aerial vehicle (UAV) platform and bring it to smaller incidents like larger fires or CBRN. Therefore, thermal (IR) and hyperspectral cameras was used as well as RGB cameras to do some testing in simulated situations:\n\n\n\n\nDetection and following smoke clouds in imageries\n\n\nDetection of \u201cnon visible\u201d gas clouds\n\n\nIdentification of \u201cchemicals\u201d\n\n\n\n\nGas Cloud Detection\n\n\nTo perform the simulations several test scenarios were prepared in two campaigns in Karlsruhe and Dortmund. The smoke of Heptane (UN 1206/Kemmler 33) was recorded as well as a mixture from gasoline (1203/33) and diesel (1202/30). A gas leakage was simulated at the Dortmund Fire Brigade Education Center using Methane (\n\\(CH_4\\)\n; 1971/23). And a gas cloud containing \u201cchemicals\u201d was simulated by a fog machine which nebulized a 50 % mixture of propylene glycol (propane-1,2-diol) and chlorophyll from the food branch.\nFirst analysis eg. for the \u201cunvisible\u201d Methane gas cloud show quite good results using the IR cameras. On UAVs offered by Sitebots and AI Drones two choices of cameras were used:\n\n\n\n\nOPTRIS PI\n\n\nFLIR Vue Pro R\n\n\n\n\nBoth cameras give the radiometric signatures and not just \u201ccolored pictures\u201d. The Images were spatially referenced by standard procedures. It was found that building differences just show intereferences in the pictures (shown in the first row of Picture 1). Good results were given by a Halcon referencing based on sub pixel accuracy (row two in Picture 1). Using difference analysis on about 25 pictures show a clear signature of the exhaling methane (row three in Picture 1).\n\n\nPicture 1: Gas cloud detection using thermal imageries\n\n\n\n\u00a0\n\n\nSpectral Analysis of Gas Clouds\n\n\nThe idea of remotely detecting and identifying chemicals using a hyperspectral sensor is not new. The so called Analytical Task Force (ATF; \nhttps://www.bbk.bund.de/DE/AufgabenundAusstattung/CBRNSchutz/TaskForce/ATF_einstieg1.html\n ) is using the Van-based RAMAN spectroscope SIGIS 2 to detect and identify chemicals in CBRN incidents.\n\n\nThe BigGIS project intended to be more flexible than a SIGIS 2 mounted in a car. Therefore, as a proof-of-concept the implementation of a system allowing for spectral analysis of gas and aerosol clouds mounted on a UAV was subject of study on the level of a proof-of-concept. Due to the fact that multispectral IR-sensors as they are utilized in the SIGIS 2 system require relatively heavy-weight cooling units disqualify these systems for the usage with a UAV. Therefore, within the project a multispectral sensor sensitive in the spectral range of visible light and near IR (450 - 950 nm) was utilized for spectral analyzation. The sensor Cubert 185 UHD Firefly (\nhttp://cubert-gmbh.com/uhd-185-firefly/\n) has a weight of about 500 g and could easily be mounted on a UAV.\n\n\nUsing this sensor \u201csmoke clouds\u201d from a mixture of \u201cDisco fog\u201d and chlorophyll (see below) were recorded. Each pixel in the image contains the spectral information of the reflected light spread over 125 bands ranging from a minimum wavelength of 450nm to a maximum of 950nm.\n\n\nTriangular Chlorophyll Index\n\n\nIn a first Analysis the propagating chlorophyll cloud was identified in the recorded image via the calculation of the \nTriangular Chlorophyll Index (TCI)\n for each pixel in the spatially referenced image.\n\n\nThe result is shown in the picture below:\nRow one in Picture 2 is showing the chlorophyll cloud propagating from west\nto east through spatially referenced pseudo color pictures. Interesting\nis the underground partly paved and partly consisted of a grass strip.\nIn the right picture the cloud covers a small\ntree.\n\n\nThe TCI algorithm was applied on the three pictures in the middle row.\nUsing a reference aerial photograph and building the difference to such\nan image (third row) one can find chlorophyll only detected on the\nasphalt, not on the green strip or the tree due to the fact that the\nmethod cannot distinguish chlorophyll from the plants from\nchlorophyll of the cloud.\n\n\n\n\nTodo\n\n\nWie ist die obige Erkl\u00e4rung zu verstehen? Geht es hier darum die Bereiche mit zeitlich konstant hohem TCI-Wert (= nicht die Chlorophyll-Wolke) auszuschlie\u00dfen?\n\n\n\n\n\n\nPicture 3 now shows the composition of the referenced and analyzed\npictures with all three stages of the moving chlorophyll cloud. One now can\nsee the grass strip and the tree in addition to the gas cloud over the\nasphalt.\n\n\n\n\nTodo\n\n\n\n\nPicture 3: eingef\u00e4rbte Elemente detailierter erkl\u00e4ren.\n\n\n\n\n\n\n\n\nLogistic Regression Classification of Cloud Reflectance\n\n\nA second experiment set was dedicated to a more general evaluation of cloud constituents. Here, the identification of constituents via logistic regression classification bases on the whole spectral range of the Cubert 185 UHD Firefly, in contrary to the previous example where the identification based on only three wavelengths.\n\n\nAs a proof of concept, multispectral images of clouds produced by a fog generator fueled with two different fog fluids and solutions of each fog fluid mixed with defined proportions of chlorophyll (TODO: fog fluids genauer spezifizieren und chlorophyll-l\u00f6sung) were recorded.\n\n\nIn the experimental setup the Cubert 185 UHD Firefly was positioned facing a white wall in 1.2 m distance as constant background. The fog generator was placed between camera and wall in such a way, that the ejected cloud passed the camera in roughly 0.6 m distance while filling the whole recorded image plane. Immediately before each measurement set the incident intensity (white-balance intensity) in front of the camera was recorded by capturing the reflectance of a spectralon coated sheet at a distance of 0.6 m. By dividing the recorded reflected cloud intensities by the white-balance intensity the cloud reflectance was determined.\n\n\n\n\n\nThe reflectance spectra of evaporated mixtures of fog fluid with chlorophyll not in all cases show the typical chlorophyll absorption minimum. Consequently, chlorophyll is not uniformly evaporated with the given setup but ejected in irregular chlorophyll bursts instead. Therefore, a TCI pre-evaluation was carried through on each pixel of all recordings of clouds of evaporated chlorophyll mixtures, in order to label positive chlorophyll spectra as training and test data for the logistic regression. After spectra inspection of several samples a threshold of TCI = 0.05 was chosen above which the spectra was labeled chlorophyll-containing. Spectra of evaporated chlorophyll mixtures with TCI \n 0.05 were not regarded in the further analysis.\nThe spectra of clouds of evaporated pure fog fluids provided the negative chlorophyll data. The table below shows the count of spectra for the different cloud categories that was used for training for the logistic regression classifiers in the next subsections.\n\n\n\n\n\n\n\n\n\n\nFog Fluid 1\n\n\nFog Fluid 2\n\n\n\n\n\n\n\n\n\n\nNo Chlorophyll\n\n\n5000\n\n\n7500\n\n\n\n\n\n\nChlorophyll\n\n\n3926\n\n\n68541\n\n\n\n\n\n\n\n\nThe test data set comprised the following sample sizes:\n\n\n\n\n\n\n\n\n\n\nFog Fluid 1\n\n\nFog Fluid 2\n\n\n\n\n\n\n\n\n\n\nNo Chlorophyll\n\n\n2500\n\n\n2500\n\n\n\n\n\n\nChlorophyll\n\n\n2500\n\n\n2463\n\n\n\n\n\n\n\n\nBased on this data different logistic regression classifiers were trained.\n\n\nChlorophyll vs. Non-Chlorophyll\n\n\n\n\nFirst a logistic regression classifier was trained to distinguish between spectra of clouds containing chlorophyll and pure fog fluid, irrespective of the type of the fog fluid. The figure above displays typical spectra for a cloud aof pure fog fluid and a cloud containing chlorophyll. In the latter case, the reflectance minimum due to the absorption maximum of chlorophyll can clearly be seen in the curve around channel 45. This also is the region where the variable importance (i.e. value of the t\u2013statistic for each model parameter (= channel)) peaks and marks the most significant channels of the classifier.\n\n\nThe training accuracy was found to be\n$$\nAcc_{train} = \\frac{true Positives + true Negatives}{Positives + Negatives} = 1.\n$$\n\n\nThe out-of-sample test also showed very reliable results:\n\n\nTest samples with fog fluid 1:\n\n\n\n\n\n\n\n\n\n\nClassified No-Chloro.\n\n\nClassified Chloro.\n\n\n\n\n\n\n\n\n\n\nSample No-Chloro.\n\n\n2500\n\n\n0\n\n\n\n\n\n\nSample Chloro.\n\n\n0\n\n\n2500\n\n\n\n\n\n\n\n\n\\[\nAcc_{test} = 1\n\\]\n$$\nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1\n$$\nTest samples with fog fluid 2:\n\n\n\n\n\n\n\n\n\n\nClassified No-Chloro.\n\n\nClassified Chloro.\n\n\n\n\n\n\n\n\n\n\nSample No-Chloro.\n\n\n2500\n\n\n0\n\n\n\n\n\n\nSample Chloro.\n\n\n0\n\n\n2463\n\n\n\n\n\n\n\n\n\\[\nAcc_{test} = 1\n\\]\nThe high accuracy bases on the very clear feature of the chlorophyll absorption dip in the spectrum.\n\n\nPure Fog Fluid 1 vs. Fog Fluid 2\n\n\n\n\nWhile the spectra of chlorophyll containing clouds show a clear distinction feature against the non-chlorophyll containing spectra, the reflectance spectra of the two pure fog fluids show a similar relative pattern, while the absolute reflectance level of the fog fluid 1 seems to be elevated compared to fog fluid 2 (see figure above).\nTherefore, a logistic regression classifier was trained for the distinction of clouds of the two pure fog fluids. The training accuracy for this classifier was found to be:\n$$\nAcc_{train} = 1\n$$\n\n\nThe out of sample performance is as follows:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n2452\n\n\n48\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n1\n\n\n2499\n\n\n\n\n\n\n\n\n\\[\nAcc_{test} = 0.99\n\\]\n\\[\nF1_{test} = 0.99\n\\]\nDespite the lack of prominent characteristic features in the reflectance spectra the accuracy and the F1 score of the classifier is yet rather high.\nThat changes when the classifier is tested on reflectance data of mixtures of the different fog fluids with chlorophyll, as shown in the table below:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n0\n\n\n2500\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n0\n\n\n2463\n\n\n\n\n\n\n\n\n\\[\nAcc_{test} = 0.49\n\\]\n\\[\nF1_{test} = 0.66\n\\]\nObviously this classifier is not robust against the mixture of features of another substance in the cloud.\n\n\nFog Fluid 1 vs. Fog Fluid 2 (with and without Chlorophyll)\n\n\nTo overcome the weakness of the previous classifier another classifier for the distinction between different fog fluids was trained including the reflectance spectra of mixtures with chlorophyll. Here, the training accuracy was found to be:\n$$\nAcc_{train} = 1\n$$\n\n\nThe out of sample performance for clouds of pure fog fluids is shown in the table below:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n2500\n\n\n0\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n0\n\n\n2500\n\n\n\n\n\n\n\n\n\\[\nAcc_{test} = 1\n\\]\n\\[\nF1_{test} = 1\n\\]\nInterestingly, the performance on the test data of pure fog fluids is even slightly better when trained with data samples including mixtures with chlorophyll. This might be explained with overfitting in the case of training only with data of pure fog fluids (TODO diskutieren).\n\n\nThe out-of-sample performance for mixtures of the different fog fluids with chlorophyll is shown in the table below:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n2014\n\n\n486\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n11\n\n\n2452\n\n\n\n\n\n\n\n\n\\[\nAcc_{test} = 0.90\n\\]\n\\[\nF1_{test} = 0.90\n\\]\nThe performance is rather convincing even though the spectra of the fog fluids are overlayed with the characteristic chlorophyll spectrum.\n\n\nConclusion\n\n\nIt was shown that logistic regression can be used to identify constituents of clouds on basis of the reflectance spectra in the range of visible light with the here presented system, especially when strong characteristic features are present as in the case of chlorophyll. Besides that, even the identification of constituents with weaker characteristic features can be carried through as in the case for the distinction between the two fog fluids. For the latter case, the above findings suggest that the classifier for the constituent of interest should be carried through with data also regarding a variety of probable accompanying cloud constituents. Otherwise, the overlay of the different spectra might diminish the classifiers performance.", 
            "title": "Optical Remote Sensing"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/#optical-remote-sensing", 
            "text": "The starting point for the BOS scenario in the BigGIS frame is the satellite-based emergency management services of Copernicus or the charter \u201cSpace and major Disasters\u201d. The idea was to implement similar sensors on an unmanned aerial vehicle (UAV) platform and bring it to smaller incidents like larger fires or CBRN. Therefore, thermal (IR) and hyperspectral cameras was used as well as RGB cameras to do some testing in simulated situations:   Detection and following smoke clouds in imageries  Detection of \u201cnon visible\u201d gas clouds  Identification of \u201cchemicals\u201d", 
            "title": "Optical Remote Sensing"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/#gas-cloud-detection", 
            "text": "To perform the simulations several test scenarios were prepared in two campaigns in Karlsruhe and Dortmund. The smoke of Heptane (UN 1206/Kemmler 33) was recorded as well as a mixture from gasoline (1203/33) and diesel (1202/30). A gas leakage was simulated at the Dortmund Fire Brigade Education Center using Methane ( \\(CH_4\\) ; 1971/23). And a gas cloud containing \u201cchemicals\u201d was simulated by a fog machine which nebulized a 50 % mixture of propylene glycol (propane-1,2-diol) and chlorophyll from the food branch.\nFirst analysis eg. for the \u201cunvisible\u201d Methane gas cloud show quite good results using the IR cameras. On UAVs offered by Sitebots and AI Drones two choices of cameras were used:   OPTRIS PI  FLIR Vue Pro R   Both cameras give the radiometric signatures and not just \u201ccolored pictures\u201d. The Images were spatially referenced by standard procedures. It was found that building differences just show intereferences in the pictures (shown in the first row of Picture 1). Good results were given by a Halcon referencing based on sub pixel accuracy (row two in Picture 1). Using difference analysis on about 25 pictures show a clear signature of the exhaling methane (row three in Picture 1).  Picture 1: Gas cloud detection using thermal imageries", 
            "title": "Gas Cloud Detection"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/#spectral-analysis-of-gas-clouds", 
            "text": "The idea of remotely detecting and identifying chemicals using a hyperspectral sensor is not new. The so called Analytical Task Force (ATF;  https://www.bbk.bund.de/DE/AufgabenundAusstattung/CBRNSchutz/TaskForce/ATF_einstieg1.html  ) is using the Van-based RAMAN spectroscope SIGIS 2 to detect and identify chemicals in CBRN incidents.  The BigGIS project intended to be more flexible than a SIGIS 2 mounted in a car. Therefore, as a proof-of-concept the implementation of a system allowing for spectral analysis of gas and aerosol clouds mounted on a UAV was subject of study on the level of a proof-of-concept. Due to the fact that multispectral IR-sensors as they are utilized in the SIGIS 2 system require relatively heavy-weight cooling units disqualify these systems for the usage with a UAV. Therefore, within the project a multispectral sensor sensitive in the spectral range of visible light and near IR (450 - 950 nm) was utilized for spectral analyzation. The sensor Cubert 185 UHD Firefly ( http://cubert-gmbh.com/uhd-185-firefly/ ) has a weight of about 500 g and could easily be mounted on a UAV.  Using this sensor \u201csmoke clouds\u201d from a mixture of \u201cDisco fog\u201d and chlorophyll (see below) were recorded. Each pixel in the image contains the spectral information of the reflected light spread over 125 bands ranging from a minimum wavelength of 450nm to a maximum of 950nm.", 
            "title": "Spectral Analysis of Gas Clouds"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/#triangular-chlorophyll-index", 
            "text": "In a first Analysis the propagating chlorophyll cloud was identified in the recorded image via the calculation of the  Triangular Chlorophyll Index (TCI)  for each pixel in the spatially referenced image.  The result is shown in the picture below:\nRow one in Picture 2 is showing the chlorophyll cloud propagating from west\nto east through spatially referenced pseudo color pictures. Interesting\nis the underground partly paved and partly consisted of a grass strip.\nIn the right picture the cloud covers a small\ntree.  The TCI algorithm was applied on the three pictures in the middle row.\nUsing a reference aerial photograph and building the difference to such\nan image (third row) one can find chlorophyll only detected on the\nasphalt, not on the green strip or the tree due to the fact that the\nmethod cannot distinguish chlorophyll from the plants from\nchlorophyll of the cloud.   Todo  Wie ist die obige Erkl\u00e4rung zu verstehen? Geht es hier darum die Bereiche mit zeitlich konstant hohem TCI-Wert (= nicht die Chlorophyll-Wolke) auszuschlie\u00dfen?    Picture 3 now shows the composition of the referenced and analyzed\npictures with all three stages of the moving chlorophyll cloud. One now can\nsee the grass strip and the tree in addition to the gas cloud over the\nasphalt.   Todo   Picture 3: eingef\u00e4rbte Elemente detailierter erkl\u00e4ren.", 
            "title": "Triangular Chlorophyll Index"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/#logistic-regression-classification-of-cloud-reflectance", 
            "text": "A second experiment set was dedicated to a more general evaluation of cloud constituents. Here, the identification of constituents via logistic regression classification bases on the whole spectral range of the Cubert 185 UHD Firefly, in contrary to the previous example where the identification based on only three wavelengths.  As a proof of concept, multispectral images of clouds produced by a fog generator fueled with two different fog fluids and solutions of each fog fluid mixed with defined proportions of chlorophyll (TODO: fog fluids genauer spezifizieren und chlorophyll-l\u00f6sung) were recorded.  In the experimental setup the Cubert 185 UHD Firefly was positioned facing a white wall in 1.2 m distance as constant background. The fog generator was placed between camera and wall in such a way, that the ejected cloud passed the camera in roughly 0.6 m distance while filling the whole recorded image plane. Immediately before each measurement set the incident intensity (white-balance intensity) in front of the camera was recorded by capturing the reflectance of a spectralon coated sheet at a distance of 0.6 m. By dividing the recorded reflected cloud intensities by the white-balance intensity the cloud reflectance was determined.   The reflectance spectra of evaporated mixtures of fog fluid with chlorophyll not in all cases show the typical chlorophyll absorption minimum. Consequently, chlorophyll is not uniformly evaporated with the given setup but ejected in irregular chlorophyll bursts instead. Therefore, a TCI pre-evaluation was carried through on each pixel of all recordings of clouds of evaporated chlorophyll mixtures, in order to label positive chlorophyll spectra as training and test data for the logistic regression. After spectra inspection of several samples a threshold of TCI = 0.05 was chosen above which the spectra was labeled chlorophyll-containing. Spectra of evaporated chlorophyll mixtures with TCI   0.05 were not regarded in the further analysis.\nThe spectra of clouds of evaporated pure fog fluids provided the negative chlorophyll data. The table below shows the count of spectra for the different cloud categories that was used for training for the logistic regression classifiers in the next subsections.      Fog Fluid 1  Fog Fluid 2      No Chlorophyll  5000  7500    Chlorophyll  3926  68541     The test data set comprised the following sample sizes:      Fog Fluid 1  Fog Fluid 2      No Chlorophyll  2500  2500    Chlorophyll  2500  2463     Based on this data different logistic regression classifiers were trained.", 
            "title": "Logistic Regression Classification of Cloud Reflectance"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/#chlorophyll-vs-non-chlorophyll", 
            "text": "First a logistic regression classifier was trained to distinguish between spectra of clouds containing chlorophyll and pure fog fluid, irrespective of the type of the fog fluid. The figure above displays typical spectra for a cloud aof pure fog fluid and a cloud containing chlorophyll. In the latter case, the reflectance minimum due to the absorption maximum of chlorophyll can clearly be seen in the curve around channel 45. This also is the region where the variable importance (i.e. value of the t\u2013statistic for each model parameter (= channel)) peaks and marks the most significant channels of the classifier.  The training accuracy was found to be\n$$\nAcc_{train} = \\frac{true Positives + true Negatives}{Positives + Negatives} = 1.\n$$  The out-of-sample test also showed very reliable results:  Test samples with fog fluid 1:      Classified No-Chloro.  Classified Chloro.      Sample No-Chloro.  2500  0    Sample Chloro.  0  2500     \\[\nAcc_{test} = 1\n\\] $$\nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1\n$$\nTest samples with fog fluid 2:      Classified No-Chloro.  Classified Chloro.      Sample No-Chloro.  2500  0    Sample Chloro.  0  2463     \\[\nAcc_{test} = 1\n\\] The high accuracy bases on the very clear feature of the chlorophyll absorption dip in the spectrum.", 
            "title": "Chlorophyll vs. Non-Chlorophyll"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/#pure-fog-fluid-1-vs-fog-fluid-2", 
            "text": "While the spectra of chlorophyll containing clouds show a clear distinction feature against the non-chlorophyll containing spectra, the reflectance spectra of the two pure fog fluids show a similar relative pattern, while the absolute reflectance level of the fog fluid 1 seems to be elevated compared to fog fluid 2 (see figure above).\nTherefore, a logistic regression classifier was trained for the distinction of clouds of the two pure fog fluids. The training accuracy for this classifier was found to be:\n$$\nAcc_{train} = 1\n$$  The out of sample performance is as follows:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  2452  48    Sample Fog Fluid 2  1  2499     \\[\nAcc_{test} = 0.99\n\\] \\[\nF1_{test} = 0.99\n\\] Despite the lack of prominent characteristic features in the reflectance spectra the accuracy and the F1 score of the classifier is yet rather high.\nThat changes when the classifier is tested on reflectance data of mixtures of the different fog fluids with chlorophyll, as shown in the table below:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  0  2500    Sample Fog Fluid 2  0  2463     \\[\nAcc_{test} = 0.49\n\\] \\[\nF1_{test} = 0.66\n\\] Obviously this classifier is not robust against the mixture of features of another substance in the cloud.", 
            "title": "Pure Fog Fluid 1 vs. Fog Fluid 2"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/#fog-fluid-1-vs-fog-fluid-2-with-and-without-chlorophyll", 
            "text": "To overcome the weakness of the previous classifier another classifier for the distinction between different fog fluids was trained including the reflectance spectra of mixtures with chlorophyll. Here, the training accuracy was found to be:\n$$\nAcc_{train} = 1\n$$  The out of sample performance for clouds of pure fog fluids is shown in the table below:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  2500  0    Sample Fog Fluid 2  0  2500     \\[\nAcc_{test} = 1\n\\] \\[\nF1_{test} = 1\n\\] Interestingly, the performance on the test data of pure fog fluids is even slightly better when trained with data samples including mixtures with chlorophyll. This might be explained with overfitting in the case of training only with data of pure fog fluids (TODO diskutieren).  The out-of-sample performance for mixtures of the different fog fluids with chlorophyll is shown in the table below:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  2014  486    Sample Fog Fluid 2  11  2452     \\[\nAcc_{test} = 0.90\n\\] \\[\nF1_{test} = 0.90\n\\] The performance is rather convincing even though the spectra of the fog fluids are overlayed with the characteristic chlorophyll spectrum.", 
            "title": "Fog Fluid 1 vs. Fog Fluid 2 (with and without Chlorophyll)"
        }, 
        {
            "location": "/demos/opticalRemoteSensing/#conclusion", 
            "text": "It was shown that logistic regression can be used to identify constituents of clouds on basis of the reflectance spectra in the range of visible light with the here presented system, especially when strong characteristic features are present as in the case of chlorophyll. Besides that, even the identification of constituents with weaker characteristic features can be carried through as in the case for the distinction between the two fog fluids. For the latter case, the above findings suggest that the classifier for the constituent of interest should be carried through with data also regarding a variety of probable accompanying cloud constituents. Otherwise, the overlay of the different spectra might diminish the classifiers performance.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/demos/optimize-drones/", 
            "text": "Responsible person for this section\n\n\nKatharina Glock\n\n\n\n\nOptimal flight plan for drones\n\n\n\n\nTodo\n\n\n\n\ndescribe the idea\n\n\nadd some images\n\n\nadd some links to related papers\n\n\nadd links to related github repos\n\n\nadd links to related scenarios", 
            "title": "Optimal flight plan for drones"
        }, 
        {
            "location": "/demos/optimize-drones/#optimal-flight-plan-for-drones", 
            "text": "Todo   describe the idea  add some images  add some links to related papers  add links to related github repos  add links to related scenarios", 
            "title": "Optimal flight plan for drones"
        }, 
        {
            "location": "/demos/optimize-sensors/", 
            "text": "Responsible person for this section\n\n\nKatharina Glock\n\n\n\n\nPlacement of sensors under uncertainty\n\n\n\n\nTodo\n\n\n\n\ndescribe the idea\n\n\nadd some images\n\n\nadd some links to related papers\n\n\nadd links to related github repos\n\n\nadd links to related scenarios", 
            "title": "Placement of sensors under uncertainty"
        }, 
        {
            "location": "/demos/optimize-sensors/#placement-of-sensors-under-uncertainty", 
            "text": "Todo   describe the idea  add some images  add some links to related papers  add links to related github repos  add links to related scenarios", 
            "title": "Placement of sensors under uncertainty"
        }, 
        {
            "location": "/demos/stability_of_hotspots/", 
            "text": "Responsible person for this section\n\n\nMarc Gassenschmidt\n\n\n\n\nStability of hotspots\n\n\n\n\nTodo\n\n\n\n\ncomparison of different metrics (soh, jaccard, ...)\n\n\ncomparison of different methods (gstar, focalgstar, ...)", 
            "title": "Stability of hotspots"
        }, 
        {
            "location": "/demos/stability_of_hotspots/#stability-of-hotspots", 
            "text": "Todo   comparison of different metrics (soh, jaccard, ...)  comparison of different methods (gstar, focalgstar, ...)", 
            "title": "Stability of hotspots"
        }, 
        {
            "location": "/demos/urban-heat-islands/", 
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nUrban Heat Islands\n\n\n\n\nTodo\n\n\n\n\nTranslate to English\n\n\nadd links to related github repos\n\n\nadd some images (but not too many)\n\n\nadd links to related papers\n\n\ndescribe APIs especially from the end-users' point of view\n\n\n\n\n\n\n\n\nTemperaturinseln in Karlsruhe und anderen St\u00e4dten\n\n\nTemperaturdaten: Volunteered geographic data (z.B. wunderground.com)\n\n\n\n\nKorrelation zwischen Bereichen in verschiedenen St\u00e4dten mit \u00e4hnlichem Temperaturverlauf -\n Zugang zu Ursachen f\u00fcr Temperaturentwicklung\n\n\n\n\n\n\nVorstellung der Heat-Islands-Analyse\n\n\n\n\nWetterstationen\n\n\nTechnik: Sensebox (\nhttps://sensebox.de\n)\n\n\nBeispielstation (\nhttps://opensensemap.org/explore/58b4354fe53e0b001251119d\n)\n\n\nHotspotanalyse (SoH, Stability of Hotspots):\n\n\nAbh\u00e4ngigkeit des Auftretens von Hotspots von der Aggregationsstufe\n\n\nAusblick:\n\n\nSensorfusion in Kooperation mit SDIL (smart data innovation lab)\n\n\n\n\nRelated Scenarios\n\n\n\n\nSmart City", 
            "title": "Urban Heat Islands"
        }, 
        {
            "location": "/demos/urban-heat-islands/#urban-heat-islands", 
            "text": "Todo   Translate to English  add links to related github repos  add some images (but not too many)  add links to related papers  describe APIs especially from the end-users' point of view     Temperaturinseln in Karlsruhe und anderen St\u00e4dten  Temperaturdaten: Volunteered geographic data (z.B. wunderground.com)   Korrelation zwischen Bereichen in verschiedenen St\u00e4dten mit \u00e4hnlichem Temperaturverlauf -  Zugang zu Ursachen f\u00fcr Temperaturentwicklung    Vorstellung der Heat-Islands-Analyse   Wetterstationen  Technik: Sensebox ( https://sensebox.de )  Beispielstation ( https://opensensemap.org/explore/58b4354fe53e0b001251119d )  Hotspotanalyse (SoH, Stability of Hotspots):  Abh\u00e4ngigkeit des Auftretens von Hotspots von der Aggregationsstufe  Ausblick:  Sensorfusion in Kooperation mit SDIL (smart data innovation lab)", 
            "title": "Urban Heat Islands"
        }, 
        {
            "location": "/demos/urban-heat-islands/#related-scenarios", 
            "text": "Smart City", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/demos/visual-analysis-of-uhi/", 
            "text": "Visual Analysis of Spatio-Temporal Event Predictions\n\n\nInvestigating Causes for Urban Heat Islands\n\n\nMotivation\n\n\nUsed Data Sources\n\n\n\n\nWunderground\n\n\nATKIS/ALKIS\n\n\nDWD\n\n\n...\n\n\n\n\nVisual Analysis of Urban Heat Islands\n\n\n\n\nPrediction of Urban Heat Islands with Machine Learning\n\n\nGlyph-based visualization of land usage and UHI occurences\n\n\nInteractive visualizations allowing for brushing and filtering\n\n\nInteractive parameter steering to investigate impacts of different variables on the occurence UHIs\n\n\n...\n\n\n\n\nRelated Scenarios\n\n\n\n\nSmart City", 
            "title": "Visual Analysis of Spatio-Temporal Event Predictions"
        }, 
        {
            "location": "/demos/visual-analysis-of-uhi/#visual-analysis-of-spatio-temporal-event-predictions", 
            "text": "Investigating Causes for Urban Heat Islands", 
            "title": "Visual Analysis of Spatio-Temporal Event Predictions"
        }, 
        {
            "location": "/demos/visual-analysis-of-uhi/#motivation", 
            "text": "", 
            "title": "Motivation"
        }, 
        {
            "location": "/demos/visual-analysis-of-uhi/#used-data-sources", 
            "text": "Wunderground  ATKIS/ALKIS  DWD  ...", 
            "title": "Used Data Sources"
        }, 
        {
            "location": "/demos/visual-analysis-of-uhi/#visual-analysis-of-urban-heat-islands", 
            "text": "Prediction of Urban Heat Islands with Machine Learning  Glyph-based visualization of land usage and UHI occurences  Interactive visualizations allowing for brushing and filtering  Interactive parameter steering to investigate impacts of different variables on the occurence UHIs  ...", 
            "title": "Visual Analysis of Urban Heat Islands"
        }, 
        {
            "location": "/demos/visual-analysis-of-uhi/#related-scenarios", 
            "text": "Smart City", 
            "title": "Related Scenarios"
        }, 
        {
            "location": "/docs-howto/", 
            "text": "Documentation Howto\n\n\nWe use \nmkdocs\n for documenting the project.\nThe goal is to make the documentation process as simple as possible,\nto allow versioning and to use pull requests for text reviews.\nI should edit the docs locally, preview it in a browser and then suggest a pull request.\n\n\nThe amount of formatting elements is deliberately small.\nApart from wiki, we try to keep each page self contained and to minimize\ninterlinking between pages because it only complicates reading of the docs.\n\n\nThe documentation is written as a set of Markdown files within the \ndocs/\n directory and after deployment\navailable as a static website: \nDocs Website\n.\n\n\nBefore building the docs\n\n\nFirst of all, you need \npython\n and \npip\n to be installed.\nUsing pip, you need to install the following packages:\n\n\n\n\nmkdocs\n : Provides the executable command \nmkdocs\n.\n\n\nmkdocs-material\n : A material design theme. See also \nthis page\n.\n\n\npyembed-markdown\n : A markdown extension that allows for embedding Youtube videos in documents.\n                         See also \nthis page\n.\n\n\n\n\n\n\nHow to install as a user (recommended)\n\n\nYou can install the packages locally as a user into \n~/.local/\n\n\npip install --user mkdocs\npip install --user mkdocs-material\npip install --user pyembed-markdown\n\n\n\n\n\n\n\nHow to install system-wide as root (not recommended)\n\n\npip install mkdocs\npip install mkdocs-material\npip install pyembed-markdown\n\n\n\n\n\n\n\n\nHow to upgrade (as a user)\n\n\nMake sure you are using \nmkdocs version 0.17.2+\n.\n\npip install -U --user mkdocs\npip install -U --user mkdocs-material\npip install -U --user pyembed-markdown\n\n\n\n\n\nRecommended editor\n\n\nSince we use the markdown format, you can use any plain text editor.\nHowever, we suggest to use \nIntelliJ IDEA\n\nwith the \nMarkdown Support plugin\n (both are free) which gives you:\n\n\n\n\nsyntax highlighting\n\n\npath completion of links such as image file names\n\n\nrefactoring, which is handy when renaming markdown files which are liked from other files\n\n\nfancy search\n\n\noutline of the document structure\n\n\nautomated simplified preview (which is not that important due to the mkdocs hot-reload)\n\n\n\n\nHow to edit\n\n\nBefore editing the documentation, start the live-reloading docs server\nusing \nmkdocs serve\n within the project root directory.\nThen, open the page \nhttp://127.0.0.1:8000\n in your\nbrowser and watch your edits being reloaded automatically.\n\n\n1\nmkdocs serve\n\n\n\n\n\n\nINFO    -  Building documentation... \nINFO    -  Cleaning site directory \n[I 171024 15:03:51 server:283] Serving on http://127.0.0.1:8000\n[I 171024 15:03:51 handlers:60] Start watching changes\n\n\n\n\nYou can now edit the markdown documents with the \ndocs/\n directory.\n\n\n\n\nWarning\n\n\nIf you are renaming or creating new markdown files (\n*.md\n extension), you should also update the\n\npages\n configuration section in \nmkdocs.yml\n. Otherwise the update will not be reloaded in your browser.\nThis is a known limitation of mkdocs that will be addressed in future releases. Until then, there is\na small shell script that updates the \npages\n section automatically. However, it only works on Unix systems.\n\n# refreshes the pages section\n\n./fix-mkdocs-pages.sh mkdocs.yml\n\n\n\n\n\nDeployment\n\n\nUsing the command \nmkdocs gh-deploy\n we can generate a static \nDocs Website\n\nand deploy it automatically as a github page (served from \ngh-pages\n branch).\n\n\n\n\nInfo\n\n\nThe newly deployed version appears after few seconds.\n\n\n\n\n\n\nWarning\n\n\nThis operation is relevant only to the administrators of the \nbiggis-docs\n repository.\nAs a regular contributor, you should only preview your local changes and create\na pull request instead of directly deploying the built docs to the gh-pages branch.\n\n\n\n\nDocumentation layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n  index.md    # The documentation homepage.\n  ...         # Other markdown pages, images and other files.\n\n\n\n\n\nFor the sake of simplicity, we use two-level hierarchy inside \ndocs/\n:\n\n\n\n\nLevel 1\n : areas (directories) that appear in the main menu.\n\n\nLevel 2\n : pages (markdown files) that appear in the left side bar.\n\n\nLevel 3\n : headings (H1, H2, ...) that appear in the table of contents on the right\n\n\n\n\n\n\nWarning\n\n\nDo not use spaces in file names. Replace them with underscores \n_\n.\nThis allows for easier refactoring because spaces are transformed to \n%20\n in markdown.\n\n\n\n\nFormatting examples\n\n\nSectioning, Headings and Table of Contents\n# Chapter\n\n## Section\n\n### Subsection\n\n\n\n\nFootnotes\nSome text with a footnote[^1]\n\n[^1]: Text of the footnote\n\n\n\n\nSee also \nhttps://squidfunk.github.io/mkdocs-material/extensions/footnotes/\nCitations, Notes and Admonition\n!!! cite\n    Here comes the citation including authors, title, year, doi, url ...\n\n\n\n\nCite\nHere comes the citation including authors, title, year, doi, url ...\n!!! note\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n\n\n\n\nNote\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa.\nFor more options see \nhttps://squidfunk.github.io/mkdocs-material/extensions/admonition/\nCollapsible blocks\n??? \nPhasellus posuere in sem ut cursus\n\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n\n\n\n\nPhasellus posuere in sem ut cursus\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa.\nFor more information see \nhttps://facelessuser.github.io/pymdown-extensions/extensions/details/\nImages\nYou can include images into the documentation in the following format:\nSVG\n (scalable vectors).\nJPG\n (photos)\nPNG\n (raster graphics)\nIn contrast to scientific papers, it is not possible to create references to numbered figures in markdown.\n![Image \nalt\n description](path/to/image.svg)\n\n\n\n\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#images-and-media\nNote\nWhen editing a file e.g. \npath/to/ABC.md\n, store all related images in the same folder\nfolder (\npath/to/ABC\n). This way, different topics are better encapsulated.\nFigures with caption (sort of)\nWith the following hack, you can create a nice looking caption rendered under a figure.\n![](path/to/image.svg)\n\n **Figure:**\n\n Here comes some multi-line caption.\n\n Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n\n\n\n\nFigure:\n\nHere comes some multi-line caption.\nLorem ipsum dolor sit amet, consectetur adipiscing elit...\nMorbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis.\nTables\nFirst Header | Second Header | Third Header\n------------ | ------------- | ------------\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell\n\n\n\n\nFirst Header\nSecond Header\nThird Header\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#tables\nTables with alignment\nLeft         | Center        | Right\n---          |:--            |--:\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell\n\n\n\n\nLeft\nCenter\nRight\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#tables\nMathematical Formulas\nFormula are generated using \nMathJax\n, which is similar to LaTeX.\nSee also this \nquick reference\n.\n$$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$\n\n\n\n\n\\[\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\\]\nSource Code with Code Highlighting\nCode can be displayed inline like this:\n`print 1+{variable}`\n\n\n\n\nOr it can be displayed in a code block with optional syntax highlighting if the language is specified.\n```python\ndef my_function():\n    \njust a test\n\n    print 8/2 \n```\n\n\n\n\ndef\n \nmy_function\n():\n\n    \njust a test\n\n    \nprint\n \n8\n/\n2\n \n\n\n\nSmart Symbols\nSome smart symbols: --\n,  \n--, 1st, 2nd, 1/4\n\n\n\n\nSome smart symbols: \n,  \n, 1\nst\n, 2\nnd\n, \nSee also \nhttps://facelessuser.github.io/pymdown-extensions/extensions/smartsymbols/\nSequence diagrams\n```sequence\nTitle: Example sequence diagram\nA-\nB: Sync call\nB--\nA: Sync return\nA-\nC: Another sync call\nC-\nD: Async call\nD--\nC: Async return\n```\n\n\n\nTitle: Example sequence diagram\nA-\nB: Sync call\nB--\nA: Sync return\nA-\nC: Another sync call\nC-\nD: Async call\nD--\nC: Async return\n\n\nEmbedded Youtube Videos\n[!embed](https://www.youtube.com/watch?v=QQKVzZpXTpQ)\n\n\n\n\n\n\nFor more information see \nhttps://pyembed.github.io/usage/markdown/\nHTML (please only in special cases)\nIn special cases, you can also use raw HTML in your document.\n \nstyle\n.special img {height:32px; vertical-align:middle}\n/style\n\n \ndiv class=\nspecial\n\n   [![](scenarios/img/scen-smartcity.svg) Smart City](scenarios/01_city)\n \n/div\n\n\n\n\n\n.special img {height:32px; vertical-align:middle}\n\n \n\n   \n Smart City", 
            "title": "Documentation Howto"
        }, 
        {
            "location": "/docs-howto/#documentation-howto", 
            "text": "We use  mkdocs  for documenting the project.\nThe goal is to make the documentation process as simple as possible,\nto allow versioning and to use pull requests for text reviews.\nI should edit the docs locally, preview it in a browser and then suggest a pull request.  The amount of formatting elements is deliberately small.\nApart from wiki, we try to keep each page self contained and to minimize\ninterlinking between pages because it only complicates reading of the docs.  The documentation is written as a set of Markdown files within the  docs/  directory and after deployment\navailable as a static website:  Docs Website .", 
            "title": "Documentation Howto"
        }, 
        {
            "location": "/docs-howto/#before-building-the-docs", 
            "text": "First of all, you need  python  and  pip  to be installed.\nUsing pip, you need to install the following packages:   mkdocs  : Provides the executable command  mkdocs .  mkdocs-material  : A material design theme. See also  this page .  pyembed-markdown  : A markdown extension that allows for embedding Youtube videos in documents.\n                         See also  this page .    How to install as a user (recommended)  You can install the packages locally as a user into  ~/.local/  pip install --user mkdocs\npip install --user mkdocs-material\npip install --user pyembed-markdown    How to install system-wide as root (not recommended)  pip install mkdocs\npip install mkdocs-material\npip install pyembed-markdown    How to upgrade (as a user)  Make sure you are using  mkdocs version 0.17.2+ . pip install -U --user mkdocs\npip install -U --user mkdocs-material\npip install -U --user pyembed-markdown", 
            "title": "Before building the docs"
        }, 
        {
            "location": "/docs-howto/#recommended-editor", 
            "text": "Since we use the markdown format, you can use any plain text editor.\nHowever, we suggest to use  IntelliJ IDEA \nwith the  Markdown Support plugin  (both are free) which gives you:   syntax highlighting  path completion of links such as image file names  refactoring, which is handy when renaming markdown files which are liked from other files  fancy search  outline of the document structure  automated simplified preview (which is not that important due to the mkdocs hot-reload)", 
            "title": "Recommended editor"
        }, 
        {
            "location": "/docs-howto/#how-to-edit", 
            "text": "Before editing the documentation, start the live-reloading docs server\nusing  mkdocs serve  within the project root directory.\nThen, open the page  http://127.0.0.1:8000  in your\nbrowser and watch your edits being reloaded automatically.  1 mkdocs serve   INFO    -  Building documentation... \nINFO    -  Cleaning site directory \n[I 171024 15:03:51 server:283] Serving on http://127.0.0.1:8000\n[I 171024 15:03:51 handlers:60] Start watching changes  You can now edit the markdown documents with the  docs/  directory.   Warning  If you are renaming or creating new markdown files ( *.md  extension), you should also update the pages  configuration section in  mkdocs.yml . Otherwise the update will not be reloaded in your browser.\nThis is a known limitation of mkdocs that will be addressed in future releases. Until then, there is\na small shell script that updates the  pages  section automatically. However, it only works on Unix systems. # refreshes the pages section \n./fix-mkdocs-pages.sh mkdocs.yml", 
            "title": "How to edit"
        }, 
        {
            "location": "/docs-howto/#deployment", 
            "text": "Using the command  mkdocs gh-deploy  we can generate a static  Docs Website \nand deploy it automatically as a github page (served from  gh-pages  branch).   Info  The newly deployed version appears after few seconds.    Warning  This operation is relevant only to the administrators of the  biggis-docs  repository.\nAs a regular contributor, you should only preview your local changes and create\na pull request instead of directly deploying the built docs to the gh-pages branch.", 
            "title": "Deployment"
        }, 
        {
            "location": "/docs-howto/#documentation-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n  index.md    # The documentation homepage.\n  ...         # Other markdown pages, images and other files.  For the sake of simplicity, we use two-level hierarchy inside  docs/ :   Level 1  : areas (directories) that appear in the main menu.  Level 2  : pages (markdown files) that appear in the left side bar.  Level 3  : headings (H1, H2, ...) that appear in the table of contents on the right    Warning  Do not use spaces in file names. Replace them with underscores  _ .\nThis allows for easier refactoring because spaces are transformed to  %20  in markdown.", 
            "title": "Documentation layout"
        }, 
        {
            "location": "/docs-howto/#formatting-examples", 
            "text": "Sectioning, Headings and Table of Contents # Chapter\n\n## Section\n\n### Subsection  Footnotes Some text with a footnote[^1]\n\n[^1]: Text of the footnote  See also  https://squidfunk.github.io/mkdocs-material/extensions/footnotes/ Citations, Notes and Admonition !!! cite\n    Here comes the citation including authors, title, year, doi, url ...  Cite Here comes the citation including authors, title, year, doi, url ... !!! note\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.  Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa. For more options see  https://squidfunk.github.io/mkdocs-material/extensions/admonition/ Collapsible blocks ???  Phasellus posuere in sem ut cursus \n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.  Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa. For more information see  https://facelessuser.github.io/pymdown-extensions/extensions/details/ Images You can include images into the documentation in the following format: SVG  (scalable vectors). JPG  (photos) PNG  (raster graphics) In contrast to scientific papers, it is not possible to create references to numbered figures in markdown. ![Image  alt  description](path/to/image.svg)  See also  http://www.mkdocs.org/user-guide/writing-your-docs/#images-and-media Note When editing a file e.g.  path/to/ABC.md , store all related images in the same folder\nfolder ( path/to/ABC ). This way, different topics are better encapsulated. Figures with caption (sort of) With the following hack, you can create a nice looking caption rendered under a figure. ![](path/to/image.svg)  **Figure:**  Here comes some multi-line caption.  Lorem ipsum dolor sit amet, consectetur adipiscing elit.  Figure: \nHere comes some multi-line caption.\nLorem ipsum dolor sit amet, consectetur adipiscing elit...\nMorbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis. Tables First Header | Second Header | Third Header\n------------ | ------------- | ------------\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell  First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell See also  http://www.mkdocs.org/user-guide/writing-your-docs/#tables Tables with alignment Left         | Center        | Right\n---          |:--            |--:\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell  Left Center Right Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell See also  http://www.mkdocs.org/user-guide/writing-your-docs/#tables Mathematical Formulas Formula are generated using  MathJax , which is similar to LaTeX.\nSee also this  quick reference . $$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$  \\[\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\\] Source Code with Code Highlighting Code can be displayed inline like this: `print 1+{variable}`  Or it can be displayed in a code block with optional syntax highlighting if the language is specified. ```python\ndef my_function():\n     just a test \n    print 8/2 \n```  def   my_function (): \n     just a test \n     print   8 / 2    Smart Symbols Some smart symbols: -- ,   --, 1st, 2nd, 1/4  Some smart symbols:  ,   , 1 st , 2 nd ,  See also  https://facelessuser.github.io/pymdown-extensions/extensions/smartsymbols/ Sequence diagrams ```sequence\nTitle: Example sequence diagram\nA- B: Sync call\nB-- A: Sync return\nA- C: Another sync call\nC- D: Async call\nD-- C: Async return\n```  Title: Example sequence diagram\nA- B: Sync call\nB-- A: Sync return\nA- C: Another sync call\nC- D: Async call\nD-- C: Async return  Embedded Youtube Videos [!embed](https://www.youtube.com/watch?v=QQKVzZpXTpQ)   For more information see  https://pyembed.github.io/usage/markdown/ HTML (please only in special cases) In special cases, you can also use raw HTML in your document.   style .special img {height:32px; vertical-align:middle} /style \n  div class= special \n   [![](scenarios/img/scen-smartcity.svg) Smart City](scenarios/01_city)\n  /div   .special img {height:32px; vertical-align:middle} \n  \n     Smart City", 
            "title": "Formatting examples"
        }, 
        {
            "location": "/methods/", 
            "text": "About Method\n\n\nThis section contains a \nlist of methods\n that serve as a \ntheoretical background\n\nfor the other sections inside the documentation, especially for the \nDemos\n.\n\n\nSome methods depend on each other which is reflected in their ordering.\n\n\nMethods to be included later\n\n\n\n\nExasol: spatial indices\n\n\nExasol: virtual schemas\n\n\nExasol: language bindings (e.g. R)", 
            "title": "About Method"
        }, 
        {
            "location": "/methods/#about-method", 
            "text": "This section contains a  list of methods  that serve as a  theoretical background \nfor the other sections inside the documentation, especially for the  Demos .  Some methods depend on each other which is reflected in their ordering.", 
            "title": "About Method"
        }, 
        {
            "location": "/methods/#methods-to-be-included-later", 
            "text": "Exasol: spatial indices  Exasol: virtual schemas  Exasol: language bindings (e.g. R)", 
            "title": "Methods to be included later"
        }, 
        {
            "location": "/methods/focal_getis_ord/", 
            "text": "Focal Getis-ord\n\n\n\n\nTodo\n\n\nJulian Bruns : definition of Focal G* using points \n\n\n\n\nThe Focal Getis-Ord \n\\(G^*_i\\)\n statistic differs from the \nStandard Getis-ord\n ...\n\n\n\\[\n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\\]\nwhere:\n\n\n\n\nTODO\n\n\nTODO", 
            "title": "Focal Getis-ord"
        }, 
        {
            "location": "/methods/focal_getis_ord/#focal-getis-ord", 
            "text": "Todo  Julian Bruns : definition of Focal G* using points    The Focal Getis-Ord  \\(G^*_i\\)  statistic differs from the  Standard Getis-ord  ...  \\[\n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\\] where:   TODO  TODO", 
            "title": "Focal Getis-ord"
        }, 
        {
            "location": "/methods/focal_getis_ord_raster/", 
            "text": "Foal Getis-ord on rasters\n\n\nThe rasterized Focal Getis-Ord formula looks as follows:\n\n\n\\[\nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\\]\nwhere:\n\n\n\n\n\\(R\\)\n is the input raster.\n\n\n\\(W\\)\n is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g. \n\\(5 \\times 5\\)\n, \n\\(31 \\times 31\\)\n ...\n\n\n\\(N\\)\n represents the focal count of pixels TODO (there can be NA values)\n\n\n\\(M\\)\n represents the focal mean TODO.\n\n\n\\(S\\)\n represents the focal standard deviation TODO.\n\n\n\n\nIt can be seen that the formula can be nicely refactored into:\n\n\n\n\nTODO", 
            "title": "Foal Getis-ord on rasters"
        }, 
        {
            "location": "/methods/focal_getis_ord_raster/#foal-getis-ord-on-rasters", 
            "text": "The rasterized Focal Getis-Ord formula looks as follows:  \\[\nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\\] where:   \\(R\\)  is the input raster.  \\(W\\)  is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g.  \\(5 \\times 5\\) ,  \\(31 \\times 31\\)  ...  \\(N\\)  represents the focal count of pixels TODO (there can be NA values)  \\(M\\)  represents the focal mean TODO.  \\(S\\)  represents the focal standard deviation TODO.   It can be seen that the formula can be nicely refactored into:   TODO", 
            "title": "Foal Getis-ord on rasters"
        }, 
        {
            "location": "/methods/getis_ord/", 
            "text": "Standards Getis-Ord G*\n\n\nThe standard definition of Getis-Ord \n\\(G^*_i\\)\n statistic assumes a study area with \n\\(n\\)\n points with measurements\n\n\\(X = [x_1, \\ldots, x_n]\\)\n. Moreover, it assumes weights \n\\(w_{i,j}\\)\n to be defined between all pairs of points \n\\(i\\)\n\nand \n\\(j\\)\n (for all \n\\(i,j \\in \\{ 1, \\ldots, n\\}\\)\n). The formula to compute \n\\(G^*_i\\)\n at a given point \n\\(i\\)\n is then:\n\n\n\\[\n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\\]\nwhere:\n\n\n\n\n\\(\\bar{X}\\)\n is the mean of all measurements,\n\n\n\\(S\\)\n is the standard deviation of all measurements.\n\n\n\n\nAs it is known, this statistic creates a z-score, which denotes the significance of an area in relation\nto its surrounding areas.\n\n\n\n\nNote\n\n\nFor \n\\(x \\in X\\)\n, the \n\\(zscore(x) = \\frac{x - mean(X)}{stdev(X)}\\)\n\n\n\n\n\n\nTodo\n\n\nJulian Bruns: Add references to papers", 
            "title": "Standards Getis-Ord G*"
        }, 
        {
            "location": "/methods/getis_ord/#standards-getis-ord-g", 
            "text": "The standard definition of Getis-Ord  \\(G^*_i\\)  statistic assumes a study area with  \\(n\\)  points with measurements \\(X = [x_1, \\ldots, x_n]\\) . Moreover, it assumes weights  \\(w_{i,j}\\)  to be defined between all pairs of points  \\(i\\) \nand  \\(j\\)  (for all  \\(i,j \\in \\{ 1, \\ldots, n\\}\\) ). The formula to compute  \\(G^*_i\\)  at a given point  \\(i\\)  is then:  \\[\n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\\] where:   \\(\\bar{X}\\)  is the mean of all measurements,  \\(S\\)  is the standard deviation of all measurements.   As it is known, this statistic creates a z-score, which denotes the significance of an area in relation\nto its surrounding areas.   Note  For  \\(x \\in X\\) , the  \\(zscore(x) = \\frac{x - mean(X)}{stdev(X)}\\)    Todo  Julian Bruns: Add references to papers", 
            "title": "Standards Getis-Ord G*"
        }, 
        {
            "location": "/methods/getis_ord_raster/", 
            "text": "Getis-ord G* on rasters\n\n\nThe \nStandard Getis-ord\n is defined on individual points (vector data).\nIn many situations, we want to compute \n\\(G^*_i\\)\n in a raster rather than in a point cloud.\nThis way, the computation can be expressed using map algebra operations and nicely parallelized in frameworks \nsuch as \nGeotrellis\n.\n\n\nThe rasterized Getis-Ord formula looks as follows:\n\n\n\\[\nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\\]\nwhere:\n\n\n\n\n\\(R\\)\n is the input raster.\n\n\n\\(W\\)\n is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g. \n\\(5 \\times 5\\)\n, \n\\(31 \\times 31\\)\n ...\n\n\n\\(N\\)\n represents the number of all pixels in \n\\(R\\)\n (because there can be NA values)\n\n\n\\(M\\)\n represents the global mean of \n\\(R\\)\n.\n\n\n\\(S\\)\n represents the global standard deviation of all pixels in \n\\(R\\)\n.\n\n\n\n\nIt can be seen that the formula can be nicely refactored into:\n\n\n\n\nOne \nglobal operation\n that computes \n\\(N\\)\n, \n\\(M\\)\n, \n\\(S\\)\n. These values are usually available because they\n  were pre-computed when the raster layer has been ingested into the catalog.\n\n\nOne \nfocal operation\n \n\\(R{\\stackrel{\\mathtt{sum}}{\\circ}}W\\)\n - the convolution of raster \n\\(R\\)\n with\n  the weight matrix \n\\(W\\)\n.\n\n\nOne \nlocal operation\n that puts all components toghether for each pixel in \n\\(R\\)\n.", 
            "title": "Getis-ord G* on rasters"
        }, 
        {
            "location": "/methods/getis_ord_raster/#getis-ord-g-on-rasters", 
            "text": "The  Standard Getis-ord  is defined on individual points (vector data).\nIn many situations, we want to compute  \\(G^*_i\\)  in a raster rather than in a point cloud.\nThis way, the computation can be expressed using map algebra operations and nicely parallelized in frameworks \nsuch as  Geotrellis .  The rasterized Getis-Ord formula looks as follows:  \\[\nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\\] where:   \\(R\\)  is the input raster.  \\(W\\)  is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g.  \\(5 \\times 5\\) ,  \\(31 \\times 31\\)  ...  \\(N\\)  represents the number of all pixels in  \\(R\\)  (because there can be NA values)  \\(M\\)  represents the global mean of  \\(R\\) .  \\(S\\)  represents the global standard deviation of all pixels in  \\(R\\) .   It can be seen that the formula can be nicely refactored into:   One  global operation  that computes  \\(N\\) ,  \\(M\\) ,  \\(S\\) . These values are usually available because they\n  were pre-computed when the raster layer has been ingested into the catalog.  One  focal operation   \\(R{\\stackrel{\\mathtt{sum}}{\\circ}}W\\)  - the convolution of raster  \\(R\\)  with\n  the weight matrix  \\(W\\) .  One  local operation  that puts all components toghether for each pixel in  \\(R\\) .", 
            "title": "Getis-ord G* on rasters"
        }, 
        {
            "location": "/methods/one_vs_rest/", 
            "text": "One vs. Rest\n\n\nThe One-versus-Rest (or One-versus-All = OvA) Strategy is a Method to use Binary (Two-Class) Classifiers\n(such as \nSupport Vector Machines\n ) for classifying multiple\n(more than two) Classes.\n\n\n\n\nNote\n\n\n\n\nhttps://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest\n\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#one-vs-rest-classifier-aka-one-vs-all\n\n\n\n\n\n\nOne-vs.-rest\n\n\nInputs\n\n\n\n\n\\(L \\text{ , a learner (training algorithm for binary classifiers) }\\)\n\n\n\\(\\text{ samples } \\vec{X}\\)\n\n\n\\(\\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i\\)\n\n\n\n\nOutput\n\n\n\n\n\\(\\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K}\\)\n\n\n\n\nProcedure\n\n\n\\(\\text{ For each } k \\text{ in } {1,\\ldots,K}\\)\n\n\n\\[\n\\begin{align}\n\n\\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1, \n \\text{ if } y_i = k \\\\\nz_i = 0, \n \\text{ otherwise }\n\\end{cases}\\\\\n\n\\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\\n\\]\nMaking decisions means applying all classifiers to an unseen sample and\npredicting the label for which the corresponding classifier reports the\nhighest confidence score:\n\n\n\\[\n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)\n\\]\n\n\nTodo\n\n\nAdrian Klink: Add references, optimize description", 
            "title": "One vs. Rest"
        }, 
        {
            "location": "/methods/one_vs_rest/#one-vs-rest", 
            "text": "The One-versus-Rest (or One-versus-All = OvA) Strategy is a Method to use Binary (Two-Class) Classifiers\n(such as  Support Vector Machines  ) for classifying multiple\n(more than two) Classes.   Note   https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest  https://spark.apache.org/docs/latest/ml-classification-regression.html#one-vs-rest-classifier-aka-one-vs-all", 
            "title": "One vs. Rest"
        }, 
        {
            "location": "/methods/one_vs_rest/#one-vs-rest_1", 
            "text": "", 
            "title": "One-vs.-rest"
        }, 
        {
            "location": "/methods/one_vs_rest/#inputs", 
            "text": "\\(L \\text{ , a learner (training algorithm for binary classifiers) }\\)  \\(\\text{ samples } \\vec{X}\\)  \\(\\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i\\)", 
            "title": "Inputs"
        }, 
        {
            "location": "/methods/one_vs_rest/#output", 
            "text": "\\(\\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K}\\)", 
            "title": "Output"
        }, 
        {
            "location": "/methods/one_vs_rest/#procedure", 
            "text": "\\(\\text{ For each } k \\text{ in } {1,\\ldots,K}\\)  \\[\n\\begin{align} \\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1,   \\text{ if } y_i = k \\\\\nz_i = 0,   \\text{ otherwise }\n\\end{cases}\\\\ \\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\\n\\] Making decisions means applying all classifiers to an unseen sample and\npredicting the label for which the corresponding classifier reports the\nhighest confidence score:  \\[\n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)\n\\]  Todo  Adrian Klink: Add references, optimize description", 
            "title": "Procedure"
        }, 
        {
            "location": "/methods/soh/", 
            "text": "Stability of hotspots\n\n\n\n\nTodo\n\n\nIntro text\n\n\n\n\nRelated demos:\n- \nstability of hotspots", 
            "title": "Stability of hotspots"
        }, 
        {
            "location": "/methods/soh/#stability-of-hotspots", 
            "text": "Todo  Intro text   Related demos:\n-  stability of hotspots", 
            "title": "Stability of hotspots"
        }, 
        {
            "location": "/methods/spectralAnalysisTCI/", 
            "text": "Triangular Chlorophyll Index (TCI) in Spectral Remote Sensing\n\n\nThe Triangular Chlorophyll Index (TCI) is widely used in remote sensing in the field of agricultural studies. A typical use case is e.g. the quantification of vegetation in an area for the purpose of land-use classification.\n\n\nThe TCI bases on the absorption maximum and thus a minimum of reflectance of chlorophyll at a wavelength of roughly 670 nm. The stronger the minimum of reflectance is expressed in the spectrum under survey, the higher the TCI value.\n\n\nThe TCI is calculated according to the following formular (\nhttps://www.indexdatabase.de/db/i-single.php?id=392\n):\n\n\n\\[\nTCI = 1.2 \\cdot {\\left( {R_{700nm}-R_{550nm}} \\right){-1.5} \\cdot {\\left( {R_{670nm}-R_{550nm}} \\right) \\cdot \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}}}\n\\]\nHere, \n\\(R_{550nm}\\)\n, \n\\(R_{670nm}\\)\n and \n\\(R_{700nm}\\)\n denote the reflectance for the wavelengths 550nm, 670nm and 700nm, respectively. In order to deduce the reflectance values from the recorded reflected intensities a proper white balance has to be provided.", 
            "title": "Triangular Chlorophyll Index (TCI) in Spectral Remote Sensing"
        }, 
        {
            "location": "/methods/spectralAnalysisTCI/#triangular-chlorophyll-index-tci-in-spectral-remote-sensing", 
            "text": "The Triangular Chlorophyll Index (TCI) is widely used in remote sensing in the field of agricultural studies. A typical use case is e.g. the quantification of vegetation in an area for the purpose of land-use classification.  The TCI bases on the absorption maximum and thus a minimum of reflectance of chlorophyll at a wavelength of roughly 670 nm. The stronger the minimum of reflectance is expressed in the spectrum under survey, the higher the TCI value.  The TCI is calculated according to the following formular ( https://www.indexdatabase.de/db/i-single.php?id=392 ):  \\[\nTCI = 1.2 \\cdot {\\left( {R_{700nm}-R_{550nm}} \\right){-1.5} \\cdot {\\left( {R_{670nm}-R_{550nm}} \\right) \\cdot \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}}}\n\\] Here,  \\(R_{550nm}\\) ,  \\(R_{670nm}\\)  and  \\(R_{700nm}\\)  denote the reflectance for the wavelengths 550nm, 670nm and 700nm, respectively. In order to deduce the reflectance values from the recorded reflected intensities a proper white balance has to be provided.", 
            "title": "Triangular Chlorophyll Index (TCI) in Spectral Remote Sensing"
        }, 
        {
            "location": "/methods/support_vector_machine/", 
            "text": "Support Vector Machine\n\n\nA Support Vector Machine (SVM) is a supervised learning model (machine learning) which can be used to classify image data. \nFor Parallelization we use a Linear SVM from \nApache Spark\n.\nSince we have more than two classes the One versus All (or \nOne vs. Rest\n ) Strategy is used.\n\n\n\n\nNote\n\n\n\n\nhttps://en.wikipedia.org/wiki/Support_vector_machine\n\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine\n\n\n\n\n\n\nLinear SVM\n\n\nWe are given a training dataset of \n\\(n\\)\n points of the form\n\n\n\\[\n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)\n\\]\nwhere the \n\\(y_i\\)\n are either 1 or \u22121, each indicating the class to which\nthe point \n\\(\\vec{x}_i\\)\n belongs. Each \n\\(\\vec{x}_i\\)\n is a \n\\(p\\)\n-dimensional\n\nreal\n vector. We want to find the\n\"maximum-margin hyperplane\" that divides the group of points\n\n\\(\\vec{x}_i\\)\n for which \n\\(y_i=1\\)\n from the group of points for which\n\n\\(y_i=-1\\)\n, which is defined so that the distance between the hyperplane\nand the nearest point \n\\(\\vec{x}_i\\)\n from either group is maximized.\n\n\nAny \nhyperplane\n can be written as the set of\npoints \n\\(\\vec{x}\\)\n satisfying\n\n\n\\[\n  \\vec{w}\\cdot\\vec{x} - b=0\n\\]\nwhere \n\\({\\vec{w}}\\)\n is the (not necessarily normalized) \nnormal\nvector\n to the hyperplane. This is much\nlike \nHesse normal form\n, except that\n\n\\({\\vec{w}}\\)\n is not necessarily a unit vector. The parameter\n\n\\(\\tfrac{b}{\\|\\vec{w}\\|}\\)\n determines the offset of the hyperplane from\nthe origin along the normal vector \n\\({\\vec{w}}\\)\n.\n\n\n\n\n\n\nFigure:\n\nMaximum-margin hyperplane and margins for an SVM trained with samples from two classes.\nSamples on the margin are called the support vectors.\n\n\n\n\nHard-margin\n\n\nIf the training data are \nlinearly\nseparable\n, we can select two parallel\nhyperplanes that separate the two classes of data, so that the distance\nbetween them is as large as possible. The region bounded by these two\nhyperplanes is called the \\\"margin\\\", and the maximum-margin hyperplane\nis the hyperplane that lies halfway between them. These hyperplanes can\nbe described by the equations\n$$\n  \\vec{w}\\cdot\\vec{x} - b=1\n$$\n\n\nand\n\n\n\\[\n  \\vec{w}\\cdot\\vec{x} - b=-1\n\\]\nGeometrically, the distance between these two hyperplanes is\n\n\\(\\tfrac{2}{\\|\\vec{w}\\|}\\)\n, so to maximize the distance between the planes\nwe want to minimize \n\\(\\|\\vec{w}\\|\\)\n. As we also have to prevent data\npoints from falling into the margin, we add the following constraint:\nfor each \n\\(i\\)\n either\n\n\n\\[\n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1\n\\]\nor\n\n\n\\[\n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1\n\\]\nThese constraints state that each data point must lie on the correct\nside of the margin.\n\n\nThis can be rewritten as:\n\n\n\\[\n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)\n\\]\nWe can put this together to get the optimization problem:\n\n\n\\[\n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n\n\\]\nThe \n\\(\\vec w\\)\n and \n\\(b\\)\n that solve this problem determine our classifier,\n$$\n  \\vec{x} \\mapsto sgn(\\vec{w} \\cdot \\vec{x} - b)\n$$\n\n\nAn easy-to-see but important consequence of this geometric description\nis that the max-margin hyperplane is completely determined by those\n\n\\(\\vec{x}_i\\)\n which lie nearest to it. These \n\\(\\vec{x}_i\\)\n are called\n\nsupport vectors.\n\n\nSoft-margin\n\n\nTo extend SVM to cases in which the data are not linearly separable, we\nintroduce the \nhinge loss\n function:\n\n\n\\[\n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)\n\\]\nThis function is zero if the constraint in (1) is satisfied, in other\nwords, if \n\\(\\vec{x}_i\\)\n lies on the correct side of the margin. For data\non the wrong side of the margin, the function\\'s value is proportional\nto the distance from the margin.\n\n\nWe then wish to minimize\n\n\n\\[\n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,\n\\]\nwhere the parameter \n\\(\\lambda\\)\n determines the tradeoff between increasing\nthe margin-size and ensuring that the \n\\(\\vec{x}_i\\)\n lie on the correct\nside of the margin. Thus, for sufficiently small values of \n\\(\\lambda\\)\n,\nthe soft-margin SVM will behave identically to the hard-margin SVM if\nthe input data are linearly classifiable, but will still learn if a\nclassification rule is viable or not.\n\n\n\n\nTodo\n\n\nAdrian Klink: Add references, optimize description", 
            "title": "Support Vector Machine"
        }, 
        {
            "location": "/methods/support_vector_machine/#support-vector-machine", 
            "text": "A Support Vector Machine (SVM) is a supervised learning model (machine learning) which can be used to classify image data. \nFor Parallelization we use a Linear SVM from  Apache Spark .\nSince we have more than two classes the One versus All (or  One vs. Rest  ) Strategy is used.   Note   https://en.wikipedia.org/wiki/Support_vector_machine  https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine", 
            "title": "Support Vector Machine"
        }, 
        {
            "location": "/methods/support_vector_machine/#linear-svm", 
            "text": "We are given a training dataset of  \\(n\\)  points of the form  \\[\n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)\n\\] where the  \\(y_i\\)  are either 1 or \u22121, each indicating the class to which\nthe point  \\(\\vec{x}_i\\)  belongs. Each  \\(\\vec{x}_i\\)  is a  \\(p\\) -dimensional real  vector. We want to find the\n\"maximum-margin hyperplane\" that divides the group of points \\(\\vec{x}_i\\)  for which  \\(y_i=1\\)  from the group of points for which \\(y_i=-1\\) , which is defined so that the distance between the hyperplane\nand the nearest point  \\(\\vec{x}_i\\)  from either group is maximized.  Any  hyperplane  can be written as the set of\npoints  \\(\\vec{x}\\)  satisfying  \\[\n  \\vec{w}\\cdot\\vec{x} - b=0\n\\] where  \\({\\vec{w}}\\)  is the (not necessarily normalized)  normal\nvector  to the hyperplane. This is much\nlike  Hesse normal form , except that \\({\\vec{w}}\\)  is not necessarily a unit vector. The parameter \\(\\tfrac{b}{\\|\\vec{w}\\|}\\)  determines the offset of the hyperplane from\nthe origin along the normal vector  \\({\\vec{w}}\\) .    Figure: \nMaximum-margin hyperplane and margins for an SVM trained with samples from two classes.\nSamples on the margin are called the support vectors.", 
            "title": "Linear SVM"
        }, 
        {
            "location": "/methods/support_vector_machine/#hard-margin", 
            "text": "If the training data are  linearly\nseparable , we can select two parallel\nhyperplanes that separate the two classes of data, so that the distance\nbetween them is as large as possible. The region bounded by these two\nhyperplanes is called the \\\"margin\\\", and the maximum-margin hyperplane\nis the hyperplane that lies halfway between them. These hyperplanes can\nbe described by the equations\n$$\n  \\vec{w}\\cdot\\vec{x} - b=1\n$$  and  \\[\n  \\vec{w}\\cdot\\vec{x} - b=-1\n\\] Geometrically, the distance between these two hyperplanes is \\(\\tfrac{2}{\\|\\vec{w}\\|}\\) , so to maximize the distance between the planes\nwe want to minimize  \\(\\|\\vec{w}\\|\\) . As we also have to prevent data\npoints from falling into the margin, we add the following constraint:\nfor each  \\(i\\)  either  \\[\n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1\n\\] or  \\[\n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1\n\\] These constraints state that each data point must lie on the correct\nside of the margin.  This can be rewritten as:  \\[\n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)\n\\] We can put this together to get the optimization problem:  \\[\n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n\n\\] The  \\(\\vec w\\)  and  \\(b\\)  that solve this problem determine our classifier,\n$$\n  \\vec{x} \\mapsto sgn(\\vec{w} \\cdot \\vec{x} - b)\n$$  An easy-to-see but important consequence of this geometric description\nis that the max-margin hyperplane is completely determined by those \\(\\vec{x}_i\\)  which lie nearest to it. These  \\(\\vec{x}_i\\)  are called support vectors.", 
            "title": "Hard-margin"
        }, 
        {
            "location": "/methods/support_vector_machine/#soft-margin", 
            "text": "To extend SVM to cases in which the data are not linearly separable, we\nintroduce the  hinge loss  function:  \\[\n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)\n\\] This function is zero if the constraint in (1) is satisfied, in other\nwords, if  \\(\\vec{x}_i\\)  lies on the correct side of the margin. For data\non the wrong side of the margin, the function\\'s value is proportional\nto the distance from the margin.  We then wish to minimize  \\[\n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,\n\\] where the parameter  \\(\\lambda\\)  determines the tradeoff between increasing\nthe margin-size and ensuring that the  \\(\\vec{x}_i\\)  lie on the correct\nside of the margin. Thus, for sufficiently small values of  \\(\\lambda\\) ,\nthe soft-margin SVM will behave identically to the hard-margin SVM if\nthe input data are linearly classifiable, but will still learn if a\nclassification rule is viable or not.   Todo  Adrian Klink: Add references, optimize description", 
            "title": "Soft-margin"
        }, 
        {
            "location": "/scenarios/01_city/", 
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nSmart City", 
            "title": "Smart City"
        }, 
        {
            "location": "/scenarios/01_city/#smart-city", 
            "text": "", 
            "title": "Smart City"
        }, 
        {
            "location": "/scenarios/02_bos/", 
            "text": "Responsible person for this section\n\n\nAlexander Groeschel, Bodo Bernsdorf\n\n\n\n\nDisaster Management\n\n\n\n\nGoals\n\n\n\n\nOptimal support of the command and control team\n\n\nInformation is needed quickly (real time)\n\n\nRequirements / challenges:\n\n\nFast\n\n\nHow can data be delivered to the consumers through slow connection lines?\n\n\nDecision makers often without IT background", 
            "title": "Disaster Management"
        }, 
        {
            "location": "/scenarios/02_bos/#disaster-management", 
            "text": "", 
            "title": "Disaster Management"
        }, 
        {
            "location": "/scenarios/02_bos/#goals", 
            "text": "Optimal support of the command and control team  Information is needed quickly (real time)  Requirements / challenges:  Fast  How can data be delivered to the consumers through slow connection lines?  Decision makers often without IT background", 
            "title": "Goals"
        }, 
        {
            "location": "/scenarios/03_env/", 
            "text": "Responsible person for this section\n\n\nJohannes Kutterer\n\n\n\n\nEnvironment", 
            "title": "Environment"
        }, 
        {
            "location": "/scenarios/03_env/#environment", 
            "text": "", 
            "title": "Environment"
        }
    ]
}