{
    "docs": [
        {
            "location": "/",
            "text": "About BigGIS\n\u00b6\n\n\nBigGIS is a new generation of GIS that supports decision making in multiple\nscenarios which require processing of large and heterogeneous data sets.\n\n\nThe novelty lies in an integrated analytical approach to spatio-temporal\ndata, that are unstructured and from unreliable sources. The system provides\npredictive, prescriptive and visual tool integrated in a common analytical\npipeline.\n\n\n\n\n\n\n\n\nSmart City\n\n\nEnvironmental Management\n\n\nDisaster Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  #scenlist img {\n    height:32px;\n    vertical-align:middle\n  }\n\n\n\n\nThe project is evaluated on three scenarios:\n\n\n  \n Smart City\n\n  : urban heat islands, particulate matter\n  \n\n  \n Environmental management\n\n  : health threatening animals and plants\n  \n\n  \n Disaster control, civil protection\n\n  : air pollution, toxic chemicals\n\n\n\n\n\n\n\nWhy BigGIS?\n\u00b6\n\n\nCurrent GIS solution are mostly tackling big data related requirements in\nterms of data volume or data velocity. In the era of cloud computing,\nleveraging cloud-based resources is a widely adopted pattern. In addition,\nwith the advent of big data analytics, performing massively parallel\nanalytical tasks on large-scale data at rest or data in motion is as well\nbecoming a feasible approach shaping the design of today\u2019s GIS. Although\nscaling out enables GIS to tackle the aforementioned big data induced\nrequirements, there are still two major open issues. Firstly, dealing with\nvarying data types across multiple data sources (variety) lead to data and\nschema heterogeneity, e.g., to describe locations such as addresses, relative\nspatial relationships or different coordinates reference systems. Secondly,\nmodeling the inherent uncertainties in data (veracity), e.g., real-world\nnoise and erroneous values due to the nature of the data collecting process.\nBoth being crucial tasks in data management and analytics that directly\naffect the information retrieval and decision-making quality and moreover\nthe generated knowledge on human-side (value). By leveraging the the\ncontinuous refinement model, we present a holistic approach that explicitly\ndeals with all big data dimensions. By integrating the user in the\nprocess, computers can learn from the cognitive and perceptive skills of\nhuman analysis to create hidden connections between data and the problem\ndomain. This helps to decrease the noise and uncertainty and allows to\nbuild up trust in the analysis results on user side which will eventually\nlead to an increasing likelihood of relevant findings and generated\nknowledge.\n\n\nContact and Support\n\u00b6\n\n\n\n\n\n\n\n\nRole\n\n\nName\n\n\nE-mail\n\n\n\n\n\n\n\n\n\n\nContact person\n\n\nProf. Dr. Thomas Setzer\n\n\nsetzer@fzi.de\n\n\n\n\n\n\nProject coordination\n\n\nDr. Viliam Simko\n\n\nsimko@fzi.de",
            "title": "About BigGIS"
        },
        {
            "location": "/#about-biggis",
            "text": "BigGIS is a new generation of GIS that supports decision making in multiple\nscenarios which require processing of large and heterogeneous data sets.  The novelty lies in an integrated analytical approach to spatio-temporal\ndata, that are unstructured and from unreliable sources. The system provides\npredictive, prescriptive and visual tool integrated in a common analytical\npipeline.     Smart City  Environmental Management  Disaster Control            \n  #scenlist img {\n    height:32px;\n    vertical-align:middle\n  }  The project is evaluated on three scenarios: \n    Smart City \n  : urban heat islands, particulate matter\n   \n    Environmental management \n  : health threatening animals and plants\n   \n    Disaster control, civil protection \n  : air pollution, toxic chemicals",
            "title": "About BigGIS"
        },
        {
            "location": "/#why-biggis",
            "text": "Current GIS solution are mostly tackling big data related requirements in\nterms of data volume or data velocity. In the era of cloud computing,\nleveraging cloud-based resources is a widely adopted pattern. In addition,\nwith the advent of big data analytics, performing massively parallel\nanalytical tasks on large-scale data at rest or data in motion is as well\nbecoming a feasible approach shaping the design of today\u2019s GIS. Although\nscaling out enables GIS to tackle the aforementioned big data induced\nrequirements, there are still two major open issues. Firstly, dealing with\nvarying data types across multiple data sources (variety) lead to data and\nschema heterogeneity, e.g., to describe locations such as addresses, relative\nspatial relationships or different coordinates reference systems. Secondly,\nmodeling the inherent uncertainties in data (veracity), e.g., real-world\nnoise and erroneous values due to the nature of the data collecting process.\nBoth being crucial tasks in data management and analytics that directly\naffect the information retrieval and decision-making quality and moreover\nthe generated knowledge on human-side (value). By leveraging the the\ncontinuous refinement model, we present a holistic approach that explicitly\ndeals with all big data dimensions. By integrating the user in the\nprocess, computers can learn from the cognitive and perceptive skills of\nhuman analysis to create hidden connections between data and the problem\ndomain. This helps to decrease the noise and uncertainty and allows to\nbuild up trust in the analysis results on user side which will eventually\nlead to an increasing likelihood of relevant findings and generated\nknowledge.",
            "title": "Why BigGIS?"
        },
        {
            "location": "/#contact-and-support",
            "text": "Role  Name  E-mail      Contact person  Prof. Dr. Thomas Setzer  setzer@fzi.de    Project coordination  Dr. Viliam Simko  simko@fzi.de",
            "title": "Contact and Support"
        },
        {
            "location": "/biggis-github-repos/",
            "text": "List of Github Repositories\n\u00b6\n\n\n\n  \n\n    \n\n      \n\n      \n\n      \n\n      \n\n        \nedit\n\n      \n\n    \n\n    \n\n      \n\n        \n\n          \n\n            \n\n              {{ item.name }}\n            \n\n            \n\n              {{ item.description }}\n            \n\n            \n\n              {{ item.note }}\n            \n\n            \n\n              \n{{ item.created_at }}\n\n              \narchived\n\n              \n\n              \n\n                Open\n              \n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}\n\n\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'https://api.github.com/orgs/biggis-project/repos',\n    items_edit_url: 'https://github.com/biggis-project'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      if(this.isLoaded) return ''\n      if(this.items == null) return 'Loading ...'\n      return this.items\n    },\n    publicItems() {\n      return this.items.filter(x=>!x.private)\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n      try {\n        const resp = await axios(this.items_url)\n        this.items = resp.data.sort(sortByDate)\n      } catch(e) {\n        this.items = e.response.data.message\n      }\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Github Repositories"
        },
        {
            "location": "/biggis-github-repos/#list-of-github-repositories",
            "text": "edit \n       \n     \n     \n       \n         \n           \n             \n              {{ item.name }}\n             \n             \n              {{ item.description }}\n             \n             \n              {{ item.note }}\n             \n             \n               {{ item.created_at }} \n               archived \n               \n               \n                Open\n               \n             \n           \n         \n       \n     \n       \nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}      \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'https://api.github.com/orgs/biggis-project/repos',\n    items_edit_url: 'https://github.com/biggis-project'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      if(this.isLoaded) return ''\n      if(this.items == null) return 'Loading ...'\n      return this.items\n    },\n    publicItems() {\n      return this.items.filter(x=>!x.private)\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n      try {\n        const resp = await axios(this.items_url)\n        this.items = resp.data.sort(sortByDate)\n      } catch(e) {\n        this.items = e.response.data.message\n      }\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Github Repositories"
        },
        {
            "location": "/biggis-papers/",
            "text": "List of Papers\n\u00b6\n\n\n\n  \n\n    \n\n      \n\n      \n\n      \n\n      \n\n        \nfile_download\n\n      \n\n      \n\n        \nedit\n\n      \n\n    \n\n    \n\n      \n\n        \n\n          \n\n            \n\n              {{ item.title }}\n            \n\n            \n\n              \n{{author}}\n\n            \n\n            \n\n              {{ item.note }}\n              \n\n                {{ eventData(item) }}\n              \n\n            \n\n            \n\n              \n{{ item.date }}\n\n              \n\n              \n\n                \nopen_in_new\n\n              \n\n              \n\n                \ninsert_drive_file\n\n              \n\n              \n\n                \nopen_in_browser\n\n              \n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/papers.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/papers.json'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      return this.isLoaded ? '' : 'Loading ...';\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n    },\n    ensureArray(x) {\n      return Array.isArray(x) ? x : [x]\n    },\n    eventData(item) {\n      return [\n        item.event.title,\n        item.event.place,\n        item.event.info\n      ].filter(x => x).join(\", \")\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Papers"
        },
        {
            "location": "/biggis-papers/#list-of-papers",
            "text": "file_download \n       \n       \n         edit \n       \n     \n     \n       \n         \n           \n             \n              {{ item.title }}\n             \n             \n               {{author}} \n             \n             \n              {{ item.note }}\n               \n                {{ eventData(item) }}\n               \n             \n             \n               {{ item.date }} \n               \n               \n                 open_in_new \n               \n               \n                 insert_drive_file \n               \n               \n                 open_in_browser \n               \n             \n           \n         \n       \n     \n       \nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}     \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/papers.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/papers.json'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      return this.isLoaded ? '' : 'Loading ...';\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n    },\n    ensureArray(x) {\n      return Array.isArray(x) ? x : [x]\n    },\n    eventData(item) {\n      return [\n        item.event.title,\n        item.event.place,\n        item.event.info\n      ].filter(x => x).join(\", \")\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Papers"
        },
        {
            "location": "/biggis-presentations/",
            "text": "List of Presentations\n\u00b6\n\n\n\n  \n\n    \n\n      \n\n      \n\n      \n\n      \n\n        \nfile_download\n\n      \n\n      \n\n        \nedit\n\n      \n\n    \n\n    \n\n      \n\n        \n\n          \n\n            \n\n              {{ item.title }}\n            \n\n            \n\n              \n{{author}}\n\n            \n\n            \n\n              {{ item.note }}\n              \n\n                {{ eventData(item) }}\n              \n\n            \n\n            \n\n              \n{{ item.date }}\n\n              \n\n              \n\n                \nopen_in_new\n\n              \n\n              \n\n                \ninsert_drive_file\n\n              \n\n              \n\n                \nopen_in_browser\n\n              \n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/presentations.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/presentations.json'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      return this.isLoaded ? '' : 'Loading ...';\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n    },\n    ensureArray(x) {\n      return Array.isArray(x) ? x : [x]\n    },\n    eventData(item) {\n      return [\n        item.event.title,\n        item.event.place,\n        item.event.info\n      ].filter(x => x).join(\", \")\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Presentations"
        },
        {
            "location": "/biggis-presentations/#list-of-presentations",
            "text": "file_download \n       \n       \n         edit \n       \n     \n     \n       \n         \n           \n             \n              {{ item.title }}\n             \n             \n               {{author}} \n             \n             \n              {{ item.note }}\n               \n                {{ eventData(item) }}\n               \n             \n             \n               {{ item.date }} \n               \n               \n                 open_in_new \n               \n               \n                 insert_drive_file \n               \n               \n                 open_in_browser \n               \n             \n           \n         \n       \n     \n       \nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}     \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/presentations.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/presentations.json'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      return this.isLoaded ? '' : 'Loading ...';\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n    },\n    ensureArray(x) {\n      return Array.isArray(x) ? x : [x]\n    },\n    eventData(item) {\n      return [\n        item.event.title,\n        item.event.place,\n        item.event.info\n      ].filter(x => x).join(\", \")\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Presentations"
        },
        {
            "location": "/biggis-press/",
            "text": "List of Press Releases\n\u00b6\n\n\n\nth a * { float:right; color: white }\n\n\n\n\n\n  \n\n    \n\n      \n\n        \n{{item.title}},\n\n        \n{{item.title}},\n\n        {{item.date}}\n      \n\n    \n\n  \n\n  \nLoading list ...\n\n\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/press.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/press.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json())\n      this.items = json.sort(sortByDate)\n    }\n  }\n})\nvueapp.loadItems() // async load",
            "title": "List of Press Releases"
        },
        {
            "location": "/biggis-press/#list-of-press-releases",
            "text": "th a * { float:right; color: white }  \n   \n     \n       \n         {{item.title}}, \n         {{item.title}}, \n        {{item.date}}\n       \n     \n   \n   Loading list ...     \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/press.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/press.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json())\n      this.items = json.sort(sortByDate)\n    }\n  }\n})\nvueapp.loadItems() // async load",
            "title": "List of Press Releases"
        },
        {
            "location": "/consortium/",
            "text": "Project Consortium\n\u00b6\n\n\n\n\n\n\n\nProject Partners\n\u00b6\n\n\n\n\nFZI Forschungszentrum Informatik am KIT\n\n\nUniversit\u00e4t Konstanz\n\n\nHochschule Karlsruhe\n\n\nDisy Informationssysteme GmbH\n\n\nEXASOL AG\n\n\nEFTAS Fernerkundung Technologietransfer GmbH\n\n\nLandesanstalt f\u00fcr Umwelt Messungen und Naturschutz\n\n\n\n\nAssociated Partners\n\u00b6\n\n\n\n\nTHW Karlsruhe\n\n\nStadt Karlsruhe",
            "title": "Project Consortium"
        },
        {
            "location": "/consortium/#project-consortium",
            "text": "",
            "title": "Project Consortium"
        },
        {
            "location": "/consortium/#project-partners",
            "text": "FZI Forschungszentrum Informatik am KIT  Universit\u00e4t Konstanz  Hochschule Karlsruhe  Disy Informationssysteme GmbH  EXASOL AG  EFTAS Fernerkundung Technologietransfer GmbH  Landesanstalt f\u00fcr Umwelt Messungen und Naturschutz",
            "title": "Project Partners"
        },
        {
            "location": "/consortium/#associated-partners",
            "text": "THW Karlsruhe  Stadt Karlsruhe",
            "title": "Associated Partners"
        },
        {
            "location": "/contributing/",
            "text": "Contributing\n\u00b6\n\n\nWe value all kinds of contributions from the community, not just actual\ncode. If you do like to contribute actual code in the form of bug fixes, new\nfeatures or other patches this page gives you more info on how to do it.\n\n\nGit Branching Model\n\u00b6\n\n\nThe BigGIS team follows the standard practice of using the\n\nmaster\n branch as main integration branch.\n\n\nGit Commit Messages\n\u00b6\n\n\nWe follow the 'imperative present tense' style for commit messages.\n(e.g. \"Add new EnterpriseWidgetLoader instance\")\n\n\nIssue Tracking\n\u00b6\n\n\nIf you find a bug and would like to report it please go to the github\nissue tracker of a given sub-project and file an issue.\n\n\nPull Requests\n\u00b6\n\n\nIf you'd like to submit a code contribution please fork BigGIS and\nsend us pull request against the \nmaster\n branch. Like any other open\nsource project, we might ask you to go through some iterations of\ndiscussion and refinement before merging.\n\n\nContributing documentation\n\u00b6\n\n\nsee \nDocumentation Howto",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#contributing",
            "text": "We value all kinds of contributions from the community, not just actual\ncode. If you do like to contribute actual code in the form of bug fixes, new\nfeatures or other patches this page gives you more info on how to do it.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#git-branching-model",
            "text": "The BigGIS team follows the standard practice of using the master  branch as main integration branch.",
            "title": "Git Branching Model"
        },
        {
            "location": "/contributing/#git-commit-messages",
            "text": "We follow the 'imperative present tense' style for commit messages.\n(e.g. \"Add new EnterpriseWidgetLoader instance\")",
            "title": "Git Commit Messages"
        },
        {
            "location": "/contributing/#issue-tracking",
            "text": "If you find a bug and would like to report it please go to the github\nissue tracker of a given sub-project and file an issue.",
            "title": "Issue Tracking"
        },
        {
            "location": "/contributing/#pull-requests",
            "text": "If you'd like to submit a code contribution please fork BigGIS and\nsend us pull request against the  master  branch. Like any other open\nsource project, we might ask you to go through some iterations of\ndiscussion and refinement before merging.",
            "title": "Pull Requests"
        },
        {
            "location": "/contributing/#contributing-documentation",
            "text": "see  Documentation Howto",
            "title": "Contributing documentation"
        },
        {
            "location": "/docs-howto/",
            "text": "Documentation Howto\n\u00b6\n\n\nWe use \nmkdocs\n for documenting the project.\nThe goal is to make the documentation process as simple as possible,\nto allow versioning and to use pull requests for text reviews.\nI should edit the docs locally, preview it in a browser and then suggest a pull request.\n\n\nThe amount of formatting elements is deliberately small.\nApart from wiki, we try to keep each page self contained and to minimize\ninterlinking between pages because it only complicates reading of the docs.\n\n\nThe documentation is written as a set of Markdown files within the \ndocs/\n directory and after deployment\navailable as a static website: \nDocs Website\n.\n\n\nBefore building the docs\n\u00b6\n\n\nFirst of all, you need \npython 3\n and \npip3\n to be installed.\n(On some Linux distros, you need to use pip3 instead of pip)\n\n\nUsing pip, you need to install the following packages:\n\n\n\n\nmkdocs\n : Provides the executable command \nmkdocs\n.\n\n\nmkdocs-material\n : A material design theme. See also \nthis page\n.\n\n\npyembed-markdown\n : A markdown extension that allows for embedding Youtube videos in documents.\n                         See also \nthis page\n.\n\n\nmkdocs-awesome-pages-plugin\n : This plugin automatically generates the pages section from directory structure.\n  (For more info see \nthis github repository\n)\n\n\n\n\n\n\nHow to install as a user (recommended)\n\n\nYou can install the packages locally as a user into \n~/.local/\n\n\npip3 install --user mkdocs\npip3 install --user mkdocs-material\npip3 install --user pyembed-markdown\npip3 install --user mkdocs-awesome-pages-plugin\n\n\n\n\n\n\n\nHow to install system-wide as root (not recommended)\n\n\npip3 install mkdocs\npip3 install mkdocs-material\npip3 install pyembed-markdown\npip3 install mkdocs-awesome-pages-plugin\n\n\n\n\n\n\n\n\nHow to upgrade (as a user)\n\n\nMake sure you are using \nmkdocs version 0.17.2+\n.\n\npip3 install -U --user mkdocs\npip3 install -U --user mkdocs-material\npip3 install -U --user pyembed-markdown\npip3 install -U --user mkdocs-awesome-pages-plugin\n\n\n\n\n\nRecommended editor\n\u00b6\n\n\nSince we use the markdown format, you can use any plain text editor.\nHowever, we suggest to use \nIntelliJ IDEA\n\nwith the \nMarkdown Support plugin\n (both are free) which gives you:\n\n\n\n\nsyntax highlighting\n\n\npath completion of links such as image file names\n\n\nrefactoring, which is handy when renaming markdown files which are liked from other files\n\n\nfancy search\n\n\noutline of the document structure\n\n\nautomated simplified preview (which is not that important due to the mkdocs hot-reload)\n\n\n\n\nHow to edit\n\u00b6\n\n\nBefore editing the documentation, start the live-reloading docs server\nusing \nmkdocs serve\n within the project root directory.\nThen, open the page \nhttp://127.0.0.1:8000\n in your\nbrowser and watch your edits being reloaded automatically.\n\n\n1\nmkdocs serve\n\n\n\n\n\n\nINFO    -  Building documentation... \nINFO    -  Cleaning site directory \n[I 171024 15:03:51 server:283] Serving on http://127.0.0.1:8000\n[I 171024 15:03:51 handlers:60] Start watching changes\n\n\n\n\nYou can now edit the markdown documents within the \ndocs/\n directory.\n\n\nDeployment\n\u00b6\n\n\nUsing the command \nmkdocs gh-deploy\n we can generate a static \nDocs Website\n\nand deploy it automatically as a github page (served from \ngh-pages\n branch).\n\n\n\n\nInfo\n\n\nThe newly deployed version appears after few seconds.\n\n\n\n\n\n\nWarning\n\n\nThis operation is relevant only to the administrators of the \nbiggis-docs\n repository.\nAs a regular contributor, you should only preview your local changes and create\na pull request instead of directly deploying the built docs to the gh-pages branch.\n\n\n\n\nDocumentation layout\n\u00b6\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n  index.md    # The documentation homepage.\n  ...         # Other markdown pages, images and other files.\n\n\n\n\n\nFor the sake of simplicity, we use two-level hierarchy inside \ndocs/\n:\n\n\n\n\nLevel 1\n : areas (directories) that appear in the main menu.\n\n\nLevel 2\n : pages (markdown files) that appear in the left side bar.\n\n\nLevel 3\n : headings (H1, H2, ...) that appear in the table of contents on the right\n\n\n\n\n\n\nWarning\n\n\nDo not use spaces in file names. Replace them with underscores \n_\n.\nThis allows for easier refactoring because spaces are transformed to \n%20\n in markdown.\n\n\n\n\nFormatting examples\n\u00b6\n\n\nSectioning, Headings and Table of Contents\n# Chapter\n\n## Section\n\n### Subsection\n\n\n\n\nFootnotes\nSome text with a footnote[^LABEL]\n\n[^LABEL]: Text of the footnote\n\n\n\n\nSee also \nhttps://squidfunk.github.io/mkdocs-material/extensions/footnotes/\nCitations, Notes and Admonition\n!!! cite\n    Here comes the citation including authors, title, year, doi, url ...\n\n\n\n\nCite\nHere comes the citation including authors, title, year, doi, url ...\n!!! note\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n\n\n\n\nNote\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa.\nFor more options see \nhttps://squidfunk.github.io/mkdocs-material/extensions/admonition/\nCollapsible blocks\n??? \"Phasellus posuere in sem ut cursus\"\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n\n\n\n\nPhasellus posuere in sem ut cursus\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa.\nFor more information see \nhttps://facelessuser.github.io/pymdown-extensions/extensions/details/\nImages\nYou can include images into the documentation in the following format:\nSVG\n (scalable vectors).\nJPG\n (photos)\nPNG\n (raster graphics)\nIn contrast to scientific papers, it is not possible to create references to numbered figures in markdown.\n![Image \"alt\" description](path/to/image.svg)\n\n\n\n\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#images-and-media\nNote\nWhen editing a file e.g. \npath/to/ABC.md\n, store all related images in the same folder\nfolder (\npath/to/ABC\n). This way, different topics are better encapsulated.\nFigures with caption (on top)\nWith the following hack, you can create a nice looking caption rendered under a figure.\n!!! info \"Figure: Here comes a single line title\"\n    ![](path/to/image.svg)\n\n    Here comes some additional multi-line text.\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n    Morbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis.\n\n\n\n\nFigure: Here comes a single line title\nHere comes some additional multi-line text.\nLorem ipsum dolor sit amet, consectetur adipiscing elit.\nMorbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis.\nTables\nFirst Header | Second Header | Third Header\n------------ | ------------- | ------------\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell\n\n\n\n\nFirst Header\nSecond Header\nThird Header\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#tables\nTables with alignment\nLeft         | Center        | Right\n---          |:--            |--:\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell\n\n\n\n\nLeft\nCenter\nRight\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#tables\nMathematical Formulas\nFormula are generated using \nMathJax\n, which is similar to LaTeX.\nSee also this \nquick reference\n.\n$$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$\n\n\n\n\n\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\n\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\nSource Code with Code Highlighting\nCode can be displayed inline like this:\n`print 1+{variable}`\n\n\n\n\nOr it can be displayed in a code block with optional syntax highlighting if the language is specified.\n```python\ndef my_function():\n    \"just a test\"\n    print 8/2 \n```\n\n\n\n\ndef\n \nmy_function\n():\n\n    \n\"just a test\"\n\n    \nprint\n \n8\n/\n2\n \n\n\n\nSmart Symbols\nSome smart symbols: -->,  <--, 1st, 2nd, 1/4\n\n\n\n\nSome smart symbols: \u2192,  \u2190, 1\nst\n, 2\nnd\n, \u00bc\nSee also \nhttps://facelessuser.github.io/pymdown-extensions/extensions/smartsymbols/\nSequence diagrams\n```sequence\nTitle: Example sequence diagram\nA->B: Sync call\nB-->A: Sync return\nA->C: Another sync call\nC->>D: Async call\nD-->>C: Async return\n```\n\n\n\nTitle: Example sequence diagram\nA->B: Sync call\nB-->A: Sync return\nA->C: Another sync call\nC->>D: Async call\nD-->>C: Async return\n\n\nEmbedded Youtube Videos\n[!embed](https://www.youtube.com/watch?v=QQKVzZpXTpQ)\n\n\n\n\n\n\nFor more information see \nhttps://pyembed.github.io/usage/markdown/\nHTML (please only in special cases)\nIn special cases, you can also use raw HTML in your document.\n <style>.special img {height:32px; vertical-align:middle}</style>\n <div class=\"special\">\n   [![](https://www.gstatic.com/webp/gallery3/5.png)](https://developers.google.com/speed/webp/gallery2)\n   Click on this icon\n </div>\n\n\n\n\n.special img {height:32px; vertical-align:middle}\n\n \n\n   \n\n   Click on this icon",
            "title": "Documentation Howto"
        },
        {
            "location": "/docs-howto/#documentation-howto",
            "text": "We use  mkdocs  for documenting the project.\nThe goal is to make the documentation process as simple as possible,\nto allow versioning and to use pull requests for text reviews.\nI should edit the docs locally, preview it in a browser and then suggest a pull request.  The amount of formatting elements is deliberately small.\nApart from wiki, we try to keep each page self contained and to minimize\ninterlinking between pages because it only complicates reading of the docs.  The documentation is written as a set of Markdown files within the  docs/  directory and after deployment\navailable as a static website:  Docs Website .",
            "title": "Documentation Howto"
        },
        {
            "location": "/docs-howto/#before-building-the-docs",
            "text": "First of all, you need  python 3  and  pip3  to be installed.\n(On some Linux distros, you need to use pip3 instead of pip)  Using pip, you need to install the following packages:   mkdocs  : Provides the executable command  mkdocs .  mkdocs-material  : A material design theme. See also  this page .  pyembed-markdown  : A markdown extension that allows for embedding Youtube videos in documents.\n                         See also  this page .  mkdocs-awesome-pages-plugin  : This plugin automatically generates the pages section from directory structure.\n  (For more info see  this github repository )    How to install as a user (recommended)  You can install the packages locally as a user into  ~/.local/  pip3 install --user mkdocs\npip3 install --user mkdocs-material\npip3 install --user pyembed-markdown\npip3 install --user mkdocs-awesome-pages-plugin    How to install system-wide as root (not recommended)  pip3 install mkdocs\npip3 install mkdocs-material\npip3 install pyembed-markdown\npip3 install mkdocs-awesome-pages-plugin    How to upgrade (as a user)  Make sure you are using  mkdocs version 0.17.2+ . pip3 install -U --user mkdocs\npip3 install -U --user mkdocs-material\npip3 install -U --user pyembed-markdown\npip3 install -U --user mkdocs-awesome-pages-plugin",
            "title": "Before building the docs"
        },
        {
            "location": "/docs-howto/#recommended-editor",
            "text": "Since we use the markdown format, you can use any plain text editor.\nHowever, we suggest to use  IntelliJ IDEA \nwith the  Markdown Support plugin  (both are free) which gives you:   syntax highlighting  path completion of links such as image file names  refactoring, which is handy when renaming markdown files which are liked from other files  fancy search  outline of the document structure  automated simplified preview (which is not that important due to the mkdocs hot-reload)",
            "title": "Recommended editor"
        },
        {
            "location": "/docs-howto/#how-to-edit",
            "text": "Before editing the documentation, start the live-reloading docs server\nusing  mkdocs serve  within the project root directory.\nThen, open the page  http://127.0.0.1:8000  in your\nbrowser and watch your edits being reloaded automatically.  1 mkdocs serve   INFO    -  Building documentation... \nINFO    -  Cleaning site directory \n[I 171024 15:03:51 server:283] Serving on http://127.0.0.1:8000\n[I 171024 15:03:51 handlers:60] Start watching changes  You can now edit the markdown documents within the  docs/  directory.",
            "title": "How to edit"
        },
        {
            "location": "/docs-howto/#deployment",
            "text": "Using the command  mkdocs gh-deploy  we can generate a static  Docs Website \nand deploy it automatically as a github page (served from  gh-pages  branch).   Info  The newly deployed version appears after few seconds.    Warning  This operation is relevant only to the administrators of the  biggis-docs  repository.\nAs a regular contributor, you should only preview your local changes and create\na pull request instead of directly deploying the built docs to the gh-pages branch.",
            "title": "Deployment"
        },
        {
            "location": "/docs-howto/#documentation-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n  index.md    # The documentation homepage.\n  ...         # Other markdown pages, images and other files.  For the sake of simplicity, we use two-level hierarchy inside  docs/ :   Level 1  : areas (directories) that appear in the main menu.  Level 2  : pages (markdown files) that appear in the left side bar.  Level 3  : headings (H1, H2, ...) that appear in the table of contents on the right    Warning  Do not use spaces in file names. Replace them with underscores  _ .\nThis allows for easier refactoring because spaces are transformed to  %20  in markdown.",
            "title": "Documentation layout"
        },
        {
            "location": "/docs-howto/#formatting-examples",
            "text": "Sectioning, Headings and Table of Contents # Chapter\n\n## Section\n\n### Subsection  Footnotes Some text with a footnote[^LABEL]\n\n[^LABEL]: Text of the footnote  See also  https://squidfunk.github.io/mkdocs-material/extensions/footnotes/ Citations, Notes and Admonition !!! cite\n    Here comes the citation including authors, title, year, doi, url ...  Cite Here comes the citation including authors, title, year, doi, url ... !!! note\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.  Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa. For more options see  https://squidfunk.github.io/mkdocs-material/extensions/admonition/ Collapsible blocks ??? \"Phasellus posuere in sem ut cursus\"\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.  Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa. For more information see  https://facelessuser.github.io/pymdown-extensions/extensions/details/ Images You can include images into the documentation in the following format: SVG  (scalable vectors). JPG  (photos) PNG  (raster graphics) In contrast to scientific papers, it is not possible to create references to numbered figures in markdown. ![Image \"alt\" description](path/to/image.svg)  See also  http://www.mkdocs.org/user-guide/writing-your-docs/#images-and-media Note When editing a file e.g.  path/to/ABC.md , store all related images in the same folder\nfolder ( path/to/ABC ). This way, different topics are better encapsulated. Figures with caption (on top) With the following hack, you can create a nice looking caption rendered under a figure. !!! info \"Figure: Here comes a single line title\"\n    ![](path/to/image.svg)\n\n    Here comes some additional multi-line text.\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n    Morbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis.  Figure: Here comes a single line title Here comes some additional multi-line text.\nLorem ipsum dolor sit amet, consectetur adipiscing elit.\nMorbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis. Tables First Header | Second Header | Third Header\n------------ | ------------- | ------------\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell  First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell See also  http://www.mkdocs.org/user-guide/writing-your-docs/#tables Tables with alignment Left         | Center        | Right\n---          |:--            |--:\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell  Left Center Right Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell See also  http://www.mkdocs.org/user-guide/writing-your-docs/#tables Mathematical Formulas Formula are generated using  MathJax , which is similar to LaTeX.\nSee also this  quick reference . $$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$  \n\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \n\\frac{n!}{k!(n-k)!} = \\binom{n}{k} Source Code with Code Highlighting Code can be displayed inline like this: `print 1+{variable}`  Or it can be displayed in a code block with optional syntax highlighting if the language is specified. ```python\ndef my_function():\n    \"just a test\"\n    print 8/2 \n```  def   my_function (): \n     \"just a test\" \n     print   8 / 2    Smart Symbols Some smart symbols: -->,  <--, 1st, 2nd, 1/4  Some smart symbols: \u2192,  \u2190, 1 st , 2 nd , \u00bc See also  https://facelessuser.github.io/pymdown-extensions/extensions/smartsymbols/ Sequence diagrams ```sequence\nTitle: Example sequence diagram\nA->B: Sync call\nB-->A: Sync return\nA->C: Another sync call\nC->>D: Async call\nD-->>C: Async return\n```  Title: Example sequence diagram\nA->B: Sync call\nB-->A: Sync return\nA->C: Another sync call\nC->>D: Async call\nD-->>C: Async return  Embedded Youtube Videos [!embed](https://www.youtube.com/watch?v=QQKVzZpXTpQ)   For more information see  https://pyembed.github.io/usage/markdown/ HTML (please only in special cases) In special cases, you can also use raw HTML in your document.  <style>.special img {height:32px; vertical-align:middle}</style>\n <div class=\"special\">\n   [![](https://www.gstatic.com/webp/gallery3/5.png)](https://developers.google.com/speed/webp/gallery2)\n   Click on this icon\n </div>  .special img {height:32px; vertical-align:middle} \n  \n    \n   Click on this icon",
            "title": "Formatting examples"
        },
        {
            "location": "/architecture/ArchOverview/",
            "text": "Overview\n\u00b6\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/biggis-infrastructure\n\n\n\n\n\n\nThe BigGIS architecture, as depicted in the picture above, consists of several layers that are \nbriefly discussed in the following.\n\n\nModelling\n\u00b6\n\n\nPipelines in BigGIS are modelled leveraging \nStreamPipes\n. StreamPipes allows to \ntransform and analyse sensor landscape and other data streams with less programming effort. We \nextend StreamPipes to support geo-spatial data analytics, e.g. raster data.\n\n\nAnalytics\n\u00b6\n\n\nTo process and analyze the geo-spatial data, BigGIS relies on multiple big data analytics \nframeworks. While \nApache Flink\n is mainly used for sensor data,\nsome application use cases need to process geo-spatial raster or vector data in batches as well. \nThus, we integrate \nApache Spark\n featuring \nGeoTrellis\n into our architectural design in order to process geographic data. GeoTrellis provides a \nnumber of operations to manipulate raster data including map algebra operations. In addition, we \nprovide other data science notebooks (RStudio, Jupyter) to exploratively analyze the data.\n\n\nMiddleware & Connectors\n\u00b6\n\n\nApache Kafka\n is used as the primary message broker. It handles the \ncommunication between the data processing elements, i.e. nodes, within the analytics pipelines. \nBesides, \nActiveMQ\n is another message broker which can be used in \naddition to Kafka. Currently, the main purpose is to provide an endpoint for the websocket \nconnections required by the real-time dashboard of the StreamPipes UI.\n\n\nStorage Backends\n\u00b6\n\n\nInternally, BigGIS uses a variety of different storage backends for designated purposes.\n\n\n\n\nHDFS\n for GeoTrellis catalog and Spark jobs.\n\n\nExasol\n for fast access of stored data.\n\n\nCouchDB\n for pipelines, users and visualizations created in the \ndashboard.\n\n\nRDF4J\n (formerly Sesame) as a semantic backend of StreamPipes.\n\n\n\n\nContainer Management\n\u00b6\n\n\nRunning these containers in a distributed manner requires a wide variety of technologies, that \nmust be integrated and\nmanaged throughout their lifecycle. To easily deploy our containers, our infrastructure is \ndesigned to run on\n\nRancher\n as our container management platform. Rancher enables \norganizations to run and manage Docker and Kubernetes in production, providing four major \ncomponents, that are:\n\n\n\n\nInfrastructure Orchestration\n\n\nContainer Orchestration and Scheduling\n\n\nApplication Catalog\n\n\nAuthentication\n\n\n\n\nThat way, it is fairly easy to set up a distributed Rancher cluster in a couple of minutes (see \nthe official \ndocumentation\n for more information).\n\n\nInfrastructure\n\u00b6\n\n\nBigGIS infrastructure leverages \nbwCloud\n Infrastructure-as-a-Service \n(IaaS) offer powered by Openstack. See the section \nbwCloud\n for \nadditional information about the infrastructure setup.",
            "title": "Overview"
        },
        {
            "location": "/architecture/ArchOverview/#overview",
            "text": "Note  Related repository is  https://github.com/biggis-project/biggis-infrastructure    The BigGIS architecture, as depicted in the picture above, consists of several layers that are \nbriefly discussed in the following.",
            "title": "Overview"
        },
        {
            "location": "/architecture/ArchOverview/#modelling",
            "text": "Pipelines in BigGIS are modelled leveraging  StreamPipes . StreamPipes allows to \ntransform and analyse sensor landscape and other data streams with less programming effort. We \nextend StreamPipes to support geo-spatial data analytics, e.g. raster data.",
            "title": "Modelling"
        },
        {
            "location": "/architecture/ArchOverview/#analytics",
            "text": "To process and analyze the geo-spatial data, BigGIS relies on multiple big data analytics \nframeworks. While  Apache Flink  is mainly used for sensor data,\nsome application use cases need to process geo-spatial raster or vector data in batches as well. \nThus, we integrate  Apache Spark  featuring  GeoTrellis  into our architectural design in order to process geographic data. GeoTrellis provides a \nnumber of operations to manipulate raster data including map algebra operations. In addition, we \nprovide other data science notebooks (RStudio, Jupyter) to exploratively analyze the data.",
            "title": "Analytics"
        },
        {
            "location": "/architecture/ArchOverview/#middleware-connectors",
            "text": "Apache Kafka  is used as the primary message broker. It handles the \ncommunication between the data processing elements, i.e. nodes, within the analytics pipelines. \nBesides,  ActiveMQ  is another message broker which can be used in \naddition to Kafka. Currently, the main purpose is to provide an endpoint for the websocket \nconnections required by the real-time dashboard of the StreamPipes UI.",
            "title": "Middleware &amp; Connectors"
        },
        {
            "location": "/architecture/ArchOverview/#storage-backends",
            "text": "Internally, BigGIS uses a variety of different storage backends for designated purposes.   HDFS  for GeoTrellis catalog and Spark jobs.  Exasol  for fast access of stored data.  CouchDB  for pipelines, users and visualizations created in the \ndashboard.  RDF4J  (formerly Sesame) as a semantic backend of StreamPipes.",
            "title": "Storage Backends"
        },
        {
            "location": "/architecture/ArchOverview/#container-management",
            "text": "Running these containers in a distributed manner requires a wide variety of technologies, that \nmust be integrated and\nmanaged throughout their lifecycle. To easily deploy our containers, our infrastructure is \ndesigned to run on Rancher  as our container management platform. Rancher enables \norganizations to run and manage Docker and Kubernetes in production, providing four major \ncomponents, that are:   Infrastructure Orchestration  Container Orchestration and Scheduling  Application Catalog  Authentication   That way, it is fairly easy to set up a distributed Rancher cluster in a couple of minutes (see \nthe official  documentation  for more information).",
            "title": "Container Management"
        },
        {
            "location": "/architecture/ArchOverview/#infrastructure",
            "text": "BigGIS infrastructure leverages  bwCloud  Infrastructure-as-a-Service \n(IaaS) offer powered by Openstack. See the section  bwCloud  for \nadditional information about the infrastructure setup.",
            "title": "Infrastructure"
        },
        {
            "location": "/architecture/Components/",
            "text": "Responsible person for this section\n\n\nPatrick Wiener\n\n\n\n\nComponents\n\u00b6\n\n\ntbd. some infos about the structure and hierarchical composition of our Docker images.\n\n\n\n\nall sources should be on github\n\n\nimages should be hosted on dockerhub\n\n\nlist of docker images that should be available:\n\n\nHDFS (should use all bwCloud resources available to BigGIS)\n\n\nKafka with Zookeeper (overview of queues needed)\n\n\nFlink\n\n\nSpark\n\n\nGeotrellis libraries (part of the Spark container?)\n\n\nAccumulo with Geomesa (or Geowave)\n\n\nStreamPipes\n\n\nExasolution\n\n\nExasolution should support Accumulo(Geomesa/Geowave) through virtual schema\n\n\nExasolution should support indexing using 2D, 3D, 4D space filling curves (lat,lon,time,elevation)\n\n\n\n\n\n\nGeo-Server\n\n\nmit Plugin f\u00fcr Accumulo\n\n\nzur Transformation von Formaten\n\n\nauch als Datenquelle f\u00fcr Cadenza",
            "title": "Components"
        },
        {
            "location": "/architecture/Components/#components",
            "text": "tbd. some infos about the structure and hierarchical composition of our Docker images.   all sources should be on github  images should be hosted on dockerhub  list of docker images that should be available:  HDFS (should use all bwCloud resources available to BigGIS)  Kafka with Zookeeper (overview of queues needed)  Flink  Spark  Geotrellis libraries (part of the Spark container?)  Accumulo with Geomesa (or Geowave)  StreamPipes  Exasolution  Exasolution should support Accumulo(Geomesa/Geowave) through virtual schema  Exasolution should support indexing using 2D, 3D, 4D space filling curves (lat,lon,time,elevation)    Geo-Server  mit Plugin f\u00fcr Accumulo  zur Transformation von Formaten  auch als Datenquelle f\u00fcr Cadenza",
            "title": "Components"
        },
        {
            "location": "/architecture/Platform_bwCloud/",
            "text": "Responsible person for this section\n\n\nPatrick Wiener\n\n\n\n\nbwCloud\n\u00b6\n\n\n\n\nWithin the scope of BigGIS, we build our platform on top of the \nbwCloud\n offering. The Baden-W\u00fcrttemberg Cloud (bwCloud) \nprovides virtual machines (servers) for members of science- and research institutions in Baden-W\u00fcrttemberg (e.g. students and \nstaff-members) much like Amazon\u2019s EC2. Building on the openstack-platform the infrastructure is currently operated by four sites \nin Baden-W\u00fcrttemberg: the Universities of Mannheim, Karlsruhe, Ulm and Freiburg.\n\n\nInfrastructure\n\u00b6\n\n\nCurrently, we run a total of \nnine\n Virtual Machines (VM), serving different kinds of purposes, that are providing (1) \na single-node Rancher Testing environment, (2) a multi-node Rancher Cluster environment, and (3) infrastructure supporting services (e.g. private\nDocker registry for sensitive images, OpenVPN server to connect to the virtual private cloud).\n\n\n\n\n\n\n\n\n\n\nm1.xlarge\n\n\nm1.large\n\n\nm1.medium\n\n\n\n\n\n\n\n\n\n\nOS\n\n\nUbuntu 16.04\n\n\nUbuntu 16.04\n\n\nUbuntu 16.04\n\n\n\n\n\n\nCPU\n\n\n8 vCPU\n\n\n4 vCPU\n\n\n2 vCPU\n\n\n\n\n\n\nMemory\n\n\n8 GB\n\n\n8 GB\n\n\n4 GB\n\n\n\n\n\n\nDisk\n\n\n50 GB\n\n\n50 GB\n\n\n50 GB\n\n\n\n\n\n\n#VM\n\n\n1\n\n\n7\n\n\n1\n\n\n\n\n\n\nUsage\n\n\nRancher Dev/Testing\n\n\nRancher Server, Rancher Cluster (5 VM), Docker Registry\n\n\nOpenVPN\n\n\n\n\n\n\n\n\nRancher is deployed as a set of Docker containers. Running Rancher involves launching at least two containers. One container as the management \nserver\n and another container on a node as an \nagent\n.\n\n\n\n\n\n\nFigure:\n\nContainer-based Rancher Setup in bwCloud.\n\n\n\n\nWhilte the server runs on a single VM, we set up a single node agent for development and testing environment as well as a five node cluster environment to run the BigGIS components in a distributed manner (see figure below). Both, the dev/testing as well as the cluster environment make use of a the network file system (NFS) in order to overcome the problem of strictly coupeling a service to a node. While this is only mandatory in the cluster environment, it makes sense to setup the dev/testing environment as an exact clone to have the same workflows within Rancher.",
            "title": "bwCloud"
        },
        {
            "location": "/architecture/Platform_bwCloud/#bwcloud",
            "text": "Within the scope of BigGIS, we build our platform on top of the  bwCloud  offering. The Baden-W\u00fcrttemberg Cloud (bwCloud) \nprovides virtual machines (servers) for members of science- and research institutions in Baden-W\u00fcrttemberg (e.g. students and \nstaff-members) much like Amazon\u2019s EC2. Building on the openstack-platform the infrastructure is currently operated by four sites \nin Baden-W\u00fcrttemberg: the Universities of Mannheim, Karlsruhe, Ulm and Freiburg.",
            "title": "bwCloud"
        },
        {
            "location": "/architecture/Platform_bwCloud/#infrastructure",
            "text": "Currently, we run a total of  nine  Virtual Machines (VM), serving different kinds of purposes, that are providing (1) \na single-node Rancher Testing environment, (2) a multi-node Rancher Cluster environment, and (3) infrastructure supporting services (e.g. private\nDocker registry for sensitive images, OpenVPN server to connect to the virtual private cloud).      m1.xlarge  m1.large  m1.medium      OS  Ubuntu 16.04  Ubuntu 16.04  Ubuntu 16.04    CPU  8 vCPU  4 vCPU  2 vCPU    Memory  8 GB  8 GB  4 GB    Disk  50 GB  50 GB  50 GB    #VM  1  7  1    Usage  Rancher Dev/Testing  Rancher Server, Rancher Cluster (5 VM), Docker Registry  OpenVPN     Rancher is deployed as a set of Docker containers. Running Rancher involves launching at least two containers. One container as the management  server  and another container on a node as an  agent .    Figure: \nContainer-based Rancher Setup in bwCloud.   Whilte the server runs on a single VM, we set up a single node agent for development and testing environment as well as a five node cluster environment to run the BigGIS components in a distributed manner (see figure below). Both, the dev/testing as well as the cluster environment make use of a the network file system (NFS) in order to overcome the problem of strictly coupeling a service to a node. While this is only mandatory in the cluster environment, it makes sense to setup the dev/testing environment as an exact clone to have the same workflows within Rancher.",
            "title": "Infrastructure"
        },
        {
            "location": "/architecture/Semantic/",
            "text": "Semantic\n\u00b6\n\n\n\n\nResponsible person for this section\n\n\nMatthias Frank\n\n\n\n\nDue to the heterogeneity of \ndata sources\n within BigGIS, an explicit semantic is required that provides\nmeaningful descriptions of formats, syntax and semantics of each source. For that purpose, we have developed a \n\nsemantic data management\n platform that enables five star meta data including shared vocabularies\nand background knowledge from concept that are available as Linked Open Data. Using the knowledge base retrieved from\nthe semantic data management platform, we allow users of the platform to add additional meta data to JSON messages\nof environmental observations. Bases on these annotations, we provide \nsemantic message enrichment\n\non-the-fly and add explicit semantics to the key-value pairs of the observation, including provenance, measured quantities,\nunits and data types. The semantically enriched messages are the basis for further evaluation and transformation.\nOne example for further processing of a semantically enriched data stream is \nshape constraint validation\n,\nwhich we demonstrate in the last subsection.\n\n\nSemantic Data Management\n\u00b6\n\n\nThe first step towards unified and meaningful descriptions of observations is the semantic data management platform.\nWe aim to have these descriptions in an open, machine processable and interlinked format, or in short, as five star data:\n\n\n\n\n\n\nFigure:\n\nFrom Open Data to Five Star Data\n\n\n\n\nFor that purpose we introduce Linked Data Wiki (LD-Wiki), a MediaWiki bases knowledge management platform. The core of\nLD-Wiki is the Linked Data Management Module (LDaMM) for updating, querying, reasoning, linking and rule execution\non semantic statements in both, LOD and local storage. The architecture of LD-Wiki is as follows:\n\n\n\n\n\n\nFigure:\n\nArchitecture of Linked Data Wiki\n\n\n\n\nLD-Wiki enables easy reuse of properties of well-known entities and context knowledge from LOD. As an example,\nthe common knowledge of the concept of a city is already described in LOD sources like schema.org, WikiData or DBPedia.\nTo find instances of this concept in LOD, we link the category in LD-Wiki to the according LOD concepts:\n\n\n\n\n\n\nFigure:\n\nLinking concepts from LOD to LD-Wiki\n\n\n\n\nWhen creating new instances of this category withing LD-Wiki, existing instances of the related concepts in LOD with \na similar label can be employed and the values of their properties can be reused in the context of LD-Wiki:\n\n\n\n\n\n\nFigure:\n\nLinking entities from LOD to LD-Wiki\n\n\n\n\nSemantic Message Enrichment\n\u00b6\n\n\nThe second step towards unified and meaningful descriptions of observations is the annotation of observation messages.\nWith these annotations, we add explicit semantics to the key-value pairs of a JSON message:\n\n\n\n\n\n\nFigure:\n\nExplicit Semantics for JSON messages\n\n\n\n\nIn the backend, the annotations are linked to shared concepts for unified and meaningful interpretation of observations\nfrom heterogeneous sensors. Based on these annotations, an uplifting of non-semantic data streams from heterogeneous\nobservation stations with explicit semantics can be performed on-the-fly:\n\n\n\n\n\n\nFigure:\n\nSemantic uplifting of JSON messages to JSON-LD based on meta data\n\n\n\n\nThe semantic enrichment process creates a new message with a list of values. Each \nvalue\nvalue\n of the new message is assigned\nfor each member \nn\nn\n of a message with the according explicit semantic of meta data \nm\nm\n as\n\n\n\n\n\n    \\sum_{n=0}^N \\sum_{m=0}^M value_{n,m} := \n    \\begin{cases}\n        member_{n,val} \\circ meta_{m,val}       & \\text{if } member_{n,key} = meta_{m,key}\\\\\n        \\emptyset                                                               & \\text{otherwise}\n    \\end{cases}\n\n\n\n\n    \\sum_{n=0}^N \\sum_{m=0}^M value_{n,m} := \n    \\begin{cases}\n        member_{n,val} \\circ meta_{m,val}       & \\text{if } member_{n,key} = meta_{m,key}\\\\\n        \\emptyset                                                               & \\text{otherwise}\n    \\end{cases}\n\n\n\n\n\nwith \nmember\nmember\n being the set of \nN\nN\n members of a concrete message without any meta data and \nmeta\nmeta\n being the set of meta\ndata for a class of messages with explicit semantic of \nM\nM\n known types of observations. Each element of both sets\nconsists of \nkey\nkey\n and \nvalue\nvalue\n. If \nkey_{n}\nkey_{n}\n of a member element is equal to \nkey_{m}\nkey_{m}\n of a meta data element,\n\nvalue_{n}\nvalue_{n}\n of member element is combined with \nvalue_{m}\nvalue_{m}\n of the meta data element using the binary operator \n\\circ\n\\circ\n\nin order to add explicit semantics to the original observation. The functionality of the binary operator \n\\circ\n\\circ\n is defined as\n\n\n\n\n\n    f(member, meta) = \n    \\begin{cases}\n        data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{string\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{number\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{bool\\right\\} \\\\\n            object(member, meta)                                                & \\quad  \\text{if } member \\in \\left\\{object\\right\\} \\\\\n            \\sum_{n=0}^N member_{n} \\circ meta_{n}          & \\quad  \\text{if } member \\in \\left\\{array\\right\\} \\\\\n        \\emptyset                                                                       & \\quad  \\text{if } member \\in \\left\\{null\\right\\}\n    \\end{cases}\n\n\n\n\n    f(member, meta) = \n    \\begin{cases}\n        data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{string\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{number\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{bool\\right\\} \\\\\n            object(member, meta)                                                & \\quad  \\text{if } member \\in \\left\\{object\\right\\} \\\\\n            \\sum_{n=0}^N member_{n} \\circ meta_{n}          & \\quad  \\text{if } member \\in \\left\\{array\\right\\} \\\\\n        \\emptyset                                                                       & \\quad  \\text{if } member \\in \\left\\{null\\right\\}\n    \\end{cases}\n\n\n\n\n\nThe function \ndata(member, meta)\ndata(member, meta)\n adds the data type property of \nmeta\nmeta\n to the new message and includes the literal\nvalue of \nmember\nmember\n in conjunction with the XSD data type associated with \nmeta\nmeta\n. The function \nobject(member, meta)\nobject(member, meta)\n\nadds the object property of \nmeta\nmeta\n to the new message and includes an URI reference to the object in \nmember\nmember\n.\nAs the value of an array is nothing else but a list of \nN\nN\n values, we can simply execute the function \nf(member, meta)\nf(member, meta)\n\non each element \nn\nn\n of that list. However, as we identify the meta data for each member by its key, there can only be\na single data type property or object property which is applied to all elements of an array. Different types of values\nwithin a single array are not supported.\n\n\nAs an example, we use the JSON messages of \nLUBW stations\n and push them to a \nmessage broker:\n\n\n{\n    \"no2\":61,\n    \"ozn\":10,\n    \"luqx\":0,\n    \"latitude\":48.18169,\n    \"heigth\":510,\n    \"so2\":0,\n    \"station\":\"DEBY189\",\n    \"pm10\":0,\n    \"timestamp\":1516191751218,\n    \"longitude\":11.46445\n}\n\n\n\n\n\n\n\nListing:\n\nExample of a LUBW observation as JSON\n\n\n\n\n\n\n\n\nFigure:\n\nExample for LUBW data stream\n\n\n\n\nAnother example are the JSON messages of \nsenseBox-based weather stations\n,\nwhich are pushed to another topic of the message broker:\n\n\n{\n    \"title\": \"Temperatur\",\n    \"unit\": \"\u00b0C\",\n    \"sensorType\": \"HDC1008\",\n    \"icon\": \"osem-thermometer\",\n    \"_id\": \"59ec966d49f6f80011c1239a\",\n    \"lastMeasurement\": {\n        \"value\": \"7.98\",\n        \"createdAt\": \"2018-01-18T13:02:14.330Z\"\n    }\n}\n\n\n\n\n\n\n\nListing:\n\nExample of a senseBox observation as JSON\n\n\n\n\n\n\n\n\nFigure:\n\nExample for senseBox data stream\n\n\n\n\nBy employing the explicit meta data, we gain a new message stream of JSON-LD messages with explicit semantic of\neach observation:\n\n\n\n\n\n\nFigure:\n\nExample for semantically enriched data stream\n\n\n\n\nSemantic Data Validation\n\u00b6\n\n\nOnce we have a data stream of observation messages from heterogeneous data sources but with explicit semantics,\nwe can also perform a semantic validation for all messages. For these validations, we again employ our annotation\nplatform to define data shapes (patterns) that should be fulfilled by all observation messages, regardless from which observation\nstation they are retrieved:\n\n\n\n\n\n\nFigure:\n\nShape constraint annotation\n\n\n\n\nApplying these shapes on the JSON-LD observation messages tells us immediately whether the observation is conform\nto the defined shape or not:\n\n\n\n\n\n\nFigure:\n\nShape constraint validation of JSON-LD messages",
            "title": "Semantic"
        },
        {
            "location": "/architecture/Semantic/#semantic",
            "text": "Responsible person for this section  Matthias Frank   Due to the heterogeneity of  data sources  within BigGIS, an explicit semantic is required that provides\nmeaningful descriptions of formats, syntax and semantics of each source. For that purpose, we have developed a  semantic data management  platform that enables five star meta data including shared vocabularies\nand background knowledge from concept that are available as Linked Open Data. Using the knowledge base retrieved from\nthe semantic data management platform, we allow users of the platform to add additional meta data to JSON messages\nof environmental observations. Bases on these annotations, we provide  semantic message enrichment \non-the-fly and add explicit semantics to the key-value pairs of the observation, including provenance, measured quantities,\nunits and data types. The semantically enriched messages are the basis for further evaluation and transformation.\nOne example for further processing of a semantically enriched data stream is  shape constraint validation ,\nwhich we demonstrate in the last subsection.",
            "title": "Semantic"
        },
        {
            "location": "/architecture/Semantic/#semantic-data-management",
            "text": "The first step towards unified and meaningful descriptions of observations is the semantic data management platform.\nWe aim to have these descriptions in an open, machine processable and interlinked format, or in short, as five star data:    Figure: \nFrom Open Data to Five Star Data   For that purpose we introduce Linked Data Wiki (LD-Wiki), a MediaWiki bases knowledge management platform. The core of\nLD-Wiki is the Linked Data Management Module (LDaMM) for updating, querying, reasoning, linking and rule execution\non semantic statements in both, LOD and local storage. The architecture of LD-Wiki is as follows:    Figure: \nArchitecture of Linked Data Wiki   LD-Wiki enables easy reuse of properties of well-known entities and context knowledge from LOD. As an example,\nthe common knowledge of the concept of a city is already described in LOD sources like schema.org, WikiData or DBPedia.\nTo find instances of this concept in LOD, we link the category in LD-Wiki to the according LOD concepts:    Figure: \nLinking concepts from LOD to LD-Wiki   When creating new instances of this category withing LD-Wiki, existing instances of the related concepts in LOD with \na similar label can be employed and the values of their properties can be reused in the context of LD-Wiki:    Figure: \nLinking entities from LOD to LD-Wiki",
            "title": "Semantic Data Management"
        },
        {
            "location": "/architecture/Semantic/#semantic-message-enrichment",
            "text": "The second step towards unified and meaningful descriptions of observations is the annotation of observation messages.\nWith these annotations, we add explicit semantics to the key-value pairs of a JSON message:    Figure: \nExplicit Semantics for JSON messages   In the backend, the annotations are linked to shared concepts for unified and meaningful interpretation of observations\nfrom heterogeneous sensors. Based on these annotations, an uplifting of non-semantic data streams from heterogeneous\nobservation stations with explicit semantics can be performed on-the-fly:    Figure: \nSemantic uplifting of JSON messages to JSON-LD based on meta data   The semantic enrichment process creates a new message with a list of values. Each  value value  of the new message is assigned\nfor each member  n n  of a message with the according explicit semantic of meta data  m m  as   \n    \\sum_{n=0}^N \\sum_{m=0}^M value_{n,m} := \n    \\begin{cases}\n        member_{n,val} \\circ meta_{m,val}       & \\text{if } member_{n,key} = meta_{m,key}\\\\\n        \\emptyset                                                               & \\text{otherwise}\n    \\end{cases}  \n    \\sum_{n=0}^N \\sum_{m=0}^M value_{n,m} := \n    \\begin{cases}\n        member_{n,val} \\circ meta_{m,val}       & \\text{if } member_{n,key} = meta_{m,key}\\\\\n        \\emptyset                                                               & \\text{otherwise}\n    \\end{cases}   with  member member  being the set of  N N  members of a concrete message without any meta data and  meta meta  being the set of meta\ndata for a class of messages with explicit semantic of  M M  known types of observations. Each element of both sets\nconsists of  key key  and  value value . If  key_{n} key_{n}  of a member element is equal to  key_{m} key_{m}  of a meta data element, value_{n} value_{n}  of member element is combined with  value_{m} value_{m}  of the meta data element using the binary operator  \\circ \\circ \nin order to add explicit semantics to the original observation. The functionality of the binary operator  \\circ \\circ  is defined as   \n    f(member, meta) = \n    \\begin{cases}\n        data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{string\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{number\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{bool\\right\\} \\\\\n            object(member, meta)                                                & \\quad  \\text{if } member \\in \\left\\{object\\right\\} \\\\\n            \\sum_{n=0}^N member_{n} \\circ meta_{n}          & \\quad  \\text{if } member \\in \\left\\{array\\right\\} \\\\\n        \\emptyset                                                                       & \\quad  \\text{if } member \\in \\left\\{null\\right\\}\n    \\end{cases}  \n    f(member, meta) = \n    \\begin{cases}\n        data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{string\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{number\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{bool\\right\\} \\\\\n            object(member, meta)                                                & \\quad  \\text{if } member \\in \\left\\{object\\right\\} \\\\\n            \\sum_{n=0}^N member_{n} \\circ meta_{n}          & \\quad  \\text{if } member \\in \\left\\{array\\right\\} \\\\\n        \\emptyset                                                                       & \\quad  \\text{if } member \\in \\left\\{null\\right\\}\n    \\end{cases}   The function  data(member, meta) data(member, meta)  adds the data type property of  meta meta  to the new message and includes the literal\nvalue of  member member  in conjunction with the XSD data type associated with  meta meta . The function  object(member, meta) object(member, meta) \nadds the object property of  meta meta  to the new message and includes an URI reference to the object in  member member .\nAs the value of an array is nothing else but a list of  N N  values, we can simply execute the function  f(member, meta) f(member, meta) \non each element  n n  of that list. However, as we identify the meta data for each member by its key, there can only be\na single data type property or object property which is applied to all elements of an array. Different types of values\nwithin a single array are not supported.  As an example, we use the JSON messages of  LUBW stations  and push them to a \nmessage broker:  {\n    \"no2\":61,\n    \"ozn\":10,\n    \"luqx\":0,\n    \"latitude\":48.18169,\n    \"heigth\":510,\n    \"so2\":0,\n    \"station\":\"DEBY189\",\n    \"pm10\":0,\n    \"timestamp\":1516191751218,\n    \"longitude\":11.46445\n}   Listing: \nExample of a LUBW observation as JSON     Figure: \nExample for LUBW data stream   Another example are the JSON messages of  senseBox-based weather stations ,\nwhich are pushed to another topic of the message broker:  {\n    \"title\": \"Temperatur\",\n    \"unit\": \"\u00b0C\",\n    \"sensorType\": \"HDC1008\",\n    \"icon\": \"osem-thermometer\",\n    \"_id\": \"59ec966d49f6f80011c1239a\",\n    \"lastMeasurement\": {\n        \"value\": \"7.98\",\n        \"createdAt\": \"2018-01-18T13:02:14.330Z\"\n    }\n}   Listing: \nExample of a senseBox observation as JSON     Figure: \nExample for senseBox data stream   By employing the explicit meta data, we gain a new message stream of JSON-LD messages with explicit semantic of\neach observation:    Figure: \nExample for semantically enriched data stream",
            "title": "Semantic Message Enrichment"
        },
        {
            "location": "/architecture/Semantic/#semantic-data-validation",
            "text": "Once we have a data stream of observation messages from heterogeneous data sources but with explicit semantics,\nwe can also perform a semantic validation for all messages. For these validations, we again employ our annotation\nplatform to define data shapes (patterns) that should be fulfilled by all observation messages, regardless from which observation\nstation they are retrieved:    Figure: \nShape constraint annotation   Applying these shapes on the JSON-LD observation messages tells us immediately whether the observation is conform\nto the defined shape or not:    Figure: \nShape constraint validation of JSON-LD messages",
            "title": "Semantic Data Validation"
        },
        {
            "location": "/architecture/StreamPipes/",
            "text": "StreamPipes\n\u00b6\n\n\n\n\nResponsible person for this section\n\n\nMatthias Frank\n\n\n\n\n\n\nWith StreamPipes, it is possible to build distributed pipelines to process your data in real-time without development\neffort by providing an easy-to-use graphical user interface on top of existing stream processing frameworks.\nPipelines consist of data streams, real-time event processors and data sinks. StreamPipes provides a system that\ncan be used by domain experts with little to no technical understanding to explore and analyse their data.\nThe whole system is designed in a way that it is flexible and can be easily extended by adding data streams,\ndata processors or data sinks. A community edition of StreamPipes is available under the Apache Licence. \n\n\n\n\nFeatures Overview\n\n\nOfficial Documentation\n\n\n\n\nBigGIS extensions\n\u00b6\n\n\nWithin the BigGIS project, we added several GIS-specific extensions to the StreamPipes platform.\n\n\n\n\n\n\nClimate data\n\n\n\n\n\n\nSenseBoxAdapter\n\n\nprovides the semantic description for the externally created SenseBox data stream\nfor the integration into StreamPipes\n\n\n\n\n\n\nSenseboxMetadataEnricher\n\n\nenriches each message in the SenseBox measurements data stream with meta-data (location, OpenSenseMap-Id)\n\n\n\n\n\n\n\n\n\n\nRaster processing using geotrellis.\n\n\n\n\n\n\nRasterDataEndlessSource\n\n\ngenerates an endless Kafka stream of rasterdata messages to easily test and debug\nother rasterdata processing components\n\n\n\n\n\n\nRasterDataAdapter\n\n\nprovides the semantic description for the RasterDataEndlessSource",
            "title": "StreamPipes"
        },
        {
            "location": "/architecture/StreamPipes/#streampipes",
            "text": "Responsible person for this section  Matthias Frank    With StreamPipes, it is possible to build distributed pipelines to process your data in real-time without development\neffort by providing an easy-to-use graphical user interface on top of existing stream processing frameworks.\nPipelines consist of data streams, real-time event processors and data sinks. StreamPipes provides a system that\ncan be used by domain experts with little to no technical understanding to explore and analyse their data.\nThe whole system is designed in a way that it is flexible and can be easily extended by adding data streams,\ndata processors or data sinks. A community edition of StreamPipes is available under the Apache Licence.    Features Overview  Official Documentation",
            "title": "StreamPipes"
        },
        {
            "location": "/architecture/StreamPipes/#biggis-extensions",
            "text": "Within the BigGIS project, we added several GIS-specific extensions to the StreamPipes platform.    Climate data    SenseBoxAdapter  provides the semantic description for the externally created SenseBox data stream\nfor the integration into StreamPipes    SenseboxMetadataEnricher  enriches each message in the SenseBox measurements data stream with meta-data (location, OpenSenseMap-Id)      Raster processing using geotrellis.    RasterDataEndlessSource  generates an endless Kafka stream of rasterdata messages to easily test and debug\nother rasterdata processing components    RasterDataAdapter  provides the semantic description for the RasterDataEndlessSource",
            "title": "BigGIS extensions"
        },
        {
            "location": "/data-sources/",
            "text": "Datasets in BigGIS\n\u00b6\n\n\nThis section describes publicly available datasets that we used thoughout the project.",
            "title": "Datasets in BigGIS"
        },
        {
            "location": "/data-sources/#datasets-in-biggis",
            "text": "This section describes publicly available datasets that we used thoughout the project.",
            "title": "Datasets in BigGIS"
        },
        {
            "location": "/data-sources/atmosphere/dwd/",
            "text": "DWD (Meteorological Data)\n\u00b6\n\n\n\n\nTodo\n\n\nManuel Stein (UKON)",
            "title": "DWD (Meteorological Data)"
        },
        {
            "location": "/data-sources/atmosphere/dwd/#dwd-meteorological-data",
            "text": "Todo  Manuel Stein (UKON)",
            "title": "DWD (Meteorological Data)"
        },
        {
            "location": "/data-sources/atmosphere/lubw/",
            "text": "LUBW Stations\n\u00b6\n\n\nAir Measurements\n\u00b6\n\n\nThe LUBW mangages 50-80 aktive measurement stations across Baden-W\u00fcrttemberg, which provide hourly information on three\nmain air pollutants (see \nFigure 1\n):\n\n\n\n\nNitrogen oxides\n\n\nOzone \n\n\nParticular matter (Feinstaub)\n\n\n\n\nThe data is public and can be downloaded from the official web-page:\n\nhttps://www.lubw.baden-wuerttemberg.de/luft/messwerte-immissionswerte#karte\n\n\n\n\nFigure 1: Webpage LUBW with distribution of air measurement stations.\n\n\n\n\n\n\nLUBW REST API\n\u00b6\n\n\nAir measurment data collected by the LUBW (Nirtogen oxides, Ozone, Particular Matter) is also provided by a REST-Service\nfor the BigGIS project. The service is not public and delivers the current measurement value of each measurement\nstation and each pollutant in json format. The service also allows to aquire data on air pollutant from Bavaria,\nThuringia, Saxony-Anhalt and Schleswig-holstein.\n\n\nData on air pollutants across different states of germany can be acquired by the Federal Environmental Agency:\n\nhttps://www.umweltbundesamt.de/daten/luftbelastung/aktuelle-luftdaten#/start?s=q64FAA==&_k=ep8c63\n\n\nMeteorological Station at FZI\n\u00b6\n\n\nIn January 2017 a meteorological station was mounted at the roof of FZI for measuring air pressure, temperature, wind\nspeed, precipitation, humidity and global radiation  (see \nFigure 2\n). The measurements serve for calibrating and validating the sense\nboxes. The measurement device will be dismounted in April 2018.\n\n\n\n\nFigure 2: Meteorological Station at FZI",
            "title": "LUBW Stations"
        },
        {
            "location": "/data-sources/atmosphere/lubw/#lubw-stations",
            "text": "",
            "title": "LUBW Stations"
        },
        {
            "location": "/data-sources/atmosphere/lubw/#air-measurements",
            "text": "The LUBW mangages 50-80 aktive measurement stations across Baden-W\u00fcrttemberg, which provide hourly information on three\nmain air pollutants (see  Figure 1 ):   Nitrogen oxides  Ozone   Particular matter (Feinstaub)   The data is public and can be downloaded from the official web-page: https://www.lubw.baden-wuerttemberg.de/luft/messwerte-immissionswerte#karte   Figure 1: Webpage LUBW with distribution of air measurement stations.",
            "title": "Air Measurements"
        },
        {
            "location": "/data-sources/atmosphere/lubw/#lubw-rest-api",
            "text": "Air measurment data collected by the LUBW (Nirtogen oxides, Ozone, Particular Matter) is also provided by a REST-Service\nfor the BigGIS project. The service is not public and delivers the current measurement value of each measurement\nstation and each pollutant in json format. The service also allows to aquire data on air pollutant from Bavaria,\nThuringia, Saxony-Anhalt and Schleswig-holstein.  Data on air pollutants across different states of germany can be acquired by the Federal Environmental Agency: https://www.umweltbundesamt.de/daten/luftbelastung/aktuelle-luftdaten#/start?s=q64FAA==&_k=ep8c63",
            "title": "LUBW REST API"
        },
        {
            "location": "/data-sources/atmosphere/lubw/#meteorological-station-at-fzi",
            "text": "In January 2017 a meteorological station was mounted at the roof of FZI for measuring air pressure, temperature, wind\nspeed, precipitation, humidity and global radiation  (see  Figure 2 ). The measurements serve for calibrating and validating the sense\nboxes. The measurement device will be dismounted in April 2018.   Figure 2: Meteorological Station at FZI",
            "title": "Meteorological Station at FZI"
        },
        {
            "location": "/data-sources/atmosphere/sensebox-fzi/",
            "text": "SenseBox-based Weather Stations (FZI)\n\u00b6\n\n\n\n\nTodo\n\n\nJulian\n\n\n\n\n\n\n\n\nsupport two different modes of data transmission: WLAN and LoRa\n\n\n\n\nLoRa is preferred for longer range and easier deployment (no site-specific configuration)\n\n\n\n\n\n\n\n\nLoRa gateways\n\n\n\n\nTwo LoRa gateways should be deployed, one at FZI, the other at LUBW-KA (TODO:clarify)\n\n\nGateways are a \nKickstarter project\n,\n    and have not yet been delivered as of 2017-12-15. \nLast status\n\n    on Kickstarter is that the gateways are ready to ship (2017-11-06)\n\n\nJulian handles Kickstarter and deployment\n\n\n\n\n\n\n\n\nData handling\n\n\n\n\na common Kafka queue for events from both transports\n\n\none Kafka message per transmitted event, containing all measurements (temperature, humidity, air pressure, internal temperature, light, UV radiation)\n\n\ntransport specific adapters for \nWLAN\n\n    and \nLoRa\n\n\nshould be handled in a stream-processing way (pipeline modeled using StreamPipes)\n\n\nData source and metadata enrichment exist as \nStreamPipes components\n\n\n\n\n\n\nshould be stored within BigGIS database i.e. Accumulo/Exasolution (sensor,lat,lon,ts)\n\n\na \nFlink job\n\n    that persists the events from the Kafka queue into MySQL or PostgreSQL exists,\n    could be extended to support other JDBC databases\n\n\n\n\n\n\nshould be sent to \nhttps://opensensemap.org/\n (TODO:Jochen)\n\n\nOutlier filtering node (kafka \u2192 flink \u2192 kafka)\n\n\n\n\n\n\n\n\nDeployment\n\n\n\n\n34 LoRa sensor units should be deployed, Julian handles locations, external organizations etc.\n\n\n\n\n\n\n\n\nWeb-based mobile-friendly app\n\u00b6\n\n\n\n\nQR code contains stations id and URL that leads to public web\n\n\nthe web page contains info about the station and the project\n\n\nadmin can click and change station information (or register a new station)\n\n\nlat/lon is taken from the phone (HTML5 geolocation api)\n\n\nadmin can add additional parameters (placement details)",
            "title": "SenseBox-based Weather Stations (FZI)"
        },
        {
            "location": "/data-sources/atmosphere/sensebox-fzi/#sensebox-based-weather-stations-fzi",
            "text": "Todo  Julian     support two different modes of data transmission: WLAN and LoRa   LoRa is preferred for longer range and easier deployment (no site-specific configuration)     LoRa gateways   Two LoRa gateways should be deployed, one at FZI, the other at LUBW-KA (TODO:clarify)  Gateways are a  Kickstarter project ,\n    and have not yet been delivered as of 2017-12-15.  Last status \n    on Kickstarter is that the gateways are ready to ship (2017-11-06)  Julian handles Kickstarter and deployment     Data handling   a common Kafka queue for events from both transports  one Kafka message per transmitted event, containing all measurements (temperature, humidity, air pressure, internal temperature, light, UV radiation)  transport specific adapters for  WLAN \n    and  LoRa  should be handled in a stream-processing way (pipeline modeled using StreamPipes)  Data source and metadata enrichment exist as  StreamPipes components    should be stored within BigGIS database i.e. Accumulo/Exasolution (sensor,lat,lon,ts)  a  Flink job \n    that persists the events from the Kafka queue into MySQL or PostgreSQL exists,\n    could be extended to support other JDBC databases    should be sent to  https://opensensemap.org/  (TODO:Jochen)  Outlier filtering node (kafka \u2192 flink \u2192 kafka)     Deployment   34 LoRa sensor units should be deployed, Julian handles locations, external organizations etc.",
            "title": "SenseBox-based Weather Stations (FZI)"
        },
        {
            "location": "/data-sources/atmosphere/sensebox-fzi/#web-based-mobile-friendly-app",
            "text": "QR code contains stations id and URL that leads to public web  the web page contains info about the station and the project  admin can click and change station information (or register a new station)  lat/lon is taken from the phone (HTML5 geolocation api)  admin can add additional parameters (placement details)",
            "title": "Web-based mobile-friendly app"
        },
        {
            "location": "/data-sources/atmosphere/wunderground/",
            "text": "Wunderground Dataset\n\u00b6\n\n\nTo create models that support the visual analysis of the urban heat island effect, it is crucial to select suitable data\nsets that can be analyzed and transformed into useful visualizations. Data always must be preprocessed to clean and\ntransform the data to improve the quality to avoid misinterpretations of the consecutive visualizations and to enable\nusers to draw valid and helpful conclusions. Therefore, data processing functions as central part of any analysis system\nof underlying complex data. During this process, we follow the chronological order of the well known KDD pipeline.\n\n\nData Description\n\u00b6\n\n\nWe use temperature data from the private Weather Underground Station Network (WUSN) that has a better spatial\ndistribution of stations than station networks from public authorities like the German Weather Service (DWD). The WUSN\nis a crowd-sourcing platform that provides meteorological data from its participating users to gather high-resolution\ndata. Unfortunately, the WUSN dataset often contains invalid values that must be handled by cleaning, regression or\ndeletion with KNIME. To create a feature vector that contains multiple different data that are part of the complex\nvariable composition we found the meteorological variables that are provided by the GWS (DWD) to be useful. The DWD data\nunderlies certain quality standards and is more reliable than the data from WUSN, which also provides meteorological\nrecords. Weather cannot be simply be in uenced by human compared to surface characteristics.\n\n\nWeather Underground Station Network\n\u00b6\n\n\nIn Germany, the WUSN consists of 25298 stations (see Figure 1 and Figure 2) (1832 stations within city borders) that\nprovide multivariate data with a high temporal resolution of up to 2-3 measurements per hour (see Table 1 and Figure 3\nfor more details). In the preprocessing step, we replaced missing and invalid (out of possible range) values for every\nstation by using the average value of the previous and next recording to keep valuable information. We filtered all\nother meteorological attributes that are provided since temperature is sufficient to decide whether the location of a\nstation is warmer than the surrounding. We decide to average the available temperature data hourly which results in a\ndataset containing about 13,6 million data rows for the year of 2016. Unfortunately, some stations have values missing\nfor the range of multiple months, which doesn't affect the visualizations but are important when exploring the\nvisualization and interpreting the results.\n\n\n\n\nFigure 1: Distribution of weather stations in and around Germany.\n\n\n\n\n\n\n\n\nFigure 2: Distribution of weather stations in and around Germany displaed as voronoi diagram.\n\n\n\n\n\n\n\n\nFigure 3: Each weather station is represented by a single row.\n\n\nIt can clearly be seen that the total amount of weather stations is strongly increasing from the beginning of 2015.\n\n\n\n\n\n\n\nTable 1: Description of the data provided from the wunderground network stations.\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nUHI Index\n\n\nServes as indicator whether this station is a hotspot (local UHI) at this location and point in time\n\n\n\n\n\n\nStation ID\n\n\nUnique WU Station Identifier\n\n\n\n\n\n\nLocation\n\n\nGeographic position of the station consisting of longitude and latitude\n\n\n\n\n\n\nTime\n\n\nTimestamp of the recording with minute-by-minute precision\n\n\n\n\n\n\nTemperature\n\n\nProvided in Celsius values with an accuracy of one decimal place\n\n\n\n\n\n\nSurrounding Temp.\n\n\nBilinear interpolated temperature of WU neighbor stations using the distance to the central station as weighting factor\n\n\n\n\n\n\nNeighbor Stations\n\n\nList containing Station IDs up to 10 nearby stations\n\n\n\n\n\n\n\n\n\n\nFor the identification of local urban heat islands (hotspots), we considered the temporal and spatial attributes of the\nWU stations. First, for every station that lies inside of the city borders, the closest (up to 10 stations) neighbor\nstations where identified. Then, a temperature value for the respective surrounding of each city WU station was\ncalculated using a bilinear strategy considering the distance as weighting factor for the temperature value.",
            "title": "Wunderground Dataset"
        },
        {
            "location": "/data-sources/atmosphere/wunderground/#wunderground-dataset",
            "text": "To create models that support the visual analysis of the urban heat island effect, it is crucial to select suitable data\nsets that can be analyzed and transformed into useful visualizations. Data always must be preprocessed to clean and\ntransform the data to improve the quality to avoid misinterpretations of the consecutive visualizations and to enable\nusers to draw valid and helpful conclusions. Therefore, data processing functions as central part of any analysis system\nof underlying complex data. During this process, we follow the chronological order of the well known KDD pipeline.",
            "title": "Wunderground Dataset"
        },
        {
            "location": "/data-sources/atmosphere/wunderground/#data-description",
            "text": "We use temperature data from the private Weather Underground Station Network (WUSN) that has a better spatial\ndistribution of stations than station networks from public authorities like the German Weather Service (DWD). The WUSN\nis a crowd-sourcing platform that provides meteorological data from its participating users to gather high-resolution\ndata. Unfortunately, the WUSN dataset often contains invalid values that must be handled by cleaning, regression or\ndeletion with KNIME. To create a feature vector that contains multiple different data that are part of the complex\nvariable composition we found the meteorological variables that are provided by the GWS (DWD) to be useful. The DWD data\nunderlies certain quality standards and is more reliable than the data from WUSN, which also provides meteorological\nrecords. Weather cannot be simply be in uenced by human compared to surface characteristics.",
            "title": "Data Description"
        },
        {
            "location": "/data-sources/atmosphere/wunderground/#weather-underground-station-network",
            "text": "In Germany, the WUSN consists of 25298 stations (see Figure 1 and Figure 2) (1832 stations within city borders) that\nprovide multivariate data with a high temporal resolution of up to 2-3 measurements per hour (see Table 1 and Figure 3\nfor more details). In the preprocessing step, we replaced missing and invalid (out of possible range) values for every\nstation by using the average value of the previous and next recording to keep valuable information. We filtered all\nother meteorological attributes that are provided since temperature is sufficient to decide whether the location of a\nstation is warmer than the surrounding. We decide to average the available temperature data hourly which results in a\ndataset containing about 13,6 million data rows for the year of 2016. Unfortunately, some stations have values missing\nfor the range of multiple months, which doesn't affect the visualizations but are important when exploring the\nvisualization and interpreting the results.   Figure 1: Distribution of weather stations in and around Germany.     Figure 2: Distribution of weather stations in and around Germany displaed as voronoi diagram.     Figure 3: Each weather station is represented by a single row.  It can clearly be seen that the total amount of weather stations is strongly increasing from the beginning of 2015.    Table 1: Description of the data provided from the wunderground network stations.     Variable  Description      UHI Index  Serves as indicator whether this station is a hotspot (local UHI) at this location and point in time    Station ID  Unique WU Station Identifier    Location  Geographic position of the station consisting of longitude and latitude    Time  Timestamp of the recording with minute-by-minute precision    Temperature  Provided in Celsius values with an accuracy of one decimal place    Surrounding Temp.  Bilinear interpolated temperature of WU neighbor stations using the distance to the central station as weighting factor    Neighbor Stations  List containing Station IDs up to 10 nearby stations      For the identification of local urban heat islands (hotspots), we considered the temporal and spatial attributes of the\nWU stations. First, for every station that lies inside of the city borders, the closest (up to 10 stations) neighbor\nstations where identified. Then, a temperature value for the respective surrounding of each city WU station was\ncalculated using a bilinear strategy considering the distance as weighting factor for the temperature value.",
            "title": "Weather Underground Station Network"
        },
        {
            "location": "/data-sources/biosphere/vitimeteo/",
            "text": "Drosophila Suzukii Observations (Vitimeteo)\n\u00b6\n\n\n\n\nTodo\n\n\nJohannes Kutterer (Disy)\n\n\n\n\nsee also \ninvasive-species",
            "title": "Drosophila Suzukii Observations (Vitimeteo)"
        },
        {
            "location": "/data-sources/biosphere/vitimeteo/#drosophila-suzukii-observations-vitimeteo",
            "text": "Todo  Johannes Kutterer (Disy)   see also  invasive-species",
            "title": "Drosophila Suzukii Observations (Vitimeteo)"
        },
        {
            "location": "/data-sources/ground/landcover/",
            "text": "Digital ortho-images (Arial photographs)\n\u00b6\n\n\nDigital ortho-images were aquired across different flight campaigns between 2013 and 2015 building a mosaic that covers\nBaden-W\u00fcrttemberg entirely (see \nFigure 1 A\n). The georeferenced images come with 4 spectral bands (blue,\ngreen, red, infra-red) and a spatial resolution of 20cm (see \nFigure 1 B\n). The non-public dataset was provided by the State Institute for Environment of Baden-W\u00fcrttemberg\n(LUBW) and State Agency for Spatial Information and Rural Development Baden-W\u00fcrttemberg (LGL).\n\n\n\n\nFigure 1:\n A) Overview on image aquisition years B) False color composite (R= infra-red, G=green, B=blue) of digital ortho-images with 20cm spatial resolution.",
            "title": "Digital ortho-images (Arial photographs)"
        },
        {
            "location": "/data-sources/ground/landcover/#digital-ortho-images-arial-photographs",
            "text": "Digital ortho-images were aquired across different flight campaigns between 2013 and 2015 building a mosaic that covers\nBaden-W\u00fcrttemberg entirely (see  Figure 1 A ). The georeferenced images come with 4 spectral bands (blue,\ngreen, red, infra-red) and a spatial resolution of 20cm (see  Figure 1 B ). The non-public dataset was provided by the State Institute for Environment of Baden-W\u00fcrttemberg\n(LUBW) and State Agency for Spatial Information and Rural Development Baden-W\u00fcrttemberg (LGL).   Figure 1:  A) Overview on image aquisition years B) False color composite (R= infra-red, G=green, B=blue) of digital ortho-images with 20cm spatial resolution.",
            "title": "Digital ortho-images (Arial photographs)"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/",
            "text": "Lightweight Camera Systems\n\u00b6\n\n\nRGB\n\u00b6\n\n\n\n\nTodo\n\n\nBodo\n\n\n\n\nIR\n\u00b6\n\n\n\n\nTodo\n\n\nBodo\n\n\n\n\nHyperspectral\n\u00b6\n\n\n\n\nTodo\n\n\nAlex",
            "title": "Lightweight Camera Systems"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/#lightweight-camera-systems",
            "text": "",
            "title": "Lightweight Camera Systems"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/#rgb",
            "text": "Todo  Bodo",
            "title": "RGB"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/#ir",
            "text": "Todo  Bodo",
            "title": "IR"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/#hyperspectral",
            "text": "Todo  Alex",
            "title": "Hyperspectral"
        },
        {
            "location": "/data-sources/ground/nasa-elevation/",
            "text": "NASA Elevation Profile\n\u00b6\n\n\n\n\nTodo\n\n\nManuel Stein (UKON)",
            "title": "NASA Elevation Profile"
        },
        {
            "location": "/data-sources/ground/nasa-elevation/#nasa-elevation-profile",
            "text": "Todo  Manuel Stein (UKON)",
            "title": "NASA Elevation Profile"
        },
        {
            "location": "/data-sources/ground/nasa-modis/",
            "text": "MODIS Dataset\n\u00b6\n\n\nOverview\n\u00b6\n\n\nThe Land Surface Temperature (LST) and Emissivity daily data are retrieved at 1km pixels by the generalized split-window\nalgorithm and at 6km grids by the day/night algorithm. In the split-window algorithm, emissivities in bands 31 and 32\nare estimated from land cover types, atmospheric column water vapor and lower boundary air surface temperature are\nseparated into tractable sub-ranges for optimal retrieval. In the day/night algorithm, daytime and nighttime LSTs and\nsurface emissivities are retrieved from pairs of day and night MODIS observations in seven TIR bands. The\nproduct\n1\n is comprised of LSTs, quality assessment, observation time, view angles, and emissivities.\n\n\nDetails\n\u00b6\n\n\nLST Data is available with 1km grid resolution, all others in 5km resolution, typical size of a single capture is around\n1350x2000km. MODIS provides 1-2 captures per day and location (this is due to the polar Orbit of the two MODIS\nSatelites). The data is provided as either Near-Real-Time (NRT) data with a delay time of 30min-3h or as Tile Data wich\nis available within 2-3 Days. The Tile Data can be accessed via a coordinate-based Bounding-Box request, NRT Data is\navailable via FTP. All data is stored in the HDF4 file format which contains a two dimensional array of data points\ncontaining the value, coordinates (lat, lon), record time and Quality index (used for masking cloud covered areas)\n\n\nLST Data is available since 2002 in full capability and since 1999 with reduced frequency (single satellite operation\nuntil 2002). For the BigGIS Project, we from the University of Konstanz, made this data available to all project\npartners. For this we created a crawler to collect the published Tile Data, preprocessed the published HDF4 data and\nmade the MODIS data available in our database since 2016.\n\n\n\n\nFigure 1: Example distribution of available MODIS LST data in Germany.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://modis.gsfc.nasa.gov/data/dataprod/mod11.php\n \"Last Accessed on 26.01.2018.\"\u00a0\n\u21a9",
            "title": "MODIS Dataset"
        },
        {
            "location": "/data-sources/ground/nasa-modis/#modis-dataset",
            "text": "",
            "title": "MODIS Dataset"
        },
        {
            "location": "/data-sources/ground/nasa-modis/#overview",
            "text": "The Land Surface Temperature (LST) and Emissivity daily data are retrieved at 1km pixels by the generalized split-window\nalgorithm and at 6km grids by the day/night algorithm. In the split-window algorithm, emissivities in bands 31 and 32\nare estimated from land cover types, atmospheric column water vapor and lower boundary air surface temperature are\nseparated into tractable sub-ranges for optimal retrieval. In the day/night algorithm, daytime and nighttime LSTs and\nsurface emissivities are retrieved from pairs of day and night MODIS observations in seven TIR bands. The\nproduct 1  is comprised of LSTs, quality assessment, observation time, view angles, and emissivities.",
            "title": "Overview"
        },
        {
            "location": "/data-sources/ground/nasa-modis/#details",
            "text": "LST Data is available with 1km grid resolution, all others in 5km resolution, typical size of a single capture is around\n1350x2000km. MODIS provides 1-2 captures per day and location (this is due to the polar Orbit of the two MODIS\nSatelites). The data is provided as either Near-Real-Time (NRT) data with a delay time of 30min-3h or as Tile Data wich\nis available within 2-3 Days. The Tile Data can be accessed via a coordinate-based Bounding-Box request, NRT Data is\navailable via FTP. All data is stored in the HDF4 file format which contains a two dimensional array of data points\ncontaining the value, coordinates (lat, lon), record time and Quality index (used for masking cloud covered areas)  LST Data is available since 2002 in full capability and since 1999 with reduced frequency (single satellite operation\nuntil 2002). For the BigGIS Project, we from the University of Konstanz, made this data available to all project\npartners. For this we created a crawler to collect the published Tile Data, preprocessed the published HDF4 data and\nmade the MODIS data available in our database since 2016.   Figure 1: Example distribution of available MODIS LST data in Germany.        https://modis.gsfc.nasa.gov/data/dataprod/mod11.php  \"Last Accessed on 26.01.2018.\"\u00a0 \u21a9",
            "title": "Details"
        },
        {
            "location": "/data-sources/ground/sentinel2/",
            "text": "Sentinel 2\n\u00b6\n\n\n\n\nTodo\n\n\nAdrian (EFTAS)\n: optimize text\n\n\n\n\nUsed in \nLanduse classification\n as input to classify landcover for the landuse analysis\n\n\nDescription\n\u00b6\n\n\nSentinel 2 dataset is multispectral satellite raster data with 13 bands in the visible (RGB), near infrared (NIR) and\nshort wave infrared (SWIR) spectrum. It can be updated approximately every 5 days. The Sentinel-2A and 2B satellites are\noperated by the European Space Agency (ESA) as part of the Copernicus Programme.\n\n\nsee: \nhttps://en.wikipedia.org/wiki/Sentinel-2\n\n\nSource of the data is the ESA Sentinel Portal\n\u00b6\n\n\n\n\nESA: - \nhttps://sentinel.esa.int/web/sentinel/missions/sentinel-2/data-products\n\n\nSentinel Tiles - \nSentinel-2 tiling grid kml\n\n\nAccess to Sentinel Data - \nhttps://sentinel.esa.int/web/sentinel/sentinel-data-access\n\n\nCopernicus Open Access Hub - \nhttps://scihub.copernicus.eu/dhus/#/home\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative Sources\n\u00b6\n\n\n\n\nAWS: - \nhttps://aws.amazon.com/de/public-datasets/sentinel-2/\n\n\n\n\nExample download (Tile 32UMU - 2016-05-05 - bands B2,B3,B4,B8 - Blue Green Red NIR )\n\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B02.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B03.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B04.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B08.jp2\n\n\n\nSentinel 2 bands and resolutions\n\u00b6\n\n\n\n\n\n\n\n\nBand\n\n\nBandname\n\n\nCentral Wavelength (\u00b5m)\n\n\nResolution (m)\n\n\nBandwidth (nm)\n\n\n\n\n\n\n\n\n\n\n1\n\n\nCoastal aerosol\n\n\n0.443\n\n\n60\n\n\n20\n\n\n\n\n\n\n2\n\n\nBlue\n\n\n0.490\n\n\n10\n\n\n65\n\n\n\n\n\n\n3\n\n\nGreen\n\n\n0.560\n\n\n10\n\n\n35\n\n\n\n\n\n\n4\n\n\nRed\n\n\n0.665\n\n\n10\n\n\n30\n\n\n\n\n\n\n5\n\n\nVegetation Red Edge\n\n\n0.705\n\n\n20\n\n\n15\n\n\n\n\n\n\n6\n\n\nVegetation Red Edge\n\n\n0.740\n\n\n20\n\n\n15\n\n\n\n\n\n\n7\n\n\nVegetation Red Edge\n\n\n0.783\n\n\n20\n\n\n20\n\n\n\n\n\n\n8\n\n\nNIR\n\n\n0.842\n\n\n10\n\n\n115\n\n\n\n\n\n\n8A\n\n\nNarrow NIR\n\n\n0.865\n\n\n20\n\n\n20\n\n\n\n\n\n\n9\n\n\nWater vapour\n\n\n0.945\n\n\n60\n\n\n20\n\n\n\n\n\n\n10\n\n\nSWIR \u2013 Cirrus\n\n\n1.375\n\n\n60\n\n\n20\n\n\n\n\n\n\n11\n\n\nSWIR\n\n\n1.610\n\n\n20\n\n\n90\n\n\n\n\n\n\n12\n\n\nSWIR\n\n\n2.190\n\n\n20\n\n\n180\n\n\n\n\n\n\n\n\n10m bands\n\u00b6\n\n\n\n\n\n\n\n\nBand\n\n\nBandname\n\n\nCentral Wavelength (\u00b5m)\n\n\nResolution (m)\n\n\nBandwidth (nm)\n\n\n\n\n\n\n\n\n\n\n2\n\n\nBlue\n\n\n0.490\n\n\n10\n\n\n65\n\n\n\n\n\n\n3\n\n\nGreen\n\n\n0.560\n\n\n10\n\n\n35\n\n\n\n\n\n\n4\n\n\nRed\n\n\n0.665\n\n\n10\n\n\n30\n\n\n\n\n\n\n8\n\n\nNIR\n\n\n0.842\n\n\n10\n\n\n115\n\n\n\n\n\n\n\n\n20m bands\n\u00b6\n\n\n\n\n\n\n\n\nBand\n\n\nBandname\n\n\nCentral Wavelength (\u00b5m)\n\n\nResolution (m)\n\n\nBandwidth (nm)\n\n\n\n\n\n\n\n\n\n\n5\n\n\nVegetation Red Edge\n\n\n0.705\n\n\n20\n\n\n15\n\n\n\n\n\n\n6\n\n\nVegetation Red Edge\n\n\n0.740\n\n\n20\n\n\n15\n\n\n\n\n\n\n7\n\n\nVegetation Red Edge\n\n\n0.783\n\n\n20\n\n\n20\n\n\n\n\n\n\n8A\n\n\nNarrow NIR\n\n\n0.865\n\n\n20\n\n\n20\n\n\n\n\n\n\n11\n\n\nSWIR\n\n\n1.610\n\n\n20\n\n\n90\n\n\n\n\n\n\n12\n\n\nSWIR\n\n\n2.190\n\n\n20\n\n\n180\n\n\n\n\n\n\n\n\n60m bands\n\u00b6\n\n\n\n\n\n\n\n\nBand\n\n\nBandname\n\n\nCentral Wavelength (\u00b5m)\n\n\nResolution (m)\n\n\nBandwidth (nm)\n\n\n\n\n\n\n\n\n\n\n1\n\n\nCoastal aerosol\n\n\n0.443\n\n\n60\n\n\n20\n\n\n\n\n\n\n9\n\n\nWater vapour\n\n\n0.945\n\n\n60\n\n\n20\n\n\n\n\n\n\n10\n\n\nSWIR \u2013 Cirrus\n\n\n1.375\n\n\n60\n\n\n20",
            "title": "Sentinel 2"
        },
        {
            "location": "/data-sources/ground/sentinel2/#sentinel-2",
            "text": "Todo  Adrian (EFTAS)\n: optimize text   Used in  Landuse classification  as input to classify landcover for the landuse analysis",
            "title": "Sentinel 2"
        },
        {
            "location": "/data-sources/ground/sentinel2/#description",
            "text": "Sentinel 2 dataset is multispectral satellite raster data with 13 bands in the visible (RGB), near infrared (NIR) and\nshort wave infrared (SWIR) spectrum. It can be updated approximately every 5 days. The Sentinel-2A and 2B satellites are\noperated by the European Space Agency (ESA) as part of the Copernicus Programme.  see:  https://en.wikipedia.org/wiki/Sentinel-2",
            "title": "Description"
        },
        {
            "location": "/data-sources/ground/sentinel2/#source-of-the-data-is-the-esa-sentinel-portal",
            "text": "ESA: -  https://sentinel.esa.int/web/sentinel/missions/sentinel-2/data-products  Sentinel Tiles -  Sentinel-2 tiling grid kml  Access to Sentinel Data -  https://sentinel.esa.int/web/sentinel/sentinel-data-access  Copernicus Open Access Hub -  https://scihub.copernicus.eu/dhus/#/home",
            "title": "Source of the data is the ESA Sentinel Portal"
        },
        {
            "location": "/data-sources/ground/sentinel2/#alternative-sources",
            "text": "AWS: -  https://aws.amazon.com/de/public-datasets/sentinel-2/   Example download (Tile 32UMU - 2016-05-05 - bands B2,B3,B4,B8 - Blue Green Red NIR ) wget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B02.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B03.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B04.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B08.jp2",
            "title": "Alternative Sources"
        },
        {
            "location": "/data-sources/ground/sentinel2/#sentinel-2-bands-and-resolutions",
            "text": "Band  Bandname  Central Wavelength (\u00b5m)  Resolution (m)  Bandwidth (nm)      1  Coastal aerosol  0.443  60  20    2  Blue  0.490  10  65    3  Green  0.560  10  35    4  Red  0.665  10  30    5  Vegetation Red Edge  0.705  20  15    6  Vegetation Red Edge  0.740  20  15    7  Vegetation Red Edge  0.783  20  20    8  NIR  0.842  10  115    8A  Narrow NIR  0.865  20  20    9  Water vapour  0.945  60  20    10  SWIR \u2013 Cirrus  1.375  60  20    11  SWIR  1.610  20  90    12  SWIR  2.190  20  180",
            "title": "Sentinel 2 bands and resolutions"
        },
        {
            "location": "/data-sources/ground/sentinel2/#10m-bands",
            "text": "Band  Bandname  Central Wavelength (\u00b5m)  Resolution (m)  Bandwidth (nm)      2  Blue  0.490  10  65    3  Green  0.560  10  35    4  Red  0.665  10  30    8  NIR  0.842  10  115",
            "title": "10m bands"
        },
        {
            "location": "/data-sources/ground/sentinel2/#20m-bands",
            "text": "Band  Bandname  Central Wavelength (\u00b5m)  Resolution (m)  Bandwidth (nm)      5  Vegetation Red Edge  0.705  20  15    6  Vegetation Red Edge  0.740  20  15    7  Vegetation Red Edge  0.783  20  20    8A  Narrow NIR  0.865  20  20    11  SWIR  1.610  20  90    12  SWIR  2.190  20  180",
            "title": "20m bands"
        },
        {
            "location": "/data-sources/ground/sentinel2/#60m-bands",
            "text": "Band  Bandname  Central Wavelength (\u00b5m)  Resolution (m)  Bandwidth (nm)      1  Coastal aerosol  0.443  60  20    9  Water vapour  0.945  60  20    10  SWIR \u2013 Cirrus  1.375  60  20",
            "title": "60m bands"
        },
        {
            "location": "/data-sources/ground/uba-air/",
            "text": "Umweltbundesamt Air Pollution\n\u00b6\n\n\n\n\nTodo\n\n\nManuel Stein (UKON)",
            "title": "Umweltbundesamt Air Pollution"
        },
        {
            "location": "/data-sources/ground/uba-air/#umweltbundesamt-air-pollution",
            "text": "Todo  Manuel Stein (UKON)",
            "title": "Umweltbundesamt Air Pollution"
        },
        {
            "location": "/data-sources/socio-economic/envirocar/",
            "text": "Envirocar Dataset\n\u00b6\n\n\n\n\nTodo\n\n\nManuel Stein (UKON)\n\n\n\n\nThe used dataset contains approximately around 1.7 million data points. ...",
            "title": "Envirocar Dataset"
        },
        {
            "location": "/data-sources/socio-economic/envirocar/#envirocar-dataset",
            "text": "Todo  Manuel Stein (UKON)   The used dataset contains approximately around 1.7 million data points. ...",
            "title": "Envirocar Dataset"
        },
        {
            "location": "/data-sources/socio-economic/landuse/",
            "text": "Land use data\n\u00b6\n\n\n\n\nTodo\n\n\nHannes (LUBW)\n\n\n\n\nATKIS\n\u00b6\n\n\n\n\nATKIS land use data (multiple options possible)\n\n\nshapefiles in a directory\n\n\ndata in Accumulo/Exasolution\n\n\n\n\n\n\n\n\nALKIS\n\u00b6",
            "title": "Land use data"
        },
        {
            "location": "/data-sources/socio-economic/landuse/#land-use-data",
            "text": "Todo  Hannes (LUBW)",
            "title": "Land use data"
        },
        {
            "location": "/data-sources/socio-economic/landuse/#atkis",
            "text": "ATKIS land use data (multiple options possible)  shapefiles in a directory  data in Accumulo/Exasolution",
            "title": "ATKIS"
        },
        {
            "location": "/data-sources/socio-economic/landuse/#alkis",
            "text": "",
            "title": "ALKIS"
        },
        {
            "location": "/data-sources/socio-economic/newyork-taxi/",
            "text": "New York Taxi Drives\n\u00b6\n\n\n\n\nTodo\n\n\nMatthias Frank (FZI)\n\n\n\n\n\n\nNew York taxi drives\n\n\n2GB/month \u2192 for years 2009-2015 potentially ~160GB of storage space\n\n\nmultiple options possible - TODO:Matthias\n\n\nbunch of CSV files in a directories organized per year\n\n\npoints stored in Accumulo\n\n\npoints stored in Exasolution",
            "title": "New York Taxi Drives"
        },
        {
            "location": "/data-sources/socio-economic/newyork-taxi/#new-york-taxi-drives",
            "text": "Todo  Matthias Frank (FZI)    New York taxi drives  2GB/month \u2192 for years 2009-2015 potentially ~160GB of storage space  multiple options possible - TODO:Matthias  bunch of CSV files in a directories organized per year  points stored in Accumulo  points stored in Exasolution",
            "title": "New York Taxi Drives"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/",
            "text": "Traffic Incidents\n\u00b6\n\n\nFoundations\n\u00b6\n\n\nThe traffic incidents contained for various analytical use cases within the BigGIS projects are gathered via the Bing API (Documentation is online: \nhttps://msdn.microsoft.com/en-us/library/hh441726.aspx\n). The JSON API provides data in real time about traffic incidents (Accidents, traffic congestion, construction etc.). It supports requests using either bounding boxes or route location codes. For our data gathering, we used bounding boxes worldwide and gathered the data in near real time in our database. The API returns coordinates (lat, lon), type of incident, severity (slow-down, all lanes closed etc.), estimated delay as well as start time and estimated end time. All gathered worldwide Bing Traffic Data is available in the BigGIS Database hosted at the University of Konstanz. The table values values are documented below. \n\n\nStatistics\n\u00b6\n\n\nThe database contains 3,7 million entries, each containing a unique ID, location information, start and end time as well as describing meta information such as a textual description, the type of incident and the severity. \n\n\nIncident Types\n\u00b6\n\n\nInteger value in Range 1-11 with the following mappings:\n\n\n1: Accident\n\n\n2: Congestion\n\n\n3: DisabledVehicle\n\n\n4: MassTransit\n\n\n5: Miscellaneous\n\n\n6: OtherNews\n\n\n7: PlannedEvent\n\n\n8: RoadHazard\n\n\n9: Construction\n\n\n10: Alert\n\n\n11: Weather\n\n\nSeverity\n\u00b6\n\n\nInteger value in Range 1-4:\n\n\n1: LowImpact\n\n\n2: Minor\n\n\n3: Moderate\n\n\n4: Serious",
            "title": "Traffic Incidents"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#traffic-incidents",
            "text": "",
            "title": "Traffic Incidents"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#foundations",
            "text": "The traffic incidents contained for various analytical use cases within the BigGIS projects are gathered via the Bing API (Documentation is online:  https://msdn.microsoft.com/en-us/library/hh441726.aspx ). The JSON API provides data in real time about traffic incidents (Accidents, traffic congestion, construction etc.). It supports requests using either bounding boxes or route location codes. For our data gathering, we used bounding boxes worldwide and gathered the data in near real time in our database. The API returns coordinates (lat, lon), type of incident, severity (slow-down, all lanes closed etc.), estimated delay as well as start time and estimated end time. All gathered worldwide Bing Traffic Data is available in the BigGIS Database hosted at the University of Konstanz. The table values values are documented below.",
            "title": "Foundations"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#statistics",
            "text": "The database contains 3,7 million entries, each containing a unique ID, location information, start and end time as well as describing meta information such as a textual description, the type of incident and the severity.",
            "title": "Statistics"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#incident-types",
            "text": "Integer value in Range 1-11 with the following mappings:  1: Accident  2: Congestion  3: DisabledVehicle  4: MassTransit  5: Miscellaneous  6: OtherNews  7: PlannedEvent  8: RoadHazard  9: Construction  10: Alert  11: Weather",
            "title": "Incident Types"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#severity",
            "text": "Integer value in Range 1-4:  1: LowImpact  2: Minor  3: Moderate  4: Serious",
            "title": "Severity"
        },
        {
            "location": "/demos/",
            "text": "About demos\n\u00b6\n\n\nIn this section you can find demos of various BigGIS components.\n\n\nEach demo consists of:\n\n\n\n\nA short introduction of the problem being solved.\n\n\nReferences to related methods, algorithms and models (from section \nMethods\n)\n\n\nExplanation of related code samples.\n\n\nStep-by-step tutorial, how the demo can be executed.\n\n\n\n\nDemos to be included later\n\u00b6\n\n\n\n\nExasol: demo of using a spatial index\n\n\nExasol: demo of using a virtual schema\n\n\nExasol: demo of using R-connector",
            "title": "About demos"
        },
        {
            "location": "/demos/#about-demos",
            "text": "In this section you can find demos of various BigGIS components.  Each demo consists of:   A short introduction of the problem being solved.  References to related methods, algorithms and models (from section  Methods )  Explanation of related code samples.  Step-by-step tutorial, how the demo can be executed.",
            "title": "About demos"
        },
        {
            "location": "/demos/#demos-to-be-included-later",
            "text": "Exasol: demo of using a spatial index  Exasol: demo of using a virtual schema  Exasol: demo of using R-connector",
            "title": "Demos to be included later"
        },
        {
            "location": "/demos/firefighting/",
            "text": "Firefighting\n\u00b6\n\n\nMotivation\n\u00b6\n\n\nIn the last decades the methods of firefighting have become a lot more effective. Still, the access to a sufficient\namount of water is often limited. Fighting a major fire requires several thousand liters of water per minute and so the\nwater tanks of firetrucks are empty in a matter of minutes. Here the real challenge begins: \nfinding hydrants as\nquickly as possible\n, which can supply a sufficient amount of water and at the same time are close by, while taking\ninto account:\n\n\n\n\nthe height difference, \n\n\nreducing water pressure. \n\n\n\n\nWe developed a tool that can help solving this complex situation. OpenStreetMap data is used as basis data for our\ncalculations.  The aim of our tool is to help the fire officer in charge, to make the best decision to provide the\nnecessary amount of water as quickly as possible.\n\n\nFeatures\n\u00b6\n\n\nAfter marking an incident scene on a map, the most suitable hydrant is shown on the map. In addition, the position for\nthe line of hoses is marked as well as the position of potentially needed mobile water pumps, providing the right\npressure in hilly surroundings.\n\n\nOn a side bar on the right you can interactively add the amount of water needed. The user can also choose between three\ndifferent varieties to get water, taking into account other possibilities for hydrants, hoses and pumps. After having\nchosen a solution, the corresponding elevation profile will be shown on the side bar, indicating the incident scene, the\nhydrant and the position and distance of the individual pumps.\n\n\nIt is possible to mark certain items on the map as blocked, e.g. hydrants, roads etc. leading to a new calculation of\nthe best solution without using the blocked item. This is very often necessary as hydrants might be blocked by a parking\ncar or roads are not accessible because of construction sites.",
            "title": "Firefighting"
        },
        {
            "location": "/demos/firefighting/#firefighting",
            "text": "",
            "title": "Firefighting"
        },
        {
            "location": "/demos/firefighting/#motivation",
            "text": "In the last decades the methods of firefighting have become a lot more effective. Still, the access to a sufficient\namount of water is often limited. Fighting a major fire requires several thousand liters of water per minute and so the\nwater tanks of firetrucks are empty in a matter of minutes. Here the real challenge begins:  finding hydrants as\nquickly as possible , which can supply a sufficient amount of water and at the same time are close by, while taking\ninto account:   the height difference,   reducing water pressure.    We developed a tool that can help solving this complex situation. OpenStreetMap data is used as basis data for our\ncalculations.  The aim of our tool is to help the fire officer in charge, to make the best decision to provide the\nnecessary amount of water as quickly as possible.",
            "title": "Motivation"
        },
        {
            "location": "/demos/firefighting/#features",
            "text": "After marking an incident scene on a map, the most suitable hydrant is shown on the map. In addition, the position for\nthe line of hoses is marked as well as the position of potentially needed mobile water pumps, providing the right\npressure in hilly surroundings.  On a side bar on the right you can interactively add the amount of water needed. The user can also choose between three\ndifferent varieties to get water, taking into account other possibilities for hydrants, hoses and pumps. After having\nchosen a solution, the corresponding elevation profile will be shown on the side bar, indicating the incident scene, the\nhydrant and the position and distance of the individual pumps.  It is possible to mark certain items on the map as blocked, e.g. hydrants, roads etc. leading to a new calculation of\nthe best solution without using the blocked item. This is very often necessary as hydrants might be blocked by a parking\ncar or roads are not accessible because of construction sites.",
            "title": "Features"
        },
        {
            "location": "/demos/gas-detect/",
            "text": "Responsible person for this section\n\n\nAlexander Groeschel\n\n\n\n\nGas Cloud Detection\n\u00b6\n\n\n\n\nChlorophyll-Erkennung im Befliegungsexperiment\n\n\nUnsichtbare Schadgaswolke (IR-Bereich)\n\n\nHei\u00dfe oder kalte Gase/Gegenst\u00e4nde leicht zu erkennen\n\n\nGase mit Umgebungstemperatur im IR-Bild nicht zu erkennen.\n\n\nSubtraktion von georeferenzierten und zus\u00e4tzlich positionskorrigierten Bildern aus Zeitreihen macht kleine \n  Abweichungen in Bildern sichtbar \u2192 Wolken unsichtbarer Gase im Bild sichtbar\n\n\n\n\nBefliegungskampagne am 15./16.07.17\n\u00b6\n\n\n\n\nAnalyse von Gaswolken aus der Luft\n\n\nTools:\n\n\nIR/RGB-Kamera\n\n\nHyperspektralkamera\n\n\n\n\n\n\n\n\n\n\nEtablierung einer Funkstrecke\n\n\n\u00dcbertragung von Flugplan-Daten/Bildergebnissen\n\n\n\n\n\n\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nDisaster Management\n\n\nSmart City",
            "title": "Gas Cloud Detection"
        },
        {
            "location": "/demos/gas-detect/#gas-cloud-detection",
            "text": "Chlorophyll-Erkennung im Befliegungsexperiment  Unsichtbare Schadgaswolke (IR-Bereich)  Hei\u00dfe oder kalte Gase/Gegenst\u00e4nde leicht zu erkennen  Gase mit Umgebungstemperatur im IR-Bild nicht zu erkennen.  Subtraktion von georeferenzierten und zus\u00e4tzlich positionskorrigierten Bildern aus Zeitreihen macht kleine \n  Abweichungen in Bildern sichtbar \u2192 Wolken unsichtbarer Gase im Bild sichtbar",
            "title": "Gas Cloud Detection"
        },
        {
            "location": "/demos/gas-detect/#befliegungskampagne-am-15160717",
            "text": "Analyse von Gaswolken aus der Luft  Tools:  IR/RGB-Kamera  Hyperspektralkamera      Etablierung einer Funkstrecke  \u00dcbertragung von Flugplan-Daten/Bildergebnissen",
            "title": "Befliegungskampagne am 15./16.07.17"
        },
        {
            "location": "/demos/gas-detect/#related-scenarios",
            "text": "Disaster Management  Smart City",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/gas-predict/",
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nGas Cloud Prediction\n\u00b6\n\n\n\n\nModeling of gas clouds and their dispersion over time.\n\n\nJulian's bachelor student (maybe text from his bc-thesis?)",
            "title": "Gas Cloud Prediction"
        },
        {
            "location": "/demos/gas-predict/#gas-cloud-prediction",
            "text": "Modeling of gas clouds and their dispersion over time.  Julian's bachelor student (maybe text from his bc-thesis?)",
            "title": "Gas Cloud Prediction"
        },
        {
            "location": "/demos/heatstress/",
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nHeatstress Routing App\n\u00b6\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/path-optimizer\n\n\n\n\nRelated Scenarios: \nSmart City\n\n\nThe back-end exposes a simple REST-API, that can be used for routing or to find\nthe optimal point in time. The API can be accessed on \nhttp://localhost:8080/heatstressrouting/api/v1\n.\nJSON is supported as the only output format.\n\n\nThe following sections describe the API in detail.\n\n\nServer information\n\u00b6\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/info\n\n\nReturns some information about the running service, e.g. the supported area and time range\n\n\nParameters\n\u00b6\n\n\nThe \n/info\n site takes no parameters.\n\n\nReturns\n\u00b6\n\n\nReturns some information about the running service (see sample response below):\n\n\n\n\nbbox\n: the bounding box of the area supported by the service as an array of \n[min_lat, min_lng, max_lat, max_lng]\n.\n\n\ntime_range\n: the time range supported by the service, given as time stamps of the form \n2014-08-23T00:00\n.\n\n\nplace_types\n: a list of place types supported by the optimal time api\n\n\n\n\nExample\n\u00b6\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/info\n\n\n\nSample Response:\n\n{\n\n  \n\"service\"\n:\n \n\"heat stress routing\"\n,\n\n  \n\"version\"\n:\n \n\"0.0.1-SNAPSHOT\"\n,\n\n  \n\"build_time\"\n:\n \n\"2016-09-27T07:50:42Z\"\n,\n\n  \n\"bbox\"\n:\n \n[\n\n    \n48.99\n,\n\n    \n8.385\n,\n\n    \n49.025\n,\n\n    \n8.435\n\n  \n],\n\n  \n\"time_range\"\n:\n \n{\n\n    \n\"from\"\n:\n \n\"2014-08-23T00:00\"\n,\n\n    \n\"to\"\n:\n \n\"2016-02-23T23:00\"\n\n  \n},\n\n  \n\"place_types\"\n:\n \n[\n\n    \n\"bakery\"\n,\n\n    \n\"taxi\"\n,\n\n    \n\"post_office\"\n,\n\n    \n\"ice_cream\"\n,\n\n    \n\"dentist\"\n,\n\n    \n\"post_box\"\n,\n\n    \n\"supermarket\"\n,\n\n    \n\"toilets\"\n,\n\n    \n\"bank\"\n,\n\n    \n\"cafe\"\n,\n\n    \n\"police\"\n,\n\n    \n\"doctors\"\n,\n\n    \n\"pharmacy\"\n,\n\n    \n\"drinking_water\"\n,\n\n    \n\"atm\"\n,\n\n    \n\"clinic\"\n,\n\n    \n\"kiosk\"\n,\n\n    \n\"hospital\"\n,\n\n    \n\"chemist\"\n,\n\n    \n\"fast_food\"\n\n  \n]\n\n\n}\n\n\n\n\nRouting\n\u00b6\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/routing\n\n\nComputes the optimal route (regarding heat stress) between a start and a destination at a given time.\n\n\nParameters\n\u00b6\n\n\nThe \n/routing\n api supports the following parameter (some are optional):\n\n\n\n\nstart\n: the start point as pair of a latitude value and longitude value (in that order)\n  seperated by a comma, e.g. \nstart=49.0118083,8.4251357\n.\n\n\ndestination\n: the destination as pair of a latitude value and longitude value (in that order)\n  separated by a comma, e.g. \ndestination=49.0126868,8.4065707\n. \n\n\ntime\n: the date and time the optimal route should be searched for; a time stamp of the form\n  \nYYYY-MM-DDTHH:MM:SS\n, e.g. \ntime=2015-08-31T10:00:00\n. The value must be in the time range\n  returned by \n/info\n (see \nabove\n).\n\n\nweighting\n (optional): the weightings to be used; a comma seperated list of the supported\n  weightings (\nshortest\n, \nheatindex\n and \ntemperature\n), e.g. \nweighting=shortest,heatindex,temperature\n;\n  the default is \nweighting=shortest,heatindex\n; the results for the \nshortest\n weighting are always\n  returned, even if the value is omited in the weighings list.\n\n\n\n\nReturns\n\u00b6\n\n\nThe path and some other information for each of the weightings:\n\n\n\n\nstatus\n: the status of the request; \nOK\n is everthing is okay, \nBAD_REQUEST\n if a invalid request was send or \nINTERNAL_SERVER_ERROR\n if an internal error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\nresults\n: the results for each weighting:\n\n\nweighting\n: the weighting used for that result (see parameter \nweighting\n above).\n\n\nstart\n: the coordinates of the start point as array of \n[lat, lng]\n.\n\n\ndestination\n: the coordinates of the destination as array of \n[lat, lng]\n.\n\n\ndistance\n: the length of the route in meter.\n\n\nduration\n: the walking time in milli seconds.\n\n\nroute_weights\n: the route weights of the selected weightings for the route.\n\n\npath\n: the geometry of the path found; an array of points, were each point is an array of [lat, lng]`.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/routing?start=49.0118083,8.4251357&destination=49.0126868,8.4065707&time=2015-08-31T10:00:00&weighting=shortest,heatindex,temperature\n\n\n\nSample Response:\n\n{\n\n  \n\"status\"\n:\n \n\"OK\"\n,\n\n  \n\"status_code\"\n:\n \n200\n,\n\n  \n\"results\"\n:\n \n{\n\n    \n\"shortest\"\n:\n \n{\n\n      \n\"weighting\"\n:\n \n\"shortest\"\n,\n\n      \n\"start\"\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \n\"destination\"\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \n\"distance\"\n:\n \n1698.2989202985977\n,\n\n      \n\"duration\"\n:\n \n1222740\n,\n\n      \n\"route_weights\"\n:\n \n{\n\n        \n\"temperature\"\n:\n \n50903.955833052285\n,\n\n        \n\"heatindex\"\n:\n \n50892.20496302502\n,\n\n        \n\"shortest\"\n:\n \n1698.2989202985977\n\n      \n},\n\n      \n\"path\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \n\"heatindex\"\n:\n \n{\n\n      \n\"weighting\"\n:\n \n\"heatindex\"\n,\n\n      \n\"start\"\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \n\"destination\"\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \n\"distance\"\n:\n \n1901.8839202985973\n,\n\n      \n\"duration\"\n:\n \n1369323\n,\n\n      \n\"route_weights\"\n:\n \n{\n\n        \n\"temperature\"\n:\n \n51868.74807902536\n,\n\n        \n\"heatindex\"\n:\n \n51098.277424417196\n,\n\n        \n\"shortest\"\n:\n \n1901.8839202985978\n\n      \n},\n\n      \n\"path\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \n\"temperature\"\n:\n \n{\n\n      \n\"weighting\"\n:\n \n\"temperature\"\n,\n\n      \n\"start\"\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \n\"destination\"\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \n\"distance\"\n:\n \n1901.8839202985973\n,\n\n      \n\"duration\"\n:\n \n1369323\n,\n\n      \n\"route_weights\"\n:\n \n{\n\n        \n\"temperature\"\n:\n \n51868.74807902536\n,\n\n        \n\"heatindex\"\n:\n \n51098.277424417196\n,\n\n        \n\"shortest\"\n:\n \n1901.8839202985978\n\n      \n},\n\n      \n\"path\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\nOptimal time\n\u00b6\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime\n\n\nPerformce a nearby search for a given start point and computes for every place that fulfills\na specified criterion an optimal point in time, i.e. the time with the minimal heat stress.\n\n\nParameters\n\u00b6\n\n\nThe \n/optimaltime\n api supports the following parameter (some are optional):\n\n\n\n\nstart\n: the start point as pair of a latitude value and longitude value (in that order) seperated by a comma, e.g. \nstart=49.0118083,8.4251357\n. \n\n\ntime\n: the date and time the optimal route should be searched for; a time stamp of the form \nYYYY-MM-DDTHH:MM:SS\n, e.g. \ntime=2015-08-31T10:00:00\n. The value must be in the time range returned by \n/info\n (see \nabove\n).\n\n\nplace_type\n: the place type to search for; a comma seperated list of supported place types, e.g. \nplace_type=supermarket,chemist\n; a complete list of supported place list can be queried using the \ninfo\n api (see \nabove\n). Currently the following place tyes are supported: \nbakery\n, \ntaxi\n, \npost_office\n, \nice_cream\n, \ndentist\n, \npost_box\n, \nsupermarket\n, \ntoilets\n, \nbank\n, \ncafe\n, \npolice\n, \ndoctors\n, \npharmacy\n, \ndrinking_water\n, \natm\n, \nclinic\n, \nkiosk\n, \nhospital\n, \nchemist\n, \nfast_food\n. The place types are mapped to the corresponding \nshop\n respectively \namenity\n tags.\n\n\nmax_results\n (optional): the maximum number of results to consider for the nearby search (an positive integer), e.g. \nmax_results=10\n; the default value is 5.\n\n\nmax_distance\n (optional): the maximum direct distance (as the crow flies) between the start point and the place in meter, e.g. \nmax_distance=500.0\n; the default value is 1000.0 meter.\n\n\ntime_buffer\n (optional): the minimum time needed at the place (in minutes), i.e. the optimal time is chossen so that the place is opened for a least \ntime_buffer\n when the user arrives, e.g. \ntime_buffer=30\n; the default value is 15 miniutes.\n\n\nearliest_time\n (optional): the earliest desired time, either a time stamp, e.g. \nearliest_time=2015-08-31T09:00\n or the string \nnull\n (case is ignored); the default value is \nnull\n. If both \nearliest_time\n and \nlatest_time\n are specified, \nearliest_time\n must be before \nlatest_time\n.\n\n\nlatest_time\n (optional): the latest desired time, either a time stamp, e.g. \nlatest_time=2015-08-31T17:00\n or the string \nnull\n (case is ignored); the default value is \nnull\n. If both \nearliest_time\n and \nlatest_time\n are specified, \nearliest_time\n must be before \nlatest_time\n; \nlatest_time\n must be after \ntime\n.\n\n\n\n\nReturns\n\u00b6\n\n\nThe optimal point in time for each place found in the specified radius ranked by the optimal-value:\n\n\n\n\nstatus\n: the status of the request; \nOK\n if everthing is okay, \nNO_REULTS\n if not results were found, \nBAD_REQUEST\n if a invalid request was send to the server or \nINTERNAL_SERVER_ERROR\n if an internal server error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\n\n\nresults\n: the result for each place found during the nearby search:\n\n\n\n\nrank\n: the rank of the place according to the optimal value (were 1 is the best rank).\n\n\nname\n: the name of the place.\n\n\nosm_id\n: the \nOpenStreetMap Node ID\n of the place.\n\n\nlocation\n: the coordinates of the places as an array of \n[lat, lng]\n.\n\n\nopening_hours\n: the opening hours of the place; the format specification can be found \nhere\n.\n\n\noptimal_time\n: the optimal point in time found for that place, e.g. \n2015-08-31T20:00\n\n\noptimal_value\n: the optimal value found for the place; the value considering the heat stress acording to steadman's heatindex \n(Steadmean, 1979)\n as well as the distance between the start and the place.\n\n\ndistance\n: the length of the optimal path (see \nRouting\n above) from the start to the place in meter.\n\n\nduration\n: the time needed to walk from the start to the place (in milli seconds).\n\n\npath_optimal\n: the geometry of the optimal path (see \nRouting\n above).\n\n\ndistance_shortest\n: the length of the shortest path between the start and the place (in meter).\n\n\nduration_shortest\n: the time needed to walk the shortest path between the start and the place (in milli seconds).\n\n\npath_optimal\n: the geometry of the shortest path (see \nRouting\n above).\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime?start=49.0118083,8.4251357&time=2015-08-31T10:00:00&place_type=supermarket&max_distance=1000&max_results=5&time_buffer=15&earliest_time=2015-08-31T09:00:00&latest_time=2015-08-31T20:00:00\n\n\n\nSample Response:\n\n{\n\n  \n\"status\"\n:\n \n\"OK\"\n,\n\n  \n\"status_code\"\n:\n \n200\n,\n\n  \n\"results\"\n:\n \n[\n\n    \n{\n\n      \n\"rank\"\n:\n \n1\n,\n\n      \n\"name\"\n:\n \n\"Rewe City\"\n,\n\n      \n\"osm_id\"\n:\n \n897615202\n,\n\n      \n\"location\"\n:\n \n[\n\n        \n49.0096613\n,\n\n        \n8.4237272\n\n      \n],\n\n      \n\"opening_hours\"\n:\n \n\"Mo-Sa 07:00-22:00; Su,PH off\"\n,\n\n      \n\"optimal_time\"\n:\n \n\"2015-08-31T20:00\"\n,\n\n      \n\"optimal_value\"\n:\n \n12515.36230258099\n,\n\n      \n\"distance\"\n:\n \n539.1839746027457\n,\n\n      \n\"duration\"\n:\n \n388207\n,\n\n      \n\"path_optimal\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00954480942009\n,\n\n          \n8.423681942364334\n\n        \n]\n\n      \n],\n\n      \n\"distance_shortest\"\n:\n \n468.99728441805115\n,\n\n      \n\"duration_shortest\"\n:\n \n337669\n,\n\n      \n\"path_shortest\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00954480942009\n,\n\n          \n8.423681942364334\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \n{\n\n      \n\"rank\"\n:\n \n2\n,\n\n      \n\"name\"\n:\n \n\"Oststadt Super-Bio-Markt\"\n,\n\n      \n\"osm_id\"\n:\n \n931682116\n,\n\n      \n\"location\"\n:\n \n[\n\n        \n49.009433\n,\n\n        \n8.4234214\n\n      \n],\n\n      \n\"opening_hours\"\n:\n \n\"Mo-Fr 09:00-13:00,14:00-18:30; Sa 09:00-13:00\"\n,\n\n      \n\"optimal_time\"\n:\n \n\"2015-08-31T18:09:19.199\"\n,\n\n      \n\"optimal_value\"\n:\n \n14318.962937267655\n,\n\n      \n\"distance\"\n:\n \n473.346750294328\n,\n\n      \n\"duration\"\n:\n \n340801\n,\n\n      \n\"path_optimal\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00944708743373\n,\n\n          \n8.4235711322383\n\n        \n]\n\n      \n],\n\n      \n\"distance_shortest\"\n:\n \n473.346750294328\n,\n\n      \n\"duration_shortest\"\n:\n \n340801\n,\n\n      \n\"path_shortest\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00944708743373\n,\n\n          \n8.4235711322383\n\n        \n]\n\n      \n]\n\n    \n}\n\n  \n]\n\n\n}\n\n\n\n\nError messages\n\u00b6\n\n\nIf an error occurs, e.g. because a bade request were send to the server or an internal server errors occurs, the server is sending a JSON response with the following content:\n\n\n\n\nstatus\n: the status of the request; \nOK\n if everthing is okay, \nNO_REULTS\n if not results were found, \nBAD_REQUEST\n if a invalid request was send to the server or \nINTERNAL_SERVER_ERROR\n if an internal server error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\nmessages\n: an array of human readable error messages.\n\n\n\n\nExample\n\u00b6\n\n\nExample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime?start=Schloss,%20Karlsruhe&time=2015-08-31T10:00:00&place_type=supermarket\n\n\n\nExample Response:\n\n{\n\n  \n\"status\"\n:\n \n\"BAD_REQUEST\"\n,\n\n  \n\"status_code\"\n:\n \n400\n,\n\n  \n\"messages\"\n:\n \n[\n\n    \n\"start (Schloss, Karlsruhe) could not be parsed: failed to parse coordinate; 'start' must be a pair of latitude and longitude seperated by a comma (','), e.g. '49.0118083,8.4251357')\"\n\n  \n]\n\n\n}\n\n\n\n\nReferences\n\u00b6\n\n\n\n\nReference\n\n\nSteadman, R. G. \nThe Assessment of Sultriness. Part I: A Temperature-Humidity Index Based on Human Physiology and Clothing.\n\nScience Journal of Applied Meteorology, 1979, 18, 861-873, DOI: 10.1175/1520-0450(1979)018<0861:TAOSPI>2.0.CO;2",
            "title": "Heatstress Routing App"
        },
        {
            "location": "/demos/heatstress/#heatstress-routing-app",
            "text": "Note  Related repository is  https://github.com/biggis-project/path-optimizer   Related Scenarios:  Smart City  The back-end exposes a simple REST-API, that can be used for routing or to find\nthe optimal point in time. The API can be accessed on  http://localhost:8080/heatstressrouting/api/v1 .\nJSON is supported as the only output format.  The following sections describe the API in detail.",
            "title": "Heatstress Routing App"
        },
        {
            "location": "/demos/heatstress/#server-information",
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/info  Returns some information about the running service, e.g. the supported area and time range",
            "title": "Server information"
        },
        {
            "location": "/demos/heatstress/#parameters",
            "text": "The  /info  site takes no parameters.",
            "title": "Parameters"
        },
        {
            "location": "/demos/heatstress/#returns",
            "text": "Returns some information about the running service (see sample response below):   bbox : the bounding box of the area supported by the service as an array of  [min_lat, min_lng, max_lat, max_lng] .  time_range : the time range supported by the service, given as time stamps of the form  2014-08-23T00:00 .  place_types : a list of place types supported by the optimal time api",
            "title": "Returns"
        },
        {
            "location": "/demos/heatstress/#example",
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/info  Sample Response: { \n   \"service\" :   \"heat stress routing\" , \n   \"version\" :   \"0.0.1-SNAPSHOT\" , \n   \"build_time\" :   \"2016-09-27T07:50:42Z\" , \n   \"bbox\" :   [ \n     48.99 , \n     8.385 , \n     49.025 , \n     8.435 \n   ], \n   \"time_range\" :   { \n     \"from\" :   \"2014-08-23T00:00\" , \n     \"to\" :   \"2016-02-23T23:00\" \n   }, \n   \"place_types\" :   [ \n     \"bakery\" , \n     \"taxi\" , \n     \"post_office\" , \n     \"ice_cream\" , \n     \"dentist\" , \n     \"post_box\" , \n     \"supermarket\" , \n     \"toilets\" , \n     \"bank\" , \n     \"cafe\" , \n     \"police\" , \n     \"doctors\" , \n     \"pharmacy\" , \n     \"drinking_water\" , \n     \"atm\" , \n     \"clinic\" , \n     \"kiosk\" , \n     \"hospital\" , \n     \"chemist\" , \n     \"fast_food\" \n   ]  }",
            "title": "Example"
        },
        {
            "location": "/demos/heatstress/#routing",
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/routing  Computes the optimal route (regarding heat stress) between a start and a destination at a given time.",
            "title": "Routing"
        },
        {
            "location": "/demos/heatstress/#parameters_1",
            "text": "The  /routing  api supports the following parameter (some are optional):   start : the start point as pair of a latitude value and longitude value (in that order)\n  seperated by a comma, e.g.  start=49.0118083,8.4251357 .  destination : the destination as pair of a latitude value and longitude value (in that order)\n  separated by a comma, e.g.  destination=49.0126868,8.4065707 .   time : the date and time the optimal route should be searched for; a time stamp of the form\n   YYYY-MM-DDTHH:MM:SS , e.g.  time=2015-08-31T10:00:00 . The value must be in the time range\n  returned by  /info  (see  above ).  weighting  (optional): the weightings to be used; a comma seperated list of the supported\n  weightings ( shortest ,  heatindex  and  temperature ), e.g.  weighting=shortest,heatindex,temperature ;\n  the default is  weighting=shortest,heatindex ; the results for the  shortest  weighting are always\n  returned, even if the value is omited in the weighings list.",
            "title": "Parameters"
        },
        {
            "location": "/demos/heatstress/#returns_1",
            "text": "The path and some other information for each of the weightings:   status : the status of the request;  OK  is everthing is okay,  BAD_REQUEST  if a invalid request was send or  INTERNAL_SERVER_ERROR  if an internal error occoured.  status_code : the HTTP status code returned.  results : the results for each weighting:  weighting : the weighting used for that result (see parameter  weighting  above).  start : the coordinates of the start point as array of  [lat, lng] .  destination : the coordinates of the destination as array of  [lat, lng] .  distance : the length of the route in meter.  duration : the walking time in milli seconds.  route_weights : the route weights of the selected weightings for the route.  path : the geometry of the path found; an array of points, were each point is an array of [lat, lng]`.",
            "title": "Returns"
        },
        {
            "location": "/demos/heatstress/#example_1",
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/routing?start=49.0118083,8.4251357&destination=49.0126868,8.4065707&time=2015-08-31T10:00:00&weighting=shortest,heatindex,temperature  Sample Response: { \n   \"status\" :   \"OK\" , \n   \"status_code\" :   200 , \n   \"results\" :   { \n     \"shortest\" :   { \n       \"weighting\" :   \"shortest\" , \n       \"start\" :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       \"destination\" :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       \"distance\" :   1698.2989202985977 , \n       \"duration\" :   1222740 , \n       \"route_weights\" :   { \n         \"temperature\" :   50903.955833052285 , \n         \"heatindex\" :   50892.20496302502 , \n         \"shortest\" :   1698.2989202985977 \n       }, \n       \"path\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     }, \n     \"heatindex\" :   { \n       \"weighting\" :   \"heatindex\" , \n       \"start\" :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       \"destination\" :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       \"distance\" :   1901.8839202985973 , \n       \"duration\" :   1369323 , \n       \"route_weights\" :   { \n         \"temperature\" :   51868.74807902536 , \n         \"heatindex\" :   51098.277424417196 , \n         \"shortest\" :   1901.8839202985978 \n       }, \n       \"path\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     }, \n     \"temperature\" :   { \n       \"weighting\" :   \"temperature\" , \n       \"start\" :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       \"destination\" :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       \"distance\" :   1901.8839202985973 , \n       \"duration\" :   1369323 , \n       \"route_weights\" :   { \n         \"temperature\" :   51868.74807902536 , \n         \"heatindex\" :   51098.277424417196 , \n         \"shortest\" :   1901.8839202985978 \n       }, \n       \"path\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/demos/heatstress/#optimal-time",
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/optimaltime  Performce a nearby search for a given start point and computes for every place that fulfills\na specified criterion an optimal point in time, i.e. the time with the minimal heat stress.",
            "title": "Optimal time"
        },
        {
            "location": "/demos/heatstress/#parameters_2",
            "text": "The  /optimaltime  api supports the following parameter (some are optional):   start : the start point as pair of a latitude value and longitude value (in that order) seperated by a comma, e.g.  start=49.0118083,8.4251357 .   time : the date and time the optimal route should be searched for; a time stamp of the form  YYYY-MM-DDTHH:MM:SS , e.g.  time=2015-08-31T10:00:00 . The value must be in the time range returned by  /info  (see  above ).  place_type : the place type to search for; a comma seperated list of supported place types, e.g.  place_type=supermarket,chemist ; a complete list of supported place list can be queried using the  info  api (see  above ). Currently the following place tyes are supported:  bakery ,  taxi ,  post_office ,  ice_cream ,  dentist ,  post_box ,  supermarket ,  toilets ,  bank ,  cafe ,  police ,  doctors ,  pharmacy ,  drinking_water ,  atm ,  clinic ,  kiosk ,  hospital ,  chemist ,  fast_food . The place types are mapped to the corresponding  shop  respectively  amenity  tags.  max_results  (optional): the maximum number of results to consider for the nearby search (an positive integer), e.g.  max_results=10 ; the default value is 5.  max_distance  (optional): the maximum direct distance (as the crow flies) between the start point and the place in meter, e.g.  max_distance=500.0 ; the default value is 1000.0 meter.  time_buffer  (optional): the minimum time needed at the place (in minutes), i.e. the optimal time is chossen so that the place is opened for a least  time_buffer  when the user arrives, e.g.  time_buffer=30 ; the default value is 15 miniutes.  earliest_time  (optional): the earliest desired time, either a time stamp, e.g.  earliest_time=2015-08-31T09:00  or the string  null  (case is ignored); the default value is  null . If both  earliest_time  and  latest_time  are specified,  earliest_time  must be before  latest_time .  latest_time  (optional): the latest desired time, either a time stamp, e.g.  latest_time=2015-08-31T17:00  or the string  null  (case is ignored); the default value is  null . If both  earliest_time  and  latest_time  are specified,  earliest_time  must be before  latest_time ;  latest_time  must be after  time .",
            "title": "Parameters"
        },
        {
            "location": "/demos/heatstress/#returns_2",
            "text": "The optimal point in time for each place found in the specified radius ranked by the optimal-value:   status : the status of the request;  OK  if everthing is okay,  NO_REULTS  if not results were found,  BAD_REQUEST  if a invalid request was send to the server or  INTERNAL_SERVER_ERROR  if an internal server error occoured.  status_code : the HTTP status code returned.   results : the result for each place found during the nearby search:   rank : the rank of the place according to the optimal value (were 1 is the best rank).  name : the name of the place.  osm_id : the  OpenStreetMap Node ID  of the place.  location : the coordinates of the places as an array of  [lat, lng] .  opening_hours : the opening hours of the place; the format specification can be found  here .  optimal_time : the optimal point in time found for that place, e.g.  2015-08-31T20:00  optimal_value : the optimal value found for the place; the value considering the heat stress acording to steadman's heatindex  (Steadmean, 1979)  as well as the distance between the start and the place.  distance : the length of the optimal path (see  Routing  above) from the start to the place in meter.  duration : the time needed to walk from the start to the place (in milli seconds).  path_optimal : the geometry of the optimal path (see  Routing  above).  distance_shortest : the length of the shortest path between the start and the place (in meter).  duration_shortest : the time needed to walk the shortest path between the start and the place (in milli seconds).  path_optimal : the geometry of the shortest path (see  Routing  above).",
            "title": "Returns"
        },
        {
            "location": "/demos/heatstress/#example_2",
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/optimaltime?start=49.0118083,8.4251357&time=2015-08-31T10:00:00&place_type=supermarket&max_distance=1000&max_results=5&time_buffer=15&earliest_time=2015-08-31T09:00:00&latest_time=2015-08-31T20:00:00  Sample Response: { \n   \"status\" :   \"OK\" , \n   \"status_code\" :   200 , \n   \"results\" :   [ \n     { \n       \"rank\" :   1 , \n       \"name\" :   \"Rewe City\" , \n       \"osm_id\" :   897615202 , \n       \"location\" :   [ \n         49.0096613 , \n         8.4237272 \n       ], \n       \"opening_hours\" :   \"Mo-Sa 07:00-22:00; Su,PH off\" , \n       \"optimal_time\" :   \"2015-08-31T20:00\" , \n       \"optimal_value\" :   12515.36230258099 , \n       \"distance\" :   539.1839746027457 , \n       \"duration\" :   388207 , \n       \"path_optimal\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00954480942009 , \n           8.423681942364334 \n         ] \n       ], \n       \"distance_shortest\" :   468.99728441805115 , \n       \"duration_shortest\" :   337669 , \n       \"path_shortest\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00954480942009 , \n           8.423681942364334 \n         ] \n       ] \n     }, \n     { \n       \"rank\" :   2 , \n       \"name\" :   \"Oststadt Super-Bio-Markt\" , \n       \"osm_id\" :   931682116 , \n       \"location\" :   [ \n         49.009433 , \n         8.4234214 \n       ], \n       \"opening_hours\" :   \"Mo-Fr 09:00-13:00,14:00-18:30; Sa 09:00-13:00\" , \n       \"optimal_time\" :   \"2015-08-31T18:09:19.199\" , \n       \"optimal_value\" :   14318.962937267655 , \n       \"distance\" :   473.346750294328 , \n       \"duration\" :   340801 , \n       \"path_optimal\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00944708743373 , \n           8.4235711322383 \n         ] \n       ], \n       \"distance_shortest\" :   473.346750294328 , \n       \"duration_shortest\" :   340801 , \n       \"path_shortest\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00944708743373 , \n           8.4235711322383 \n         ] \n       ] \n     } \n   ]  }",
            "title": "Example"
        },
        {
            "location": "/demos/heatstress/#error-messages",
            "text": "If an error occurs, e.g. because a bade request were send to the server or an internal server errors occurs, the server is sending a JSON response with the following content:   status : the status of the request;  OK  if everthing is okay,  NO_REULTS  if not results were found,  BAD_REQUEST  if a invalid request was send to the server or  INTERNAL_SERVER_ERROR  if an internal server error occoured.  status_code : the HTTP status code returned.  messages : an array of human readable error messages.",
            "title": "Error messages"
        },
        {
            "location": "/demos/heatstress/#example_3",
            "text": "Example Request: http://localhost:8080/heatstressrouting/api/v1/optimaltime?start=Schloss,%20Karlsruhe&time=2015-08-31T10:00:00&place_type=supermarket  Example Response: { \n   \"status\" :   \"BAD_REQUEST\" , \n   \"status_code\" :   400 , \n   \"messages\" :   [ \n     \"start (Schloss, Karlsruhe) could not be parsed: failed to parse coordinate; 'start' must be a pair of latitude and longitude seperated by a comma (','), e.g. '49.0118083,8.4251357')\" \n   ]  }",
            "title": "Example"
        },
        {
            "location": "/demos/heatstress/#references",
            "text": "Reference  Steadman, R. G.  The Assessment of Sultriness. Part I: A Temperature-Humidity Index Based on Human Physiology and Clothing. \nScience Journal of Applied Meteorology, 1979, 18, 861-873, DOI: 10.1175/1520-0450(1979)018<0861:TAOSPI>2.0.CO;2",
            "title": "References"
        },
        {
            "location": "/demos/landuse/",
            "text": "Landuse classification\n\u00b6\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/biggis-landuse\n\n\n\n\nProblem definition\n\u00b6\n\n\n\n\nWe have the following datasets:\n\n\nAn existing Landuse vector dataset\n\n\nOrthorectified Aerial images (Digital Ortho Photos = DOP) \n\n\nSatellite images (SAT), e.g. \nSentinel 2\n 10m\n\n\n\n\n\n\nWe want to select / extract landcover classes from landuse classes.\n\n\nWe want to use Support Vector Machines (or other Machine Learning Classifiers) for classifying landcover classes.\n\n\nsee section \nSupport Vector Machines\n\n\n\n\n\n\nWe want to support Multiple classes using a One versus All (OvA) strategy or One vs. Rest.\n\n\nsee section \nOne vs. Rest\n\n\n\n\n\n\nWe want to parallelize the computation in our Spark cluster using \nGeotrellis\n for data loading and export.\n\n\n\n\n\n\nResponsible person for this section\n\n\nAdrian Klink (EFTAS)\n\n\n\n\n\n\nTodo\n\n\n\n\nDescribe the idea\n\n\nMaybe add some geotrellis examples\n\n\n\n\n\n\nClassification of Aerial Images according to Land Use Classes (using Land Cover Classes as intermediate)\n\u00b6\n\n\nTools\n\u00b6\n\n\n\n\nMachine Learning\n\n\nTraining: Multiclass SVM\n\n\nGeotrellis\n\n\n\n\nScala code snippets\n\u00b6\n\n\ntype\n \nSpatialMultibandRDD\n \n=\n \nRDD\n[(\nSpatialKey\n, \nMultibandTile\n)]\n \nwith\n \nMetadata\n[\nTileLayerMetadata\n[\nSpatialKey\n]]\n\n\n\n// reading from Hadoop Layer (HDFS)\n\n\nval\n \nrdd\n \n:\n \nSpatialMultibandRDD\n \n=\n \nbiggis\n.\nlanduse\n.\napi\n.\nreadRddFromLayer\n(\nLayerId\n(\nlayerName\n,\n \nzoom\n))\n\n\n\n// writing to Hadoop Layer (HDFS)\n\n\nbiggis\n.\nlanduse\n.\napi\n.\nwriteRddToLayer\n(\nrdd\n,\n \nLayerId\n(\nlayerName\n,\n \nzoom\n))\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\nClassification of Aerial Images May-Aug 2016\n\n\nLayerstacking: Aerial Images + Satellite images (IR, Resolution 2m)\n\n\nTraining of a Multiclass SVM with manually selected training data (classified image tiles)\n\n\n\n\nFurther Steps\n\u00b6\n\n\n\n\nAdding additional Layers, e.g.\n\n\nTerrain Height\n\n\nHomogeneity of Texture\n\n\nUsing Other Classififiers\n\n\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nEnvironment\n\n\nSmart City",
            "title": "Landuse classification"
        },
        {
            "location": "/demos/landuse/#landuse-classification",
            "text": "Note  Related repository is  https://github.com/biggis-project/biggis-landuse",
            "title": "Landuse classification"
        },
        {
            "location": "/demos/landuse/#problem-definition",
            "text": "We have the following datasets:  An existing Landuse vector dataset  Orthorectified Aerial images (Digital Ortho Photos = DOP)   Satellite images (SAT), e.g.  Sentinel 2  10m    We want to select / extract landcover classes from landuse classes.  We want to use Support Vector Machines (or other Machine Learning Classifiers) for classifying landcover classes.  see section  Support Vector Machines    We want to support Multiple classes using a One versus All (OvA) strategy or One vs. Rest.  see section  One vs. Rest    We want to parallelize the computation in our Spark cluster using  Geotrellis  for data loading and export.    Responsible person for this section  Adrian Klink (EFTAS)    Todo   Describe the idea  Maybe add some geotrellis examples",
            "title": "Problem definition"
        },
        {
            "location": "/demos/landuse/#classification-of-aerial-images-according-to-land-use-classes-using-land-cover-classes-as-intermediate",
            "text": "",
            "title": "Classification of Aerial Images according to Land Use Classes (using Land Cover Classes as intermediate)"
        },
        {
            "location": "/demos/landuse/#tools",
            "text": "Machine Learning  Training: Multiclass SVM  Geotrellis",
            "title": "Tools"
        },
        {
            "location": "/demos/landuse/#scala-code-snippets",
            "text": "type   SpatialMultibandRDD   =   RDD [( SpatialKey ,  MultibandTile )]   with   Metadata [ TileLayerMetadata [ SpatialKey ]]  // reading from Hadoop Layer (HDFS)  val   rdd   :   SpatialMultibandRDD   =   biggis . landuse . api . readRddFromLayer ( LayerId ( layerName ,   zoom ))  // writing to Hadoop Layer (HDFS)  biggis . landuse . api . writeRddToLayer ( rdd ,   LayerId ( layerName ,   zoom ))",
            "title": "Scala code snippets"
        },
        {
            "location": "/demos/landuse/#example",
            "text": "Classification of Aerial Images May-Aug 2016  Layerstacking: Aerial Images + Satellite images (IR, Resolution 2m)  Training of a Multiclass SVM with manually selected training data (classified image tiles)",
            "title": "Example"
        },
        {
            "location": "/demos/landuse/#further-steps",
            "text": "Adding additional Layers, e.g.  Terrain Height  Homogeneity of Texture  Using Other Classififiers",
            "title": "Further Steps"
        },
        {
            "location": "/demos/landuse/#related-scenarios",
            "text": "Environment  Smart City",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/optimize-drones/",
            "text": "Responsible person for this section\n\n\nKatharina Glock\n\n\n\n\nOptimal flight plan for drones\n\u00b6\n\n\n\n\nTodo\n\n\n\n\ndescribe the idea\n\n\nadd some images\n\n\nadd some links to related papers\n\n\nadd links to related github repos\n\n\nadd links to related scenarios",
            "title": "Optimal flight plan for drones"
        },
        {
            "location": "/demos/optimize-drones/#optimal-flight-plan-for-drones",
            "text": "Todo   describe the idea  add some images  add some links to related papers  add links to related github repos  add links to related scenarios",
            "title": "Optimal flight plan for drones"
        },
        {
            "location": "/demos/optimize-sensors/",
            "text": "Responsible person for this section\n\n\nKatharina Glock\n\n\n\n\nPlacement of sensors under uncertainty\n\u00b6\n\n\n\n\nTodo\n\n\n\n\ndescribe the idea\n\n\nadd some images\n\n\nadd some links to related papers\n\n\nadd links to related github repos\n\n\nadd links to related scenarios",
            "title": "Placement of sensors under uncertainty"
        },
        {
            "location": "/demos/optimize-sensors/#placement-of-sensors-under-uncertainty",
            "text": "Todo   describe the idea  add some images  add some links to related papers  add links to related github repos  add links to related scenarios",
            "title": "Placement of sensors under uncertainty"
        },
        {
            "location": "/demos/urban-heat-islands/",
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nUrban Heat Islands\n\u00b6\n\n\n\n\nTodo\n\n\n\n\nTranslate to English\n\n\nadd links to related github repos\n\n\nadd some images (but not too many)\n\n\nadd links to related papers\n\n\ndescribe APIs especially from the end-users' point of view\n\n\n\n\n\n\n\n\nTemperaturinseln in Karlsruhe und anderen St\u00e4dten\n\n\nTemperaturdaten: Volunteered geographic data (z.B. wunderground.com)\n\n\n\n\nKorrelation zwischen Bereichen in verschiedenen St\u00e4dten mit \u00e4hnlichem Temperaturverlauf \u2192 Zugang zu Ursachen f\u00fcr Temperaturentwicklung\n\n\n\n\n\n\nVorstellung der Heat-Islands-Analyse\n\n\n\n\nWetterstationen\n\n\nTechnik: Sensebox (\nhttps://sensebox.de\n)\n\n\nBeispielstation (\nhttps://opensensemap.org/explore/58b4354fe53e0b001251119d\n)\n\n\nHotspotanalyse (SoH, Stability of Hotspots):\n\n\nAbh\u00e4ngigkeit des Auftretens von Hotspots von der Aggregationsstufe\n\n\nAusblick:\n\n\nSensorfusion in Kooperation mit SDIL (smart data innovation lab)\n\n\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nSmart City",
            "title": "Urban Heat Islands"
        },
        {
            "location": "/demos/urban-heat-islands/#urban-heat-islands",
            "text": "Todo   Translate to English  add links to related github repos  add some images (but not too many)  add links to related papers  describe APIs especially from the end-users' point of view     Temperaturinseln in Karlsruhe und anderen St\u00e4dten  Temperaturdaten: Volunteered geographic data (z.B. wunderground.com)   Korrelation zwischen Bereichen in verschiedenen St\u00e4dten mit \u00e4hnlichem Temperaturverlauf \u2192 Zugang zu Ursachen f\u00fcr Temperaturentwicklung    Vorstellung der Heat-Islands-Analyse   Wetterstationen  Technik: Sensebox ( https://sensebox.de )  Beispielstation ( https://opensensemap.org/explore/58b4354fe53e0b001251119d )  Hotspotanalyse (SoH, Stability of Hotspots):  Abh\u00e4ngigkeit des Auftretens von Hotspots von der Aggregationsstufe  Ausblick:  Sensorfusion in Kooperation mit SDIL (smart data innovation lab)",
            "title": "Urban Heat Islands"
        },
        {
            "location": "/demos/urban-heat-islands/#related-scenarios",
            "text": "Smart City",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/enviro-car/",
            "text": "EnviroCar (Smart City)\n\u00b6\n\n\nThe human desire for mobility is an observable global trend. To visit a place of choice fast, cheaply and safely\nnowadays has become a basic need to mankind. This increasing demand for mobility is reflected by the worldwide CO2\nemissions where motorized individual transport is contributing an essential portion to the global CO2 emissions such\nthat it was the second largest sector in 2014. The health effects caused by the world's increasing traffic are,\nadditionally, not only restricted to the emitted pollutants and emission gases but also to factors such as noise\npollution. In a recent study, for example, about 40% of the population in EU countries is exposed to road traffic noise\nat levels exceeding 55 db(A).\n\n\nMotivation\n\u00b6\n\n\nIn this work, we by propose a holistic view to the analysis of mobility data by helping experts to develop and realize\nsustainable mobility concepts. We consider visual analysis as the natural way to interact with this kind of mobility\ndata supporting analysts to create, refine and verify hypotheses. Using data from the citizen-science platform\nenviroCar, we contribute a Visual Analytics system allowing analysts to leverage their background knowledge in the\nanalysis process.\n\n\nShort section about available data\n\u00b6\n\n\nThe used dataset contains approximately around 1.7 million data points. Each data record contains 24 attributes\nreflecting sensor values of the vehicle (e.g. speed, rpm, ...) as well as a CO2 estimation. Each data point is part of a\ntrip, which is described by a trajectory. While there are 5734 trips, a set of the trips' trajectories can furthermore\nbe associated with a sensor. There are 160 registered sensors which may be directly associated wit a vehicle. Additional\ninformation about the vehicle (type, etc.) is provided as well.\n\n\n\n\nPicture 1: Distribution of trips per sensor (german).\n\n\n\n\n\n\n\n\nPicture 2: Distribution of trips per month in 2016 and 2017 (german).\n\n\n\n\n\n\nVisual Analysis of Traffic Data\n\u00b6\n\n\n\n\nPicture 3: Visualization using a clock metaphor - temporal development of the average CO2 emissions.\n\n\n\n\n\n\n\n\nPicture 4: Trip trajectories\n\n\n\nExact course of all trip trajectories in the german city of M\u00f6nchengladbach between 0 am and 6 am \n(left)\n.\nMagnification of the selected region highlighted by the red rectangle \n(right)\n.\n\n\n\n\n\n\nPicture 5: Intersection\n\n\n\n\n(a)\n Dot Map of a frequently traveled intersection.\n\n(b)\n Dense pixel display visualization of the same intersection.\n    The data points are colored and sorted based after their CO2 emissions.\n\n(c)\n Data points sorted based on their speed.\n\n(d)\n Data points sorted based on their engine speed.\n\n\n\n\nVisual Interactive Logging and Provenance\n\u00b6\n\n\nPerforming interactions in various visualizations and chaining together various filtering, aggregation and navigation\nsteps can quickly become overwhelming and can lead to the analyst losing the overview of the analysis process. This can\nlead to frustration with the system and also a loss of trust in the findings. Another important aspect is the issue of\nreproducibility of results arising from the inherent complexity of the system. To cope with both of these problems, we\nneed to enable the analyst to maintain an overview of the previous analysis process.\n\n\n\n\nPicture 6: Systematic approach for the realization of interactive visual logging.\n\n\n\n\n\n\nPaper\n\u00b6\n\n\nMore info about this project can be found in our paper \"Visual Analysis of Urban Traffic Data based on High-Resolution\nand High-Dimensional Environmental Sensor Data\" (currently under submission)\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nSmart City\n\n\nEnvironment",
            "title": "EnviroCar (Smart City)"
        },
        {
            "location": "/demos/enviro-car/#envirocar-smart-city",
            "text": "The human desire for mobility is an observable global trend. To visit a place of choice fast, cheaply and safely\nnowadays has become a basic need to mankind. This increasing demand for mobility is reflected by the worldwide CO2\nemissions where motorized individual transport is contributing an essential portion to the global CO2 emissions such\nthat it was the second largest sector in 2014. The health effects caused by the world's increasing traffic are,\nadditionally, not only restricted to the emitted pollutants and emission gases but also to factors such as noise\npollution. In a recent study, for example, about 40% of the population in EU countries is exposed to road traffic noise\nat levels exceeding 55 db(A).",
            "title": "EnviroCar (Smart City)"
        },
        {
            "location": "/demos/enviro-car/#motivation",
            "text": "In this work, we by propose a holistic view to the analysis of mobility data by helping experts to develop and realize\nsustainable mobility concepts. We consider visual analysis as the natural way to interact with this kind of mobility\ndata supporting analysts to create, refine and verify hypotheses. Using data from the citizen-science platform\nenviroCar, we contribute a Visual Analytics system allowing analysts to leverage their background knowledge in the\nanalysis process.",
            "title": "Motivation"
        },
        {
            "location": "/demos/enviro-car/#short-section-about-available-data",
            "text": "The used dataset contains approximately around 1.7 million data points. Each data record contains 24 attributes\nreflecting sensor values of the vehicle (e.g. speed, rpm, ...) as well as a CO2 estimation. Each data point is part of a\ntrip, which is described by a trajectory. While there are 5734 trips, a set of the trips' trajectories can furthermore\nbe associated with a sensor. There are 160 registered sensors which may be directly associated wit a vehicle. Additional\ninformation about the vehicle (type, etc.) is provided as well.   Picture 1: Distribution of trips per sensor (german).     Picture 2: Distribution of trips per month in 2016 and 2017 (german).",
            "title": "Short section about available data"
        },
        {
            "location": "/demos/enviro-car/#visual-analysis-of-traffic-data",
            "text": "Picture 3: Visualization using a clock metaphor - temporal development of the average CO2 emissions.     Picture 4: Trip trajectories  \nExact course of all trip trajectories in the german city of M\u00f6nchengladbach between 0 am and 6 am  (left) .\nMagnification of the selected region highlighted by the red rectangle  (right) .    Picture 5: Intersection   (a)  Dot Map of a frequently traveled intersection. (b)  Dense pixel display visualization of the same intersection.\n    The data points are colored and sorted based after their CO2 emissions. (c)  Data points sorted based on their speed. (d)  Data points sorted based on their engine speed.",
            "title": "Visual Analysis of Traffic Data"
        },
        {
            "location": "/demos/enviro-car/#visual-interactive-logging-and-provenance",
            "text": "Performing interactions in various visualizations and chaining together various filtering, aggregation and navigation\nsteps can quickly become overwhelming and can lead to the analyst losing the overview of the analysis process. This can\nlead to frustration with the system and also a loss of trust in the findings. Another important aspect is the issue of\nreproducibility of results arising from the inherent complexity of the system. To cope with both of these problems, we\nneed to enable the analyst to maintain an overview of the previous analysis process.   Picture 6: Systematic approach for the realization of interactive visual logging.",
            "title": "Visual Interactive Logging and Provenance"
        },
        {
            "location": "/demos/enviro-car/#paper",
            "text": "More info about this project can be found in our paper \"Visual Analysis of Urban Traffic Data based on High-Resolution\nand High-Dimensional Environmental Sensor Data\" (currently under submission)",
            "title": "Paper"
        },
        {
            "location": "/demos/enviro-car/#related-scenarios",
            "text": "Smart City  Environment",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/hotspot-analysis/",
            "text": "Hotspot analysis (using G*)\n\u00b6\n\n\nProblem definition\n\u00b6\n\n\n\n\nWe have a map, in this case a map of land surface temperatures\n\n\nWe want to find hotspots, i.e., areas on map that are \nsignificantly different from their surrounding area\n.\n\n\nWe want to use Getis-Ord G* statistic as the tool for finding the hotspots\n\n\nsee section \nStandards Getis-ord\n\n\n\n\n\n\nWe want to parallelize the computation in our Spark cluster.\n\n\nsee section \nRasterized Getis-ord\n\n\n\n\n\n\n\n\n\n\nHotspot analysis using geotrellis\n\u00b6\n\n\nIn this section we show a simplified version of the hotspot analysis.\nWe use the \nGeotrellis\n library to achieve the parallelization.\nSome assumptions are:\n\n\n\n\nwe use 2-dimenational data (only the spatial part without the time component)\n\n\nwe store our data as a layer of tiles in geotrellis catalog (distributed raster)\n\n\nour hotspot analysis uses the standard G* with variable window\n\n\n\n\nFirst, we need to express the G* formula in terms of the map algebra operations.\n\n\nScala code\n\u00b6\n\n\nFull source code can be found in our github repository: \n\nbiggis-project/biggis-landuse\n\n\n// typical type definition used by geotrellis\n\n\ntype\n \nSpatialRDD\n \n=\n \nRDD\n[(\nSpatialKey\n, \nTile\n)]\n\n                  \nwith\n \nMetadata\n[\nTileLayerMetadata\n[\nSpatialKey\n]]\n\n\n\ndef\n \ngetisord\n(\nrdd\n:\n \nSpatialRDD\n,\n \nweightMatrix\n:\n \nKernel\n,\n\n             \nglobMean\n:\nDouble\n,\n \nglobStdev\n:\nDouble\n,\n \nnumPixels\n:\nLong\n)\n:\n \nSpatialRDD\n \n=\n \n{\n\n\n  \nval\n \nwcells\n:\n \nArray\n[\nDouble\n]\n \n=\n \nweightMatrix\n.\ntile\n.\ntoArrayDouble\n\n  \nval\n \nsumW\n:\n \nDouble\n \n=\n \nwcells\n.\nsum\n\n  \nval\n \nsumW2\n:\n \nDouble\n \n=\n \nwcells\n.\nmap\n(\nx\n \n=>\n \nx\n \n*\n \nx\n).\nsum\n\n\n  \n// variables used in the getis-ord formula\n\n  \nval\n \nA\n:\n \nDouble\n \n=\n \nglobalMean\n \n*\n \nsumW\n\n  \nval\n \nB\n:\n \nDouble\n \n=\n \nglobalStdev\n \n*\n \nMath\n.\nsqrt\n((\nnumPixels\n \n*\n \nsumW2\n \n-\n \nsumW\n \n*\n \nsumW\n)\n \n/\n \n(\nnumPixels\n \n-\n \n1\n))\n\n\n  \nrdd\n.\nwithContext\n \n{\n\n    \n_\n.\nbufferTiles\n(\nweightMatrix\n.\nextent\n)\n\n      \n.\nmapValues\n \n{\n \ntileWithCtx\n \n=>\n\n        \ntileWithCtx\n.\ntile\n\n          \n.\nfocalSum\n(\nweightMatrix\n,\n \nSome\n(\ntileWithCtx\n.\ntargetArea\n))\n \n// focal op.\n\n          \n.\nmapDouble\n \n{\n \nx\n \n=>\n \n(\nx\n \n-\n \nA\n)\n \n/\n \nB\n \n}\n \n// local op.\n\n      \n}\n\n  \n}\n\n\n}\n\n\n\n\n\nLet's assume, we already have the following variables:\n\n\n\n\nlayerReader\n: helper class to query tiles from geotrellis catalog/layer,\n\n\nlayerId\n: ID of the raster layer used as input raster,\n\n\nkernelRadius\n: size of the weight matrix (how many pixels)\n\n\n\n\n// RDD (distributed dataset from Apache Spark) representing all tiles in the layer \n\n\nval\n \nqueryResult\n:\n \nSpatialRDD\n \n=\n\n  \nlayerReader\n.\nread\n[\nSpatialKey\n, \nTile\n, \nTileLayerMetadata\n[\nSpatialKey\n]](\nlayerId\n)\n\n\n\n// here, we use a circular kernel as a weight matrix\n\n\nval\n \nweightMatrix\n \n=\n \nKernel\n.\ncircle\n(\nkernelRadius\n,\n\n                                 \nqueryResult\n.\nmetadata\n.\ncellwidth\n,\n\n                                 \nkernelRadius\n)\n\n\n\n// use precomputed histogram metadata (stored in zoom level 0 inside the layer)\n\n\nval\n \nstats\n \n=\n \nqueryResult\n.\nhistogram\n.\nstatistics\n\n\nrequire\n(\nstats\n.\nnonEmpty\n)\n\n\n\nval\n \nStatistics\n(\n_\n,\n \nglobMean\n,\n \n_\n,\n \n_\n,\n \nglobStdev\n,\n \n_\n,\n \n_\n)\n \n=\n \nstats\n.\nget\n\n\nval\n \nnumPixels\n \n=\n \nqueryResult\n.\nhistogram\n.\ntotalCount\n\n\n\n// apply the parallelized getis ord\n\n\nval\n \noutRdd\n \n=\n \ngetisord\n(\nqueryResult\n,\n \nweightMatrix\n,\n \nglobMean\n,\n \nglobStdev\n,\n \nnumPixels\n)\n\n\n\n\n\nThe result \noutRdd\n is an RDD (distributed dataset from Apache Spark) that can be further processed\nor stored as a new layer in geotrellis catalog.",
            "title": "Hotspot analysis (using G*)"
        },
        {
            "location": "/demos/hotspot-analysis/#hotspot-analysis-using-g",
            "text": "",
            "title": "Hotspot analysis (using G*)"
        },
        {
            "location": "/demos/hotspot-analysis/#problem-definition",
            "text": "We have a map, in this case a map of land surface temperatures  We want to find hotspots, i.e., areas on map that are  significantly different from their surrounding area .  We want to use Getis-Ord G* statistic as the tool for finding the hotspots  see section  Standards Getis-ord    We want to parallelize the computation in our Spark cluster.  see section  Rasterized Getis-ord",
            "title": "Problem definition"
        },
        {
            "location": "/demos/hotspot-analysis/#hotspot-analysis-using-geotrellis",
            "text": "In this section we show a simplified version of the hotspot analysis.\nWe use the  Geotrellis  library to achieve the parallelization.\nSome assumptions are:   we use 2-dimenational data (only the spatial part without the time component)  we store our data as a layer of tiles in geotrellis catalog (distributed raster)  our hotspot analysis uses the standard G* with variable window   First, we need to express the G* formula in terms of the map algebra operations.",
            "title": "Hotspot analysis using geotrellis"
        },
        {
            "location": "/demos/hotspot-analysis/#scala-code",
            "text": "Full source code can be found in our github repository:  biggis-project/biggis-landuse  // typical type definition used by geotrellis  type   SpatialRDD   =   RDD [( SpatialKey ,  Tile )] \n                   with   Metadata [ TileLayerMetadata [ SpatialKey ]]  def   getisord ( rdd :   SpatialRDD ,   weightMatrix :   Kernel , \n              globMean : Double ,   globStdev : Double ,   numPixels : Long ) :   SpatialRDD   =   { \n\n   val   wcells :   Array [ Double ]   =   weightMatrix . tile . toArrayDouble \n   val   sumW :   Double   =   wcells . sum \n   val   sumW2 :   Double   =   wcells . map ( x   =>   x   *   x ). sum \n\n   // variables used in the getis-ord formula \n   val   A :   Double   =   globalMean   *   sumW \n   val   B :   Double   =   globalStdev   *   Math . sqrt (( numPixels   *   sumW2   -   sumW   *   sumW )   /   ( numPixels   -   1 )) \n\n   rdd . withContext   { \n     _ . bufferTiles ( weightMatrix . extent ) \n       . mapValues   {   tileWithCtx   => \n         tileWithCtx . tile \n           . focalSum ( weightMatrix ,   Some ( tileWithCtx . targetArea ))   // focal op. \n           . mapDouble   {   x   =>   ( x   -   A )   /   B   }   // local op. \n       } \n   }  }   Let's assume, we already have the following variables:   layerReader : helper class to query tiles from geotrellis catalog/layer,  layerId : ID of the raster layer used as input raster,  kernelRadius : size of the weight matrix (how many pixels)   // RDD (distributed dataset from Apache Spark) representing all tiles in the layer   val   queryResult :   SpatialRDD   = \n   layerReader . read [ SpatialKey ,  Tile ,  TileLayerMetadata [ SpatialKey ]]( layerId )  // here, we use a circular kernel as a weight matrix  val   weightMatrix   =   Kernel . circle ( kernelRadius , \n                                  queryResult . metadata . cellwidth , \n                                  kernelRadius )  // use precomputed histogram metadata (stored in zoom level 0 inside the layer)  val   stats   =   queryResult . histogram . statistics  require ( stats . nonEmpty )  val   Statistics ( _ ,   globMean ,   _ ,   _ ,   globStdev ,   _ ,   _ )   =   stats . get  val   numPixels   =   queryResult . histogram . totalCount  // apply the parallelized getis ord  val   outRdd   =   getisord ( queryResult ,   weightMatrix ,   globMean ,   globStdev ,   numPixels )   The result  outRdd  is an RDD (distributed dataset from Apache Spark) that can be further processed\nor stored as a new layer in geotrellis catalog.",
            "title": "Scala code"
        },
        {
            "location": "/demos/invasive-species/",
            "text": "Responsible person for this section\n\n\n\n\nHannes M\u00fcller (LUBW)\n\n\nJohannes Kutterer (Disy)\n\n\nDaniel Seebacher (Uni Konstanz)\n\n\n\n\n\n\nInvasive species\n\u00b6\n\n\nMotiviation\n\u00b6\n\n\nInvasive species are a major cause of ecological damage and commercial losses. A current problem spreading in North\nAmerica and Europe is the vinegar fly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and healthy\nfruits and is therefore of concern to fruit growers, such as vintners.  Consequently, large amounts of data about the\noccurrence of D. suzukii have been collected in recent years. However, there is a lack of interactive methods to\ninvestigate this data.\n\n\nUsed Data Sources\n\u00b6\n\n\n\n\nATKIS/ALKIS landuse data\n\n\nCounts of trapped Drosophila suzukii published on \nVitiMeteo\n\n\nASTER Elevation Map\n\n\n\n\nData Description\n\u00b6\n\n\n\n\nFigure 1: Data provided by \nVitiMeteo\n: Distribution of vineyards and traps in Baden-Wuerttemberg\n\n\n\n\n\n\nIn the data provided by \nVitiMeteo\n are, among other things, observations of the spread of D. suzukii. This data\nconsists of trap findings of D. suzukii as well as percentage information about how many berries were infested in a\nsample taken at the station. Additionally, there is percentage information about how many eggs were found in a sample.\nThis percentage can be over 100 %, if there are more egg findings than berries in a sample. These observations are\ncollected from 867 stations non-uniformly spread over Baden-Wuerttemberg. Some of them only report observations for one\nday, others report multiple observations over a time period of up to 1641 days. The observations are rather sparse and\nirregularly sampled, which makes the use of standard time series analysis techniques challenging, if not impossible.\nConsequently, an interactive visual analysis should enable researchers to interactively analyze this complex data\nsource.\n\n\nHypothesis\n\u00b6\n\n\nPopulations of Drosophila suzukii depend on food and shelter in the winter months. The animals also need certain levels\nof shadow and humidty during the summer months to prevent dehydration. Forests and bushes close to the vineyards may\ntherefore support the survival of Drosophila suzukii by balancing extreme temperatures and providing diverse sources of\nfood during winter time.\n\n\nPrediction of Infested Areas\n\u00b6\n\n\nWe enriched the data provided by \nVitiMeteo\n, by adding information about the environmental surroundings of each\nstation. First, we added the height information, which we extracted from ASTER. Second, we added the surrounding land\nusage information. Since a local spread is possible by D. suzukii itself, we extracted the land usage information in a\n5~km radius around each station. Finally, we have an 85 dimensional feature vector for each instance, consisting of the\nmonth of the year, the station height, and the surrounding land usage.\n\n\nWe end up with a rather imbalanced data set with four times as many negative examples as positive ones. This can cause\nproblems since many machine learning algorithms depend on the assumption that the given data set is balanced. Although\nmachine learning techniques exist which can deal with imbalanced data sets, such as the Robust Decision Trees, we want\nto employ ensemble-based classification, which is a combination of different classifiers. This allows us to improve the\nclassification performance and also to model the uncertainty of our classification, which aids people in making more\ninformed decisions. This requires the creation of a balanced data set, which we can achieve by either using\nundersampling of the majority class or oversampling of the minority class. Undersampling can be achieved by stratified\nsampling using the occurrence class as strata. However, this would remove instances from our already small data set. To\navoid this, we employ oversampling of the minority class using the Synthetic Minority Over-sampling Technique (SMOTE).\nSMOTE picks pairs of nearest neighbors in the minority class and creates artificial instances by randomly placing a\npoint on the line between the nearest neighbors until the data is balanced. Thus, allowing us to employ default machine\nlearning algorithms.\n\n\nDevelopment of the Vector Data Pipeline in BigGIS\n\u00b6\n\n\nIf you are working with geo data you are faced with two different kinds of data types: vector and raster data. Both have\ndifferent requirements for collection, processing and storing of the data. The data provided by \nVitiMeteo\n are vector\ndata. On the webpage you can find information about egg findings of D. suzukii in berries, flies catch in traps and\nobservations of the species.  For each dataset you have a geographical position, e.g., a point.\n\n\nThe vector data pipeline performs the following steps:\n\n\n\n\nCollect the data from the source\n\n\nProcess the data\n\n\nVisualize the data\n\n\n\n\n\n\nSchematic visualisation of the Vector Data Pipeline for the Drosophila suzukii data from \nVitiMeteo\n\n\n\n\n\n\n1. Collection of KEF data\n\u00b6\n\n\nFor the demo the gathering is split in two separate steps. First the data is downloaded from the website and is saved\nlocally. All further steps are run on this data local data to avoid too much traffic on the \nVitiMeteo\n web site. The\nclient for downloading the data is based on \nSpring Batch\n and\n\nSpring Boot\n. Downloaded data is saved in GeoJSON-files.\nYou can find this client here: \nhttps://github.com/DisyInformationssysteme/biggis-download-kef-data\n\n\nThis GeoJSON file is handed over to a Kafka producer which is the first step of the stream processing prototype. The\ncode can be found on \nhttps://github.com/DisyInformationssysteme/biggis-import-kef-data-to-kafka\n\n\n2. Processing of the KEF data\n\u00b6\n\n\nThis code in implemented using Apache Flink. The implemented jobs feature the import of GeoJSON coded sensor locations\nand corresponding time series. Sources are Kafka queues. Destination is a PostGIS/Postgres database. (see\n\nhttps://github.com/DisyInformationssysteme/biggis-streaming-vector-data\n)\n\n\n3. Visualization\n\u00b6\n\n\nJust providing users with the raw results of our prediction is not sufficient as we generate over 20.000 predictions for\nall months and vineyards in Baden-Wuerttemberg. Furthermore, the raw results do not provide spatial context. Thus, it is\nnot interpretable which makes it hard for experts to integrate their domain knowledge into the analysis process. Hence,\nwe need visualization to help experts to identify spatial and temporal patterns easily. To achieve this, we follow the\nvisual information seeking mantra of Ben Shneiderman: \"Overview first, zoom and filter, details on demand\"\n\n\n\n\nFigure 2: Glyph-visualization of temporal-spatial event predictions as proposed by Seebacher et al.\n\n\n\n\n\n\nWe build a geographic information system, using a map as the basis for interaction and spatio-temporal analysis. We\nconsider the familiarity of domain experts with this kind of visualization as an additional benefit. We visualize our\npredictions on the corresponding position on the map so that users are immediately aware of the geographic context.\nAdditionally, combining our geographic visualization with a visualization of the temporal predictions into a single\nvisualization is more effective, since this requires less cognitive effort for the users. Existing related systems such\nas BirdVis offer heat map overview visualizations. However, as we want to investigate the distribution of a species over\ntime, we designed a map overlay consisting of several glyphs. This partially preserves the geographic context while\nglyph can be used to encode additional contextual information. The goal of our glyph is to visualize whether a certain\nregion is endangered or not. Consequently, we visually encode the classification results of a specific month represented\nby its time segment. The basic design of a time segment is depicted in Figure 2. Therefore, we make use of the interior\nof the respective time segment to represent the classification results of the ensemble-classifiers. For each month we\nhave a distribution of safe and endangered vineyards, according to the classification. Since the number of vineyards\nstays the same over all months for each glyph, we fill the area of the time segments according to the ratio of the\nbinary outcome (endangered, not endangered). This technique results in a radial glyph similar to a stacked bar chart\nshowing fractions of the whole. We use the colors red (endangered) and blue (not endangered), as derived from the\nwarm-cold color scale to distinguish the outcome. We provide additional functionality, such as semantic zoom, dynamic\naggregation and various details-on-demand data visualizations.\n\n\n\n\nFigure 3: Overview of the Drosophigator application\n\n\n\nDrosophigator enables experts to perform a visual analysis of spatio-temporal event predictions.\n\n\n\n\nUse Cases\n\u00b6\n\n\nWe want to highlight how visualization can help domain experts to gain insights about the spread dynamics of D. suzukii.\nWe show the usefulness of our system by demonstrating how domain experts can investigate hypotheses using Drosophigator.\nTherefore, we investigated two recently proposed assumptions about the time of infestation by the JKI and the influences\nof environmental factors by Pelton et al.\n\n\n\n\nFigure 4: Overview glyph-visualization of all vineyards in Baden-Wuerttemberg.\n\n\n\nThe development over the time-segments shows that the severity of infestation and the certainty of our prediction\nincreases in late summer and stays high until the end of the year. This corroborates the hypothesis of the JKI.\n\n\n\n\n\n\nFigure 5: Comparison of the vineyards contained in two neighboring cells.\n\n\n\nThe upper cell (purple) exhibits an earlier infestation by D. suzukii that the lower cell (brown). The parallel\ncoordinates plot shows, that the vineyards in the upper cell have around 10% more surrounding woodland (\nWald\n) than\nthose in the lower cell. This finding strongly supports the hypothesis of Pelton et al.\n\n\n\n\nEvaluation\n\u00b6\n\n\nWe presented our system at the 6\nth\n workshop of the working group \"D. Suzukii\" on the 5\nth\n and 6\nth\n of December in Bad\nKreuznach, Germany. The goal of this workshop is the mutual exchange of knowledge between researchers and practitioners.\nOver 80 biologists, researchers, agri- and horticulturists from various countries participated in the workshop. The\nfocus of our talk was our application \nDrosophigator\n, especially the design and interpretation of the glyph as well as\nthe interaction possibilities with the system.  After the presentation of the system, a questionnaire was handed out to\nthe workshop participants where they could rate the different aspects of our application and could provide us with\nadditional information about their background. We use the results of this questionnaire to evaluate our system and\ndesign decisions.\n\n\n\n\nFigure 7: Evaluation of system feedback of all participants (n=37)\n\n\n\nShown are the responses of the participants on questions regarding the visualization design (\nV1, V2, V3\n), the\ninteraction design (\nI1, I2\n) and the analysis capabilities (\nA1, A2, A3, A4\n) of our system Drosophigator.\n\n\n\n\nThe results of our evaluation make it clear that there is a strong need for intuitive and interactive systems, which\nsupport the experts in their daily analysis tasks. The experts are, for the most part, very positive about\n\nDrosphigator\n. Our glyph design was comprehensible, helped them to understand the temporal occurrence of D. suzukii and\nintegrating it in a map helped them to interpret the results. Additionally, allowing for a seamless clustering of\nvineyards into larger regions is deemed important, as it allows the analysis of micro- and macroecological factors.\nHowever, experts are still divided in their opinion, whether the application can support them in their work. This is\nreflected in their opinion about the possibility to infer causes for the occurrence of D. suzukii from our application.\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nEnvironment",
            "title": "Invasive species"
        },
        {
            "location": "/demos/invasive-species/#invasive-species",
            "text": "",
            "title": "Invasive species"
        },
        {
            "location": "/demos/invasive-species/#motiviation",
            "text": "Invasive species are a major cause of ecological damage and commercial losses. A current problem spreading in North\nAmerica and Europe is the vinegar fly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and healthy\nfruits and is therefore of concern to fruit growers, such as vintners.  Consequently, large amounts of data about the\noccurrence of D. suzukii have been collected in recent years. However, there is a lack of interactive methods to\ninvestigate this data.",
            "title": "Motiviation"
        },
        {
            "location": "/demos/invasive-species/#used-data-sources",
            "text": "ATKIS/ALKIS landuse data  Counts of trapped Drosophila suzukii published on  VitiMeteo  ASTER Elevation Map",
            "title": "Used Data Sources"
        },
        {
            "location": "/demos/invasive-species/#data-description",
            "text": "Figure 1: Data provided by  VitiMeteo : Distribution of vineyards and traps in Baden-Wuerttemberg    In the data provided by  VitiMeteo  are, among other things, observations of the spread of D. suzukii. This data\nconsists of trap findings of D. suzukii as well as percentage information about how many berries were infested in a\nsample taken at the station. Additionally, there is percentage information about how many eggs were found in a sample.\nThis percentage can be over 100 %, if there are more egg findings than berries in a sample. These observations are\ncollected from 867 stations non-uniformly spread over Baden-Wuerttemberg. Some of them only report observations for one\nday, others report multiple observations over a time period of up to 1641 days. The observations are rather sparse and\nirregularly sampled, which makes the use of standard time series analysis techniques challenging, if not impossible.\nConsequently, an interactive visual analysis should enable researchers to interactively analyze this complex data\nsource.",
            "title": "Data Description"
        },
        {
            "location": "/demos/invasive-species/#hypothesis",
            "text": "Populations of Drosophila suzukii depend on food and shelter in the winter months. The animals also need certain levels\nof shadow and humidty during the summer months to prevent dehydration. Forests and bushes close to the vineyards may\ntherefore support the survival of Drosophila suzukii by balancing extreme temperatures and providing diverse sources of\nfood during winter time.",
            "title": "Hypothesis"
        },
        {
            "location": "/demos/invasive-species/#prediction-of-infested-areas",
            "text": "We enriched the data provided by  VitiMeteo , by adding information about the environmental surroundings of each\nstation. First, we added the height information, which we extracted from ASTER. Second, we added the surrounding land\nusage information. Since a local spread is possible by D. suzukii itself, we extracted the land usage information in a\n5~km radius around each station. Finally, we have an 85 dimensional feature vector for each instance, consisting of the\nmonth of the year, the station height, and the surrounding land usage.  We end up with a rather imbalanced data set with four times as many negative examples as positive ones. This can cause\nproblems since many machine learning algorithms depend on the assumption that the given data set is balanced. Although\nmachine learning techniques exist which can deal with imbalanced data sets, such as the Robust Decision Trees, we want\nto employ ensemble-based classification, which is a combination of different classifiers. This allows us to improve the\nclassification performance and also to model the uncertainty of our classification, which aids people in making more\ninformed decisions. This requires the creation of a balanced data set, which we can achieve by either using\nundersampling of the majority class or oversampling of the minority class. Undersampling can be achieved by stratified\nsampling using the occurrence class as strata. However, this would remove instances from our already small data set. To\navoid this, we employ oversampling of the minority class using the Synthetic Minority Over-sampling Technique (SMOTE).\nSMOTE picks pairs of nearest neighbors in the minority class and creates artificial instances by randomly placing a\npoint on the line between the nearest neighbors until the data is balanced. Thus, allowing us to employ default machine\nlearning algorithms.",
            "title": "Prediction of Infested Areas"
        },
        {
            "location": "/demos/invasive-species/#development-of-the-vector-data-pipeline-in-biggis",
            "text": "If you are working with geo data you are faced with two different kinds of data types: vector and raster data. Both have\ndifferent requirements for collection, processing and storing of the data. The data provided by  VitiMeteo  are vector\ndata. On the webpage you can find information about egg findings of D. suzukii in berries, flies catch in traps and\nobservations of the species.  For each dataset you have a geographical position, e.g., a point.  The vector data pipeline performs the following steps:   Collect the data from the source  Process the data  Visualize the data    Schematic visualisation of the Vector Data Pipeline for the Drosophila suzukii data from  VitiMeteo",
            "title": "Development of the Vector Data Pipeline in BigGIS"
        },
        {
            "location": "/demos/invasive-species/#1-collection-of-kef-data",
            "text": "For the demo the gathering is split in two separate steps. First the data is downloaded from the website and is saved\nlocally. All further steps are run on this data local data to avoid too much traffic on the  VitiMeteo  web site. The\nclient for downloading the data is based on  Spring Batch  and Spring Boot . Downloaded data is saved in GeoJSON-files.\nYou can find this client here:  https://github.com/DisyInformationssysteme/biggis-download-kef-data  This GeoJSON file is handed over to a Kafka producer which is the first step of the stream processing prototype. The\ncode can be found on  https://github.com/DisyInformationssysteme/biggis-import-kef-data-to-kafka",
            "title": "1. Collection of KEF data"
        },
        {
            "location": "/demos/invasive-species/#2-processing-of-the-kef-data",
            "text": "This code in implemented using Apache Flink. The implemented jobs feature the import of GeoJSON coded sensor locations\nand corresponding time series. Sources are Kafka queues. Destination is a PostGIS/Postgres database. (see https://github.com/DisyInformationssysteme/biggis-streaming-vector-data )",
            "title": "2. Processing of the KEF data"
        },
        {
            "location": "/demos/invasive-species/#3-visualization",
            "text": "Just providing users with the raw results of our prediction is not sufficient as we generate over 20.000 predictions for\nall months and vineyards in Baden-Wuerttemberg. Furthermore, the raw results do not provide spatial context. Thus, it is\nnot interpretable which makes it hard for experts to integrate their domain knowledge into the analysis process. Hence,\nwe need visualization to help experts to identify spatial and temporal patterns easily. To achieve this, we follow the\nvisual information seeking mantra of Ben Shneiderman: \"Overview first, zoom and filter, details on demand\"   Figure 2: Glyph-visualization of temporal-spatial event predictions as proposed by Seebacher et al.    We build a geographic information system, using a map as the basis for interaction and spatio-temporal analysis. We\nconsider the familiarity of domain experts with this kind of visualization as an additional benefit. We visualize our\npredictions on the corresponding position on the map so that users are immediately aware of the geographic context.\nAdditionally, combining our geographic visualization with a visualization of the temporal predictions into a single\nvisualization is more effective, since this requires less cognitive effort for the users. Existing related systems such\nas BirdVis offer heat map overview visualizations. However, as we want to investigate the distribution of a species over\ntime, we designed a map overlay consisting of several glyphs. This partially preserves the geographic context while\nglyph can be used to encode additional contextual information. The goal of our glyph is to visualize whether a certain\nregion is endangered or not. Consequently, we visually encode the classification results of a specific month represented\nby its time segment. The basic design of a time segment is depicted in Figure 2. Therefore, we make use of the interior\nof the respective time segment to represent the classification results of the ensemble-classifiers. For each month we\nhave a distribution of safe and endangered vineyards, according to the classification. Since the number of vineyards\nstays the same over all months for each glyph, we fill the area of the time segments according to the ratio of the\nbinary outcome (endangered, not endangered). This technique results in a radial glyph similar to a stacked bar chart\nshowing fractions of the whole. We use the colors red (endangered) and blue (not endangered), as derived from the\nwarm-cold color scale to distinguish the outcome. We provide additional functionality, such as semantic zoom, dynamic\naggregation and various details-on-demand data visualizations.   Figure 3: Overview of the Drosophigator application  \nDrosophigator enables experts to perform a visual analysis of spatio-temporal event predictions.",
            "title": "3. Visualization"
        },
        {
            "location": "/demos/invasive-species/#use-cases",
            "text": "We want to highlight how visualization can help domain experts to gain insights about the spread dynamics of D. suzukii.\nWe show the usefulness of our system by demonstrating how domain experts can investigate hypotheses using Drosophigator.\nTherefore, we investigated two recently proposed assumptions about the time of infestation by the JKI and the influences\nof environmental factors by Pelton et al.   Figure 4: Overview glyph-visualization of all vineyards in Baden-Wuerttemberg.  \nThe development over the time-segments shows that the severity of infestation and the certainty of our prediction\nincreases in late summer and stays high until the end of the year. This corroborates the hypothesis of the JKI.    Figure 5: Comparison of the vineyards contained in two neighboring cells.  \nThe upper cell (purple) exhibits an earlier infestation by D. suzukii that the lower cell (brown). The parallel\ncoordinates plot shows, that the vineyards in the upper cell have around 10% more surrounding woodland ( Wald ) than\nthose in the lower cell. This finding strongly supports the hypothesis of Pelton et al.",
            "title": "Use Cases"
        },
        {
            "location": "/demos/invasive-species/#evaluation",
            "text": "We presented our system at the 6 th  workshop of the working group \"D. Suzukii\" on the 5 th  and 6 th  of December in Bad\nKreuznach, Germany. The goal of this workshop is the mutual exchange of knowledge between researchers and practitioners.\nOver 80 biologists, researchers, agri- and horticulturists from various countries participated in the workshop. The\nfocus of our talk was our application  Drosophigator , especially the design and interpretation of the glyph as well as\nthe interaction possibilities with the system.  After the presentation of the system, a questionnaire was handed out to\nthe workshop participants where they could rate the different aspects of our application and could provide us with\nadditional information about their background. We use the results of this questionnaire to evaluate our system and\ndesign decisions.   Figure 7: Evaluation of system feedback of all participants (n=37)  \nShown are the responses of the participants on questions regarding the visualization design ( V1, V2, V3 ), the\ninteraction design ( I1, I2 ) and the analysis capabilities ( A1, A2, A3, A4 ) of our system Drosophigator.   The results of our evaluation make it clear that there is a strong need for intuitive and interactive systems, which\nsupport the experts in their daily analysis tasks. The experts are, for the most part, very positive about Drosphigator . Our glyph design was comprehensible, helped them to understand the temporal occurrence of D. suzukii and\nintegrating it in a map helped them to interpret the results. Additionally, allowing for a seamless clustering of\nvineyards into larger regions is deemed important, as it allows the analysis of micro- and macroecological factors.\nHowever, experts are still divided in their opinion, whether the application can support them in their work. This is\nreflected in their opinion about the possibility to infer causes for the occurrence of D. suzukii from our application.",
            "title": "Evaluation"
        },
        {
            "location": "/demos/invasive-species/#related-scenarios",
            "text": "Environment",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/optical-remote-sensing/",
            "text": "Optical Remote Sensing\n\u00b6\n\n\nThe starting point for the BOS scenario in the BigGIS frame is the satellite-based emergency management services of\nCopernicus or the charter \u201cSpace and major Disasters\u201d. The idea was to implement similar sensors on an unmanned aerial\nvehicle (UAV) platform and bring it to smaller incidents like larger fires or CBRN. Therefore, thermal (IR) and\nhyperspectral cameras was used as well as RGB cameras to do some testing in simulated situations:\n\n\n\n\nDetection and following smoke clouds in imageries\n\n\nDetection of \u201cnon visible\u201d gas clouds\n\n\nIdentification of \u201cchemicals\u201d\n\n\n\n\nGas Cloud Detection\n\u00b6\n\n\nTo perform the simulations several test scenarios were prepared in two campaigns in Karlsruhe and Dortmund. The smoke of\nHeptane (UN 1206/Kemmler 33) was recorded as well as a mixture from gasoline (1203/33) and diesel (1202/30). A gas\nleakage was simulated at the Dortmund Fire Brigade Education Center using Methane (\nCH_4\nCH_4\n; 1971/23). And a gas cloud\ncontaining \u201cchemicals\u201d was simulated by a fog machine which nebulized a 50 % mixture of propylene glycol\n(propane-1,2-diol) and chlorophyll from the food branch.\n\n\nFirst analysis eg. for the \u201cinvisible\u201d Methane gas cloud show quite good results using the IR cameras. On UAVs offered\nby Sitebots and AI Drones two choices of cameras were used:\n\n\n\n\nOPTRIS PI\n\n\nFLIR Vue Pro R\n\n\n\n\nBoth cameras give the radiometric signatures and not just \u201ccolored pictures\u201d. The Images were spatially referenced by\nstandard procedures. It was found that building differences just show intereferences in the pictures (shown in the first\nrow of Picture 1). Good results were given by a Halcon referencing based on sub pixel accuracy (row two in Picture 1).\nUsing difference analysis on about 25 pictures show a clear signature of the exhaling methane (row three in Picture 1).\n\n\n\n\nPicture 1: Gas cloud detection using thermal imageries\n\n\n\n\n\n\n\n\nSpectral Analysis of Gas Clouds\n\u00b6\n\n\nThe idea of remotely detecting and identifying chemicals using a hyperspectral sensor is not new.\nThe so called Analytical Task Force (ATF\n1\n) is using the Van-based RAMAN spectroscope SIGIS 2 to\ndetect and identify chemicals in CBRN incidents.\n\n\nThe BigGIS project intended to be more flexible than a SIGIS 2 mounted in a car. Therefore, as a proof-of-concept the\nimplementation of a system allowing for spectral analysis of gas and aerosol clouds mounted on a UAV was subject of\nstudy on the level of a proof-of-concept. Due to the fact that multispectral IR-sensors as they are utilized in the\nSIGIS 2 system require relatively heavy-weight cooling units disqualify these systems for the usage with a UAV.\nTherefore, within the project a multispectral sensor sensitive in the spectral range of visible light and near IR (450 -\n950 nm) was utilized for spectral analyzation. The sensor Cubert 185 UHD Firefly\n2\n has a weight of about 500 g\nand could easily be mounted on a UAV.\n\n\nUsing this sensor \u201csmoke clouds\u201d from a mixture of \u201cDisco fog\u201d and chlorophyll (see below) were recorded. Each pixel in\nthe image contains the spectral information of the reflected light spread over 125 bands ranging from a minimum\nwavelength of 450nm to a maximum of 950nm.\n\n\nTriangular Chlorophyll Index\n\u00b6\n\n\nIn a first Analysis the propagating chlorophyll cloud was identified in the recorded image via the calculation of the\n\nTriangular Chlorophyll Index (TCI)\n for each pixel in the spatially referenced\nimage.\n\n\nThe result is shown in the picture below:\nRow one in Picture 2 is showing the chlorophyll cloud propagating from west\nto east through spatially referenced pseudo color pictures. Interesting\nis the underground partly paved and partly consisted of a grass strip.\nIn the right picture the cloud covers a small\ntree.\n\n\nThe TCI algorithm was applied on the three pictures in the middle row.\nUsing a reference aerial photograph and building the difference to such\nan image (third row) one can find chlorophyll only detected on the\nasphalt, not on the green strip or the tree due to the fact that the\nmethod cannot distinguish chlorophyll from the plants from\nchlorophyll of the cloud.\n\n\n\n\nTODO: Chemical cloud detection\n\n\n\nWie ist die obige Erkl\u00e4rung zu verstehen? Geht es hier darum die Bereiche mit zeitlich konstant hohem TCI-Wert (=\nnicht die Chlorophyll-Wolke) auszuschlie\u00dfen?\n\n\n\n\nPicture 3 now shows the composition of the referenced and analyzed\npictures with all three stages of the moving chlorophyll cloud. One now can\nsee the grass strip and the tree in addition to the gas cloud over the\nasphalt.\n\n\n\n\nPicture 3: TODO: Eingef\u00e4rbte Elemente detailierter erkl\u00e4ren.\n\n\n\n\n\n\nLogistic Regression Classification of Cloud Reflectance\n\u00b6\n\n\nA second experiment set was dedicated to a more general evaluation of cloud constituents. Here, the identification of\nconstituents via logistic regression classification bases on the whole spectral range of the Cubert 185 UHD Firefly, in\ncontrary to the previous example where the identification based on only three wavelengths.\n\n\nAs a proof of concept, multispectral images of clouds produced by a fog generator fueled with two different fog fluids\nand solutions of each fog fluid mixed with defined proportions of chlorophyll (TODO: fog fluids genauer spezifizieren\nund chlorophyll-l\u00f6sung) were recorded.\n\n\nIn the experimental setup the Cubert 185 UHD Firefly was positioned facing a white wall in 1.2 m distance as constant\nbackground. The fog generator was placed between camera and wall in such a way, that the ejected cloud passed the camera\nin roughly 0.6 m distance while filling the whole recorded image plane. Immediately before each measurement set the\nincident intensity (white-balance intensity) in front of the camera was recorded by capturing the reflectance of a\nspectralon coated sheet at a distance of 0.6 m. By dividing the recorded reflected cloud intensities by the\nwhite-balance intensity the cloud reflectance was determined.\n\n\n\n\n\nThe reflectance spectra of evaporated mixtures of fog fluid with chlorophyll not in all cases show the typical\nchlorophyll absorption minimum. Consequently, chlorophyll is not uniformly evaporated with the given setup but ejected\nin irregular chlorophyll bursts instead. Therefore, a TCI pre-evaluation was carried through on each pixel of all\nrecordings of clouds of evaporated chlorophyll mixtures, in order to label positive chlorophyll spectra as training and\ntest data for the logistic regression. After spectra inspection of several samples a threshold of \nTCI = 0.05\nTCI = 0.05\n was\nchosen above which the spectra was labeled chlorophyll-containing. Spectra of evaporated chlorophyll mixtures with\n\nTCI < 0.05\nTCI < 0.05\n were not regarded in the further analysis. The spectra of clouds of evaporated pure fog fluids provided the\nnegative chlorophyll data. The table below shows the count of spectra for the different cloud categories that was used\nfor training for the logistic regression classifiers in the next subsections.\n\n\n\n\n\n\n\n\n\n\nFog Fluid 1\n\n\nFog Fluid 2\n\n\n\n\n\n\n\n\n\n\nNo Chlorophyll\n\n\n5000\n\n\n7500\n\n\n\n\n\n\nChlorophyll\n\n\n3926\n\n\n68541\n\n\n\n\n\n\n\n\nThe test data set comprised the following sample sizes:\n\n\n\n\n\n\n\n\n\n\nFog Fluid 1\n\n\nFog Fluid 2\n\n\n\n\n\n\n\n\n\n\nNo Chlorophyll\n\n\n2500\n\n\n2500\n\n\n\n\n\n\nChlorophyll\n\n\n2500\n\n\n2463\n\n\n\n\n\n\n\n\nBased on this data different logistic regression classifiers were trained.\n\n\nChlorophyll vs. Non-Chlorophyll\n\u00b6\n\n\n\n\nFigure TODO: Pure Fog vs Chlorophyl\n\n\n\n\n\n\nFirst a logistic regression classifier was trained to distinguish between spectra of clouds containing chlorophyll and\npure fog fluid, irrespective of the type of the fog fluid. The figure above displays typical spectra for a cloud aof\npure fog fluid and a cloud containing chlorophyll. In the latter case, the reflectance minimum due to the absorption\nmaximum of chlorophyll can clearly be seen in the curve around channel 45. This also is the region where the variable\nimportance (i.e. value of the t\u2013statistic for each model parameter (= channel)) peaks and marks the most significant\nchannels of the classifier.\n\n\nThe training accuracy was found to be\n$$\nAcc_{train} = \\frac{true Positives + true Negatives}{Positives + Negatives} = 1.\n$$\n\n\nThe out-of-sample test also showed very reliable results:\n\n\nTest samples with fog fluid 1:\n\n\n\n\n\n\n\n\n\n\nClassified No-Chloro.\n\n\nClassified Chloro.\n\n\n\n\n\n\n\n\n\n\nSample No-Chloro.\n\n\n2500\n\n\n0\n\n\n\n\n\n\nSample Chloro.\n\n\n0\n\n\n2500\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 1\n\n\n\n\nAcc_{test} = 1\n\n\n\n\n\n\n\n\nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1\n\n\n\n\nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1\n\n\n\n\n\nTest samples with fog fluid 2:\n\n\n\n\n\n\n\n\n\n\nClassified No-Chloro.\n\n\nClassified Chloro.\n\n\n\n\n\n\n\n\n\n\nSample No-Chloro.\n\n\n2500\n\n\n0\n\n\n\n\n\n\nSample Chloro.\n\n\n0\n\n\n2463\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 1\n\n\n\n\nAcc_{test} = 1\n\n\n\n\n\nThe high accuracy bases on the very clear feature of the chlorophyll absorption dip in the spectrum.\n\n\nPure Fog Fluid 1 vs. Fog Fluid 2\n\u00b6\n\n\n\n\nWhile the spectra of chlorophyll containing clouds show a clear distinction feature against the non-chlorophyll\ncontaining spectra, the reflectance spectra of the two pure fog fluids show a similar relative pattern, while the\nabsolute reflectance level of the fog fluid 1 seems to be elevated compared to fog fluid 2 (see figure above).\nTherefore, a logistic regression classifier was trained for the distinction of clouds of the two pure fog fluids. The\ntraining accuracy for this classifier was found to be:\n$$\nAcc_{train} = 1\n$$\n\n\nOut-of-sample performance:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n2452\n\n\n48\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n1\n\n\n2499\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 0.99\n\n\n\n\nAcc_{test} = 0.99\n\n\n\n\n\n\n\n\nF1_{test} = 0.99\n\n\n\n\nF1_{test} = 0.99\n\n\n\n\n\nDespite the lack of prominent characteristic features in the reflectance spectra the accuracy and the F1 score of the\nclassifier is yet rather high. That changes when the classifier is tested on reflectance data of mixtures of the\ndifferent fog fluids with chlorophyll, as shown in the table below:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n0\n\n\n2500\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n0\n\n\n2463\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 0.49\n\n\n\n\nAcc_{test} = 0.49\n\n\n\n\n\n\n\n\nF1_{test} = 0.66\n\n\n\n\nF1_{test} = 0.66\n\n\n\n\n\nObviously this classifier is not robust against the mixture of features of another substance in the cloud.\n\n\nFog Fluid 1 vs. Fog Fluid 2 (with and without Chlorophyll)\n\u00b6\n\n\nTo overcome the weakness of the previous classifier another classifier for the distinction between different fog fluids\nwas trained including the reflectance spectra of mixtures with chlorophyll. Here, the training accuracy was found to be:\n$$\nAcc_{train} = 1\n$$\n\n\nThe out of sample performance for clouds of pure fog fluids is shown in the table below:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n2500\n\n\n0\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n0\n\n\n2500\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 1\n\n\n\n\nAcc_{test} = 1\n\n\n\n\n\n\n\n\nF1_{test} = 1\n\n\n\n\nF1_{test} = 1\n\n\n\n\n\nInterestingly, the performance on the test data of pure fog fluids is even slightly better when trained with data\nsamples including mixtures with chlorophyll. This might be explained with overfitting in the case of training only with\ndata of pure fog fluids (TODO diskutieren).\n\n\nThe out-of-sample performance for mixtures of the different fog fluids with chlorophyll is shown in the table below:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n2014\n\n\n486\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n11\n\n\n2452\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 0.90\n\n\n\n\nAcc_{test} = 0.90\n\n\n\n\n\n\n\n\nF1_{test} = 0.90\n\n\n\n\nF1_{test} = 0.90\n\n\n\n\n\nThe performance is rather convincing even though the spectra of the fog fluids are overlayed with the\ncharacteristic chlorophyll spectrum.\n\n\nConclusion\n\u00b6\n\n\nIt was shown that logistic regression can be used to identify constituents of clouds on basis of the reflectance spectra\nin the range of visible light with the here presented system, especially when strong characteristic features are present\nas in the case of chlorophyll. Besides that, even the identification of constituents with weaker characteristic features\ncan be carried through as in the case for the distinction between the two fog fluids. For the latter case, the above\nfindings suggest that the classifier for the constituent of interest should be carried through with data also regarding\na variety of probable accompanying cloud constituents. Otherwise, the overlay of the different spectra might diminish\nthe classifiers performance.\n\n\n\n\n\n\n\n\n\n\nhttps://www.bbk.bund.de/DE/AufgabenundAusstattung/CBRNSchutz/TaskForce/ATF_einstieg1.html\n\u00a0\n\u21a9\n\n\n\n\n\n\nhttp://cubert-gmbh.com/uhd-185-firefly/\n\u00a0\n\u21a9",
            "title": "Optical Remote Sensing"
        },
        {
            "location": "/demos/optical-remote-sensing/#optical-remote-sensing",
            "text": "The starting point for the BOS scenario in the BigGIS frame is the satellite-based emergency management services of\nCopernicus or the charter \u201cSpace and major Disasters\u201d. The idea was to implement similar sensors on an unmanned aerial\nvehicle (UAV) platform and bring it to smaller incidents like larger fires or CBRN. Therefore, thermal (IR) and\nhyperspectral cameras was used as well as RGB cameras to do some testing in simulated situations:   Detection and following smoke clouds in imageries  Detection of \u201cnon visible\u201d gas clouds  Identification of \u201cchemicals\u201d",
            "title": "Optical Remote Sensing"
        },
        {
            "location": "/demos/optical-remote-sensing/#gas-cloud-detection",
            "text": "To perform the simulations several test scenarios were prepared in two campaigns in Karlsruhe and Dortmund. The smoke of\nHeptane (UN 1206/Kemmler 33) was recorded as well as a mixture from gasoline (1203/33) and diesel (1202/30). A gas\nleakage was simulated at the Dortmund Fire Brigade Education Center using Methane ( CH_4 CH_4 ; 1971/23). And a gas cloud\ncontaining \u201cchemicals\u201d was simulated by a fog machine which nebulized a 50 % mixture of propylene glycol\n(propane-1,2-diol) and chlorophyll from the food branch.  First analysis eg. for the \u201cinvisible\u201d Methane gas cloud show quite good results using the IR cameras. On UAVs offered\nby Sitebots and AI Drones two choices of cameras were used:   OPTRIS PI  FLIR Vue Pro R   Both cameras give the radiometric signatures and not just \u201ccolored pictures\u201d. The Images were spatially referenced by\nstandard procedures. It was found that building differences just show intereferences in the pictures (shown in the first\nrow of Picture 1). Good results were given by a Halcon referencing based on sub pixel accuracy (row two in Picture 1).\nUsing difference analysis on about 25 pictures show a clear signature of the exhaling methane (row three in Picture 1).   Picture 1: Gas cloud detection using thermal imageries",
            "title": "Gas Cloud Detection"
        },
        {
            "location": "/demos/optical-remote-sensing/#spectral-analysis-of-gas-clouds",
            "text": "The idea of remotely detecting and identifying chemicals using a hyperspectral sensor is not new.\nThe so called Analytical Task Force (ATF 1 ) is using the Van-based RAMAN spectroscope SIGIS 2 to\ndetect and identify chemicals in CBRN incidents.  The BigGIS project intended to be more flexible than a SIGIS 2 mounted in a car. Therefore, as a proof-of-concept the\nimplementation of a system allowing for spectral analysis of gas and aerosol clouds mounted on a UAV was subject of\nstudy on the level of a proof-of-concept. Due to the fact that multispectral IR-sensors as they are utilized in the\nSIGIS 2 system require relatively heavy-weight cooling units disqualify these systems for the usage with a UAV.\nTherefore, within the project a multispectral sensor sensitive in the spectral range of visible light and near IR (450 -\n950 nm) was utilized for spectral analyzation. The sensor Cubert 185 UHD Firefly 2  has a weight of about 500 g\nand could easily be mounted on a UAV.  Using this sensor \u201csmoke clouds\u201d from a mixture of \u201cDisco fog\u201d and chlorophyll (see below) were recorded. Each pixel in\nthe image contains the spectral information of the reflected light spread over 125 bands ranging from a minimum\nwavelength of 450nm to a maximum of 950nm.",
            "title": "Spectral Analysis of Gas Clouds"
        },
        {
            "location": "/demos/optical-remote-sensing/#triangular-chlorophyll-index",
            "text": "In a first Analysis the propagating chlorophyll cloud was identified in the recorded image via the calculation of the Triangular Chlorophyll Index (TCI)  for each pixel in the spatially referenced\nimage.  The result is shown in the picture below:\nRow one in Picture 2 is showing the chlorophyll cloud propagating from west\nto east through spatially referenced pseudo color pictures. Interesting\nis the underground partly paved and partly consisted of a grass strip.\nIn the right picture the cloud covers a small\ntree.  The TCI algorithm was applied on the three pictures in the middle row.\nUsing a reference aerial photograph and building the difference to such\nan image (third row) one can find chlorophyll only detected on the\nasphalt, not on the green strip or the tree due to the fact that the\nmethod cannot distinguish chlorophyll from the plants from\nchlorophyll of the cloud.   TODO: Chemical cloud detection  \nWie ist die obige Erkl\u00e4rung zu verstehen? Geht es hier darum die Bereiche mit zeitlich konstant hohem TCI-Wert (=\nnicht die Chlorophyll-Wolke) auszuschlie\u00dfen?   Picture 3 now shows the composition of the referenced and analyzed\npictures with all three stages of the moving chlorophyll cloud. One now can\nsee the grass strip and the tree in addition to the gas cloud over the\nasphalt.   Picture 3: TODO: Eingef\u00e4rbte Elemente detailierter erkl\u00e4ren.",
            "title": "Triangular Chlorophyll Index"
        },
        {
            "location": "/demos/optical-remote-sensing/#logistic-regression-classification-of-cloud-reflectance",
            "text": "A second experiment set was dedicated to a more general evaluation of cloud constituents. Here, the identification of\nconstituents via logistic regression classification bases on the whole spectral range of the Cubert 185 UHD Firefly, in\ncontrary to the previous example where the identification based on only three wavelengths.  As a proof of concept, multispectral images of clouds produced by a fog generator fueled with two different fog fluids\nand solutions of each fog fluid mixed with defined proportions of chlorophyll (TODO: fog fluids genauer spezifizieren\nund chlorophyll-l\u00f6sung) were recorded.  In the experimental setup the Cubert 185 UHD Firefly was positioned facing a white wall in 1.2 m distance as constant\nbackground. The fog generator was placed between camera and wall in such a way, that the ejected cloud passed the camera\nin roughly 0.6 m distance while filling the whole recorded image plane. Immediately before each measurement set the\nincident intensity (white-balance intensity) in front of the camera was recorded by capturing the reflectance of a\nspectralon coated sheet at a distance of 0.6 m. By dividing the recorded reflected cloud intensities by the\nwhite-balance intensity the cloud reflectance was determined.   The reflectance spectra of evaporated mixtures of fog fluid with chlorophyll not in all cases show the typical\nchlorophyll absorption minimum. Consequently, chlorophyll is not uniformly evaporated with the given setup but ejected\nin irregular chlorophyll bursts instead. Therefore, a TCI pre-evaluation was carried through on each pixel of all\nrecordings of clouds of evaporated chlorophyll mixtures, in order to label positive chlorophyll spectra as training and\ntest data for the logistic regression. After spectra inspection of several samples a threshold of  TCI = 0.05 TCI = 0.05  was\nchosen above which the spectra was labeled chlorophyll-containing. Spectra of evaporated chlorophyll mixtures with TCI < 0.05 TCI < 0.05  were not regarded in the further analysis. The spectra of clouds of evaporated pure fog fluids provided the\nnegative chlorophyll data. The table below shows the count of spectra for the different cloud categories that was used\nfor training for the logistic regression classifiers in the next subsections.      Fog Fluid 1  Fog Fluid 2      No Chlorophyll  5000  7500    Chlorophyll  3926  68541     The test data set comprised the following sample sizes:      Fog Fluid 1  Fog Fluid 2      No Chlorophyll  2500  2500    Chlorophyll  2500  2463     Based on this data different logistic regression classifiers were trained.",
            "title": "Logistic Regression Classification of Cloud Reflectance"
        },
        {
            "location": "/demos/optical-remote-sensing/#chlorophyll-vs-non-chlorophyll",
            "text": "Figure TODO: Pure Fog vs Chlorophyl    First a logistic regression classifier was trained to distinguish between spectra of clouds containing chlorophyll and\npure fog fluid, irrespective of the type of the fog fluid. The figure above displays typical spectra for a cloud aof\npure fog fluid and a cloud containing chlorophyll. In the latter case, the reflectance minimum due to the absorption\nmaximum of chlorophyll can clearly be seen in the curve around channel 45. This also is the region where the variable\nimportance (i.e. value of the t\u2013statistic for each model parameter (= channel)) peaks and marks the most significant\nchannels of the classifier.  The training accuracy was found to be\n$$\nAcc_{train} = \\frac{true Positives + true Negatives}{Positives + Negatives} = 1.\n$$  The out-of-sample test also showed very reliable results:  Test samples with fog fluid 1:      Classified No-Chloro.  Classified Chloro.      Sample No-Chloro.  2500  0    Sample Chloro.  0  2500      \nAcc_{test} = 1  \nAcc_{test} = 1    \nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1  \nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1   Test samples with fog fluid 2:      Classified No-Chloro.  Classified Chloro.      Sample No-Chloro.  2500  0    Sample Chloro.  0  2463      \nAcc_{test} = 1  \nAcc_{test} = 1   The high accuracy bases on the very clear feature of the chlorophyll absorption dip in the spectrum.",
            "title": "Chlorophyll vs. Non-Chlorophyll"
        },
        {
            "location": "/demos/optical-remote-sensing/#pure-fog-fluid-1-vs-fog-fluid-2",
            "text": "While the spectra of chlorophyll containing clouds show a clear distinction feature against the non-chlorophyll\ncontaining spectra, the reflectance spectra of the two pure fog fluids show a similar relative pattern, while the\nabsolute reflectance level of the fog fluid 1 seems to be elevated compared to fog fluid 2 (see figure above).\nTherefore, a logistic regression classifier was trained for the distinction of clouds of the two pure fog fluids. The\ntraining accuracy for this classifier was found to be:\n$$\nAcc_{train} = 1\n$$  Out-of-sample performance:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  2452  48    Sample Fog Fluid 2  1  2499      \nAcc_{test} = 0.99  \nAcc_{test} = 0.99    \nF1_{test} = 0.99  \nF1_{test} = 0.99   Despite the lack of prominent characteristic features in the reflectance spectra the accuracy and the F1 score of the\nclassifier is yet rather high. That changes when the classifier is tested on reflectance data of mixtures of the\ndifferent fog fluids with chlorophyll, as shown in the table below:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  0  2500    Sample Fog Fluid 2  0  2463      \nAcc_{test} = 0.49  \nAcc_{test} = 0.49    \nF1_{test} = 0.66  \nF1_{test} = 0.66   Obviously this classifier is not robust against the mixture of features of another substance in the cloud.",
            "title": "Pure Fog Fluid 1 vs. Fog Fluid 2"
        },
        {
            "location": "/demos/optical-remote-sensing/#fog-fluid-1-vs-fog-fluid-2-with-and-without-chlorophyll",
            "text": "To overcome the weakness of the previous classifier another classifier for the distinction between different fog fluids\nwas trained including the reflectance spectra of mixtures with chlorophyll. Here, the training accuracy was found to be:\n$$\nAcc_{train} = 1\n$$  The out of sample performance for clouds of pure fog fluids is shown in the table below:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  2500  0    Sample Fog Fluid 2  0  2500      \nAcc_{test} = 1  \nAcc_{test} = 1    \nF1_{test} = 1  \nF1_{test} = 1   Interestingly, the performance on the test data of pure fog fluids is even slightly better when trained with data\nsamples including mixtures with chlorophyll. This might be explained with overfitting in the case of training only with\ndata of pure fog fluids (TODO diskutieren).  The out-of-sample performance for mixtures of the different fog fluids with chlorophyll is shown in the table below:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  2014  486    Sample Fog Fluid 2  11  2452      \nAcc_{test} = 0.90  \nAcc_{test} = 0.90    \nF1_{test} = 0.90  \nF1_{test} = 0.90   The performance is rather convincing even though the spectra of the fog fluids are overlayed with the\ncharacteristic chlorophyll spectrum.",
            "title": "Fog Fluid 1 vs. Fog Fluid 2 (with and without Chlorophyll)"
        },
        {
            "location": "/demos/optical-remote-sensing/#conclusion",
            "text": "It was shown that logistic regression can be used to identify constituents of clouds on basis of the reflectance spectra\nin the range of visible light with the here presented system, especially when strong characteristic features are present\nas in the case of chlorophyll. Besides that, even the identification of constituents with weaker characteristic features\ncan be carried through as in the case for the distinction between the two fog fluids. For the latter case, the above\nfindings suggest that the classifier for the constituent of interest should be carried through with data also regarding\na variety of probable accompanying cloud constituents. Otherwise, the overlay of the different spectra might diminish\nthe classifiers performance.      https://www.bbk.bund.de/DE/AufgabenundAusstattung/CBRNSchutz/TaskForce/ATF_einstieg1.html \u00a0 \u21a9    http://cubert-gmbh.com/uhd-185-firefly/ \u00a0 \u21a9",
            "title": "Conclusion"
        },
        {
            "location": "/demos/soh-eval/",
            "text": "Stability of hotspots\n\u00b6\n\n\n\n\nResponsible person for this section\n\n\nMarc Gassenschmidt\n\n\n\n\ncomparison of different metrics (soh, jaccard, ...)\n\n\ncomparison of different methods (gstar, focalgstar, ...)\n\n\n\n\n\n\nSetup\n\u00b6\n\n\nIn \nparameters.Setting\n the folders have to be specified first.\n\n\ninputDirectoryCSV\n:\nFolder for the CSV data. If data does not exist change to local folders.\nTo download the data in demo. Execute the main program up to the line 16. \nThis will automatically download the New York Taxi Trip Data for January/February/March 2011-2016.\n\n\n\n\nWarning\n\n\nThis dataset is approx. 30 GB\n\n\n\n\nResults are stored in directories \nouptDirectory\n and \nstatDirectory\n.\n\n\nCalculation of hotspots for geo data\n\u00b6\n\n\nExecute \ndemo.Main'\n for the calculation of hotspots and variation of the focal matrix/weight matrix and aggregation\nlevel.\n\n\nScenario(variation of parameters between parent and child): Focal/Weight/Aggregationsstufe\n\n\nResults can be found at \"outputDirectory\"/\"Scenaria\"/focal_/\"(true|false)\"d3.csv\nFor example: \"outputDirectory\"/Aggregation/focal_falsed3.csv\n\n\nF,W,Z,Down,Up\n0,1,2,0.0,0.06053811659192825\n0,1,3,0.0,0.10396039603960396\n\n\n\n\nAs well as the TIFs in folder \n2016\n.\nCan then be displayed in the following way using QGis:\n\n\n\n\nFocal G* example - single band\n\n\n\n\n\n\nCalculation of hotspots for geotemporal data\n\u00b6\n\n\nscripts. MetricValidation\n can be executed to perform the calculation for the geo-temporal space. Here the metrics\n\ngetisOrd. SoH#getMetrikResults\n will be called. An \nArray\n with different test settings can be createdUnder\n\nscripts.MetricValidation#getScenarioSettings\n.\n\n\nThe following will be computed:\n\n\n\n\nG* for 2016\n\n\nVarious Metrics\n\n\n\n\nG* for 2011-2015 including the percentage match\n\n\n\n\n\n\nFocal G* for 2016\n\n\n\n\nVarious Metrics\n\n\nFocal G* f\u00fcr 2011-2015 including the percentage match\n\n\n\n\nThe path of the metrics results is defined in the method \nimportExport.PathFormatter#getResultDirectoryAndName\n.\n\n\nFor example \n{ouptDirectory}/GIS_Daten/2016/1/Metrik/focal/a4_w5_wT1_f20_fT2result.txt\n\n\n\n\n\n\n\n\nMetric\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nSoH_Down\n\n\n0.7549668874172185\n\n\n\n\n\n\nSoH_Up\n\n\n0.04694835680751175\n\n\n\n\n\n\nneighbours\n\n\n(1,0,1,0,1,0)\n\n\n\n\n\n\njaccard\n\n\n0.5352112676056338\n\n\n\n\n\n\npercentual\n\n\n-1.0\n\n\n\n\n\n\ntime_Down\n\n\n0.45329470205147443\n\n\n\n\n\n\ntime_Up\n\n\n0.13020833333333334\n\n\n\n\n\n\nKL\n\n\n0.9963685899429499\n\n\n\n\n\n\n...\n\n\n...\n\n\n\n\n\n\n\n\nThe paths to the TIFs are defined within the method \nimportExport.PathFormatter#getDirectoryAndName\n.\n\n\n\n\nFocal G* example  (multi-band)\n\n\n{ouptDirectory}/GIS_Daten/Mulitbandtrue/2016/3/GStar/focal/a3_w6_wT2_f16_fT3_z1",
            "title": "Stability of hotspots"
        },
        {
            "location": "/demos/soh-eval/#stability-of-hotspots",
            "text": "Responsible person for this section  Marc Gassenschmidt   comparison of different metrics (soh, jaccard, ...)  comparison of different methods (gstar, focalgstar, ...)",
            "title": "Stability of hotspots"
        },
        {
            "location": "/demos/soh-eval/#setup",
            "text": "In  parameters.Setting  the folders have to be specified first.  inputDirectoryCSV :\nFolder for the CSV data. If data does not exist change to local folders.\nTo download the data in demo. Execute the main program up to the line 16. \nThis will automatically download the New York Taxi Trip Data for January/February/March 2011-2016.   Warning  This dataset is approx. 30 GB   Results are stored in directories  ouptDirectory  and  statDirectory .",
            "title": "Setup"
        },
        {
            "location": "/demos/soh-eval/#calculation-of-hotspots-for-geo-data",
            "text": "Execute  demo.Main'  for the calculation of hotspots and variation of the focal matrix/weight matrix and aggregation\nlevel.  Scenario(variation of parameters between parent and child): Focal/Weight/Aggregationsstufe  Results can be found at \"outputDirectory\"/\"Scenaria\"/focal_/\"(true|false)\"d3.csv\nFor example: \"outputDirectory\"/Aggregation/focal_falsed3.csv  F,W,Z,Down,Up\n0,1,2,0.0,0.06053811659192825\n0,1,3,0.0,0.10396039603960396  As well as the TIFs in folder  2016 .\nCan then be displayed in the following way using QGis:   Focal G* example - single band",
            "title": "Calculation of hotspots for geo data"
        },
        {
            "location": "/demos/soh-eval/#calculation-of-hotspots-for-geotemporal-data",
            "text": "scripts. MetricValidation  can be executed to perform the calculation for the geo-temporal space. Here the metrics getisOrd. SoH#getMetrikResults  will be called. An  Array  with different test settings can be createdUnder scripts.MetricValidation#getScenarioSettings .  The following will be computed:   G* for 2016  Various Metrics   G* for 2011-2015 including the percentage match    Focal G* for 2016   Various Metrics  Focal G* f\u00fcr 2011-2015 including the percentage match   The path of the metrics results is defined in the method  importExport.PathFormatter#getResultDirectoryAndName .  For example  {ouptDirectory}/GIS_Daten/2016/1/Metrik/focal/a4_w5_wT1_f20_fT2result.txt     Metric  Value      SoH_Down  0.7549668874172185    SoH_Up  0.04694835680751175    neighbours  (1,0,1,0,1,0)    jaccard  0.5352112676056338    percentual  -1.0    time_Down  0.45329470205147443    time_Up  0.13020833333333334    KL  0.9963685899429499    ...  ...     The paths to the TIFs are defined within the method  importExport.PathFormatter#getDirectoryAndName .   Focal G* example  (multi-band)  {ouptDirectory}/GIS_Daten/Mulitbandtrue/2016/3/GStar/focal/a3_w6_wT2_f16_fT3_z1",
            "title": "Calculation of hotspots for geotemporal data"
        },
        {
            "location": "/demos/traffic-incidents/",
            "text": "Traffic Incidents\n\u00b6\n\n\nMotivation\n\u00b6\n\n\nThe visual analysis of traffic incidents is of high interest in the BOS-scenario. The real time gathering of traffic\ndata and effective ways of visualizing aim to support emergency services in reacting to incidents or when searching for\nhot spots. Furthermore, the analysis of traffic and mobility data is of high interest for urban planning as it, for\nexample, allows the planning of future infrastructure.\n\n\nVisual Analysis of Traffic Incidents\n\u00b6\n\n\nOur visual analysis allows to gain overview as well as detailed representations of the underlaying data. At first\nglance, a calendar visualization of traffic incidents (Figure 1) allows to get an impression about the temporal\ndistribution of the incidents. Figure 1, for example, allows the user to easily detect a repeating pattern in July 2016\nwhere each monday for three weeks in a row was  strongly noticeable.\n\n\n\n\nFigure 1: Calendar visualization of traffic incidents.\n\n\n\n\n\n\nAdditionally, we enable domain experts to inspect the categorical distribution of gathered traffic incidents. Domain\nexperts can interactively decide for timespans of interest (e.g., only taking incidents into account which occur at\nnight (Figure 2) or at afternoon (Figure 3)).\n\n\n\n\nFigure 2: Categorical distribution of gathered traffic incidents occuring at night.\n\n\n\n\n\n\n\n\nFigure 3: Categorical distribution of gathered traffic incidents occuring at afternoon.\n\n\n\n\n\n\nUltimately, it is of high importance to provide adequate spatial visaulizations in order to allow domain experts to\nexplore the data. We provide a more detailed animated heat map visualization on the map (see the video) as well as more\nabstract graph representation. The graph has been developed in order to detect patterns at larger scale. The graph is\nbuilt hierarchically. On the top level, a spatial clustering is used to detect areas of interest. Afterwards, we split\nthe clusters in the graph visualization based on their incident type by using color. On the next level, we indicate the\nseverity by using saturation.\n\n\n\n\nFigure 4: Several kinds of visual overviews to effectively aggregate the available data.\n\n\n\n\n\n\nVideo\n\u00b6\n\n\nA video demonstrating some of the capabilities of the analysis of traffic incidents can be found by clicking below or\n\nhere\n.",
            "title": "Traffic Incidents"
        },
        {
            "location": "/demos/traffic-incidents/#traffic-incidents",
            "text": "",
            "title": "Traffic Incidents"
        },
        {
            "location": "/demos/traffic-incidents/#motivation",
            "text": "The visual analysis of traffic incidents is of high interest in the BOS-scenario. The real time gathering of traffic\ndata and effective ways of visualizing aim to support emergency services in reacting to incidents or when searching for\nhot spots. Furthermore, the analysis of traffic and mobility data is of high interest for urban planning as it, for\nexample, allows the planning of future infrastructure.",
            "title": "Motivation"
        },
        {
            "location": "/demos/traffic-incidents/#visual-analysis-of-traffic-incidents",
            "text": "Our visual analysis allows to gain overview as well as detailed representations of the underlaying data. At first\nglance, a calendar visualization of traffic incidents (Figure 1) allows to get an impression about the temporal\ndistribution of the incidents. Figure 1, for example, allows the user to easily detect a repeating pattern in July 2016\nwhere each monday for three weeks in a row was  strongly noticeable.   Figure 1: Calendar visualization of traffic incidents.    Additionally, we enable domain experts to inspect the categorical distribution of gathered traffic incidents. Domain\nexperts can interactively decide for timespans of interest (e.g., only taking incidents into account which occur at\nnight (Figure 2) or at afternoon (Figure 3)).   Figure 2: Categorical distribution of gathered traffic incidents occuring at night.     Figure 3: Categorical distribution of gathered traffic incidents occuring at afternoon.    Ultimately, it is of high importance to provide adequate spatial visaulizations in order to allow domain experts to\nexplore the data. We provide a more detailed animated heat map visualization on the map (see the video) as well as more\nabstract graph representation. The graph has been developed in order to detect patterns at larger scale. The graph is\nbuilt hierarchically. On the top level, a spatial clustering is used to detect areas of interest. Afterwards, we split\nthe clusters in the graph visualization based on their incident type by using color. On the next level, we indicate the\nseverity by using saturation.   Figure 4: Several kinds of visual overviews to effectively aggregate the available data.",
            "title": "Visual Analysis of Traffic Incidents"
        },
        {
            "location": "/demos/traffic-incidents/#video",
            "text": "A video demonstrating some of the capabilities of the analysis of traffic incidents can be found by clicking below or here .",
            "title": "Video"
        },
        {
            "location": "/demos/uhi-visual-analysis/",
            "text": "Visual Analysis of Spatio-Temporal Event Predictions\n\u00b6\n\n\nThe well-known phenomenon of the Urban Heat Island effect in city areas is an important issue as a result from the\nongoing urbanization and industrialization process. Analyzing the causes for increased temperatures in urban areas is a\ncomplex task that depends on several variables like energy management, surface characteristics, weather, vegetation\nindex, population, industrial territory, transportation, air pollution and others. City and country planners are in need\nfor new technology that supports them during decision-making processes by meaningful visualization and data analysis\ntechniques to improve urban climate and the efficiency of energy management. We propose a visual analytics approach to\nenable domain experts to visually explore current temperature conditions of city areas and interactive steering\ntechniques on characteristic features to discover the influence of available variables on the outcome. The presented\nprediction models are intended to provide insights about future conditions to elicit the effect of various variables on\nthe intensity of urban heat islands.\n\n\nUsed Data Sources\n\u00b6\n\n\nA variety of data source is used for our visual analysis of urban heat islands such as\n\n\n\n\nWunderground\n\n\nATKIS/ALKIS\n\n\nDWD\n\n\n...\n\n\n\n\nWe incorporoated data sets for Germany, however, focused on the analysis of regions around Karlsruhe. \n\n\n\n\nPicture 1: WU Station distribution in and around Karlsruhe\n\n\n\n\n\n\nVisual Analysis of Urban Heat Islands\n\u00b6\n\n\nOur system allows the prediction of Urban Heat Islands with Machine Learning. Furthermore, we provide an interactive\nsystem for the visual analysis of the occurences of urban heat islands. Interactive parameter steering and similarity\nsearch allows to investigate impacts of different variables. \n\n\n\n\nPicture 2: Similarity search for urban heat islands, starting from Karlsruhe.\n\n\n\n\n\n\n\n\nPicture 3: Identified urban heat islands with high similarity for a station in Karlsruhe.\n\n\n\nThe identified similar weather station is positioned in Duisburg. The user is enabled to further explore the dataset\nby brushing and linking.\n\n\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nSmart City",
            "title": "Visual Analysis of Spatio-Temporal Event Predictions"
        },
        {
            "location": "/demos/uhi-visual-analysis/#visual-analysis-of-spatio-temporal-event-predictions",
            "text": "The well-known phenomenon of the Urban Heat Island effect in city areas is an important issue as a result from the\nongoing urbanization and industrialization process. Analyzing the causes for increased temperatures in urban areas is a\ncomplex task that depends on several variables like energy management, surface characteristics, weather, vegetation\nindex, population, industrial territory, transportation, air pollution and others. City and country planners are in need\nfor new technology that supports them during decision-making processes by meaningful visualization and data analysis\ntechniques to improve urban climate and the efficiency of energy management. We propose a visual analytics approach to\nenable domain experts to visually explore current temperature conditions of city areas and interactive steering\ntechniques on characteristic features to discover the influence of available variables on the outcome. The presented\nprediction models are intended to provide insights about future conditions to elicit the effect of various variables on\nthe intensity of urban heat islands.",
            "title": "Visual Analysis of Spatio-Temporal Event Predictions"
        },
        {
            "location": "/demos/uhi-visual-analysis/#used-data-sources",
            "text": "A variety of data source is used for our visual analysis of urban heat islands such as   Wunderground  ATKIS/ALKIS  DWD  ...   We incorporoated data sets for Germany, however, focused on the analysis of regions around Karlsruhe.    Picture 1: WU Station distribution in and around Karlsruhe",
            "title": "Used Data Sources"
        },
        {
            "location": "/demos/uhi-visual-analysis/#visual-analysis-of-urban-heat-islands",
            "text": "Our system allows the prediction of Urban Heat Islands with Machine Learning. Furthermore, we provide an interactive\nsystem for the visual analysis of the occurences of urban heat islands. Interactive parameter steering and similarity\nsearch allows to investigate impacts of different variables.    Picture 2: Similarity search for urban heat islands, starting from Karlsruhe.     Picture 3: Identified urban heat islands with high similarity for a station in Karlsruhe.  \nThe identified similar weather station is positioned in Duisburg. The user is enabled to further explore the dataset\nby brushing and linking.",
            "title": "Visual Analysis of Urban Heat Islands"
        },
        {
            "location": "/demos/uhi-visual-analysis/#related-scenarios",
            "text": "Smart City",
            "title": "Related Scenarios"
        },
        {
            "location": "/methods/",
            "text": "About Method\n\u00b6\n\n\nThis section contains a \nlist of methods\n that serve as a \ntheoretical background\n\nfor the other sections inside the documentation, especially for the \nDemos\n.\n\n\nSome methods depend on each other which is reflected in their ordering.\n\n\nMethods to be included later\n\u00b6\n\n\n\n\nExasol: spatial indices\n\n\nExasol: virtual schemas\n\n\nExasol: language bindings (e.g. R)",
            "title": "About Method"
        },
        {
            "location": "/methods/#about-method",
            "text": "This section contains a  list of methods  that serve as a  theoretical background \nfor the other sections inside the documentation, especially for the  Demos .  Some methods depend on each other which is reflected in their ordering.",
            "title": "About Method"
        },
        {
            "location": "/methods/#methods-to-be-included-later",
            "text": "Exasol: spatial indices  Exasol: virtual schemas  Exasol: language bindings (e.g. R)",
            "title": "Methods to be included later"
        },
        {
            "location": "/methods/focal_getis_ord/",
            "text": "Focal Getis-ord\n\u00b6\n\n\n\n\nTodo\n\n\nJulian Bruns : definition of Focal G* using points \n\n\n\n\nThe Focal Getis-Ord \nG^*_i\nG^*_i\n statistic differs from the \nStandard Getis-ord\n ...\n\n\n\n\n\n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\n\n\n\n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\n\n\n\n\nwhere:\n\n\n\n\nTODO\n\n\nTODO",
            "title": "Focal Getis-ord"
        },
        {
            "location": "/methods/focal_getis_ord/#focal-getis-ord",
            "text": "Todo  Julian Bruns : definition of Focal G* using points    The Focal Getis-Ord  G^*_i G^*_i  statistic differs from the  Standard Getis-ord  ...   \n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }  \n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }   where:   TODO  TODO",
            "title": "Focal Getis-ord"
        },
        {
            "location": "/methods/focal_getis_ord_raster/",
            "text": "Foal Getis-ord on rasters\n\u00b6\n\n\nThe rasterized Focal Getis-Ord formula looks as follows:\n\n\n\n\n\nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\n\n\n\nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\n\n\n\n\nwhere:\n\n\n\n\nR\nR\n is the input raster.\n\n\nW\nW\n is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g. \n5 \\times 5\n5 \\times 5\n, \n31 \\times 31\n31 \\times 31\n ...\n\n\nN\nN\n represents the focal count of pixels TODO (there can be NA values)\n\n\nM\nM\n represents the focal mean TODO.\n\n\nS\nS\n represents the focal standard deviation TODO.\n\n\n\n\nIt can be seen that the formula can be nicely refactored into:\n\n\n\n\nTODO",
            "title": "Foal Getis-ord on rasters"
        },
        {
            "location": "/methods/focal_getis_ord_raster/#foal-getis-ord-on-rasters",
            "text": "The rasterized Focal Getis-Ord formula looks as follows:   \nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }  \nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }   where:   R R  is the input raster.  W W  is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g.  5 \\times 5 5 \\times 5 ,  31 \\times 31 31 \\times 31  ...  N N  represents the focal count of pixels TODO (there can be NA values)  M M  represents the focal mean TODO.  S S  represents the focal standard deviation TODO.   It can be seen that the formula can be nicely refactored into:   TODO",
            "title": "Foal Getis-ord on rasters"
        },
        {
            "location": "/methods/getis_ord/",
            "text": "Standards Getis-Ord G*\n\u00b6\n\n\nThe standard definition of Getis-Ord \nG^*_i\nG^*_i\n statistic assumes a study area with \nn\nn\n points with measurements\n\nX = [x_1, \\ldots, x_n]\nX = [x_1, \\ldots, x_n]\n. Moreover, it assumes weights \nw_{i,j}\nw_{i,j}\n to be defined between all pairs of points \ni\ni\n\nand \nj\nj\n (for all \ni,j \\in \\{ 1, \\ldots, n\\}\ni,j \\in \\{ 1, \\ldots, n\\}\n). The formula to compute \nG^*_i\nG^*_i\n at a given point \ni\ni\n is then:\n\n\n\n\n\n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\n\n\n\n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\n\n\n\n\nwhere:\n\n\n\n\n\\bar{X}\n\\bar{X}\n is the mean of all measurements,\n\n\nS\nS\n is the standard deviation of all measurements.\n\n\n\n\nAs it is known, this statistic creates a z-score, which denotes the significance of an area in relation\nto its surrounding areas.\n\n\n\n\nNote\n\n\nFor \nx \\in X\nx \\in X\n, the \nzscore(x) = \\frac{x - mean(X)}{stdev(X)}\nzscore(x) = \\frac{x - mean(X)}{stdev(X)}\n\n\n\n\n\n\nTodo\n\n\nJulian Bruns: Add references to papers",
            "title": "Standards Getis-Ord G*"
        },
        {
            "location": "/methods/getis_ord/#standards-getis-ord-g",
            "text": "The standard definition of Getis-Ord  G^*_i G^*_i  statistic assumes a study area with  n n  points with measurements X = [x_1, \\ldots, x_n] X = [x_1, \\ldots, x_n] . Moreover, it assumes weights  w_{i,j} w_{i,j}  to be defined between all pairs of points  i i \nand  j j  (for all  i,j \\in \\{ 1, \\ldots, n\\} i,j \\in \\{ 1, \\ldots, n\\} ). The formula to compute  G^*_i G^*_i  at a given point  i i  is then:   \n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }  \n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }   where:   \\bar{X} \\bar{X}  is the mean of all measurements,  S S  is the standard deviation of all measurements.   As it is known, this statistic creates a z-score, which denotes the significance of an area in relation\nto its surrounding areas.   Note  For  x \\in X x \\in X , the  zscore(x) = \\frac{x - mean(X)}{stdev(X)} zscore(x) = \\frac{x - mean(X)}{stdev(X)}    Todo  Julian Bruns: Add references to papers",
            "title": "Standards Getis-Ord G*"
        },
        {
            "location": "/methods/getis_ord_raster/",
            "text": "Getis-ord G* on rasters\n\u00b6\n\n\nThe \nStandard Getis-ord\n is defined on individual points (vector data).\nIn many situations, we want to compute \nG^*_i\nG^*_i\n in a raster rather than in a point cloud.\nThis way, the computation can be expressed using map algebra operations and nicely parallelized in frameworks \nsuch as \nGeotrellis\n.\n\n\nThe rasterized Getis-Ord formula looks as follows:\n\n\n\n\n\nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\n\n\n\nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\n\n\n\n\nwhere:\n\n\n\n\nR\nR\n is the input raster.\n\n\nW\nW\n is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g. \n5 \\times 5\n5 \\times 5\n, \n31 \\times 31\n31 \\times 31\n ...\n\n\nN\nN\n represents the number of all pixels in \nR\nR\n (because there can be NA values)\n\n\nM\nM\n represents the global mean of \nR\nR\n.\n\n\nS\nS\n represents the global standard deviation of all pixels in \nR\nR\n.\n\n\n\n\nIt can be seen that the formula can be nicely refactored into:\n\n\n\n\nOne \nglobal operation\n that computes \nN\nN\n, \nM\nM\n, \nS\nS\n. These values are usually available because they\n  were pre-computed when the raster layer has been ingested into the catalog.\n\n\nOne \nfocal operation\n \nR{\\stackrel{\\mathtt{sum}}{\\circ}}W\nR{\\stackrel{\\mathtt{sum}}{\\circ}}W\n - the convolution of raster \nR\nR\n with\n  the weight matrix \nW\nW\n.\n\n\nOne \nlocal operation\n that puts all components toghether for each pixel in \nR\nR\n.",
            "title": "Getis-ord G* on rasters"
        },
        {
            "location": "/methods/getis_ord_raster/#getis-ord-g-on-rasters",
            "text": "The  Standard Getis-ord  is defined on individual points (vector data).\nIn many situations, we want to compute  G^*_i G^*_i  in a raster rather than in a point cloud.\nThis way, the computation can be expressed using map algebra operations and nicely parallelized in frameworks \nsuch as  Geotrellis .  The rasterized Getis-Ord formula looks as follows:   \nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }  \nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }   where:   R R  is the input raster.  W W  is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g.  5 \\times 5 5 \\times 5 ,  31 \\times 31 31 \\times 31  ...  N N  represents the number of all pixels in  R R  (because there can be NA values)  M M  represents the global mean of  R R .  S S  represents the global standard deviation of all pixels in  R R .   It can be seen that the formula can be nicely refactored into:   One  global operation  that computes  N N ,  M M ,  S S . These values are usually available because they\n  were pre-computed when the raster layer has been ingested into the catalog.  One  focal operation   R{\\stackrel{\\mathtt{sum}}{\\circ}}W R{\\stackrel{\\mathtt{sum}}{\\circ}}W  - the convolution of raster  R R  with\n  the weight matrix  W W .  One  local operation  that puts all components toghether for each pixel in  R R .",
            "title": "Getis-ord G* on rasters"
        },
        {
            "location": "/methods/one_vs_rest/",
            "text": "One vs. Rest\n\u00b6\n\n\nThe One-versus-Rest (or One-versus-All = OvA) Strategy is a Method to use Binary (Two-Class) Classifiers\n(such as \nSupport Vector Machines\n ) for classifying multiple\n(more than two) Classes.\n\n\n\n\nNote\n\n\n\n\nhttps://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest\n\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#one-vs-rest-classifier-aka-one-vs-all\n\n\n\n\n\n\nOne-vs.-rest\n\u00b6\n\n\nInputs\n\u00b6\n\n\n\n\nL \\text{ , a learner (training algorithm for binary classifiers) }\nL \\text{ , a learner (training algorithm for binary classifiers) }\n\n\n\\text{ samples } \\vec{X}\n\\text{ samples } \\vec{X}\n\n\n\\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i\n\\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i\n\n\n\n\nOutput\n\u00b6\n\n\n\n\n\\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K}\n\\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K}\n\n\n\n\nProcedure\n\u00b6\n\n\n\\text{ For each } k \\text{ in } {1,\\ldots,K}\n\\text{ For each } k \\text{ in } {1,\\ldots,K}\n\n\n\n\n\n\\begin{align}\n&\\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1, & \\text{ if } y_i = k \\\\\nz_i = 0, & \\text{ otherwise }\n\\end{cases}\\\\\n&\\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\\n\n\n\n\n\\begin{align}\n&\\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1, & \\text{ if } y_i = k \\\\\nz_i = 0, & \\text{ otherwise }\n\\end{cases}\\\\\n&\\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\\n\n\n\n\n\nMaking decisions means applying all classifiers to an unseen sample and\npredicting the label for which the corresponding classifier reports the\nhighest confidence score:\n\n\n\n\n\n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)\n\n\n\n\n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)\n\n\n\n\n\n\n\nTodo\n\n\nAdrian Klink: Add references, optimize description",
            "title": "One vs. Rest"
        },
        {
            "location": "/methods/one_vs_rest/#one-vs-rest",
            "text": "The One-versus-Rest (or One-versus-All = OvA) Strategy is a Method to use Binary (Two-Class) Classifiers\n(such as  Support Vector Machines  ) for classifying multiple\n(more than two) Classes.   Note   https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest  https://spark.apache.org/docs/latest/ml-classification-regression.html#one-vs-rest-classifier-aka-one-vs-all",
            "title": "One vs. Rest"
        },
        {
            "location": "/methods/one_vs_rest/#one-vs-rest_1",
            "text": "",
            "title": "One-vs.-rest"
        },
        {
            "location": "/methods/one_vs_rest/#inputs",
            "text": "L \\text{ , a learner (training algorithm for binary classifiers) } L \\text{ , a learner (training algorithm for binary classifiers) }  \\text{ samples } \\vec{X} \\text{ samples } \\vec{X}  \\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i \\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i",
            "title": "Inputs"
        },
        {
            "location": "/methods/one_vs_rest/#output",
            "text": "\\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K} \\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K}",
            "title": "Output"
        },
        {
            "location": "/methods/one_vs_rest/#procedure",
            "text": "\\text{ For each } k \\text{ in } {1,\\ldots,K} \\text{ For each } k \\text{ in } {1,\\ldots,K}   \n\\begin{align}\n&\\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1, & \\text{ if } y_i = k \\\\\nz_i = 0, & \\text{ otherwise }\n\\end{cases}\\\\\n&\\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\  \n\\begin{align}\n&\\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1, & \\text{ if } y_i = k \\\\\nz_i = 0, & \\text{ otherwise }\n\\end{cases}\\\\\n&\\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\   Making decisions means applying all classifiers to an unseen sample and\npredicting the label for which the corresponding classifier reports the\nhighest confidence score:   \n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)  \n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)    Todo  Adrian Klink: Add references, optimize description",
            "title": "Procedure"
        },
        {
            "location": "/methods/soh/",
            "text": "Stability of hotspots\n\u00b6\n\n\n\n\nTodo\n\n\nIntro text\n\n\n\n\nRelated demos:\n- \nstability of hotspots",
            "title": "Stability of hotspots"
        },
        {
            "location": "/methods/soh/#stability-of-hotspots",
            "text": "Todo  Intro text   Related demos:\n-  stability of hotspots",
            "title": "Stability of hotspots"
        },
        {
            "location": "/methods/spectral_analysis_tci/",
            "text": "Triangular Chlorophyll Index (TCI) in Spectral Remote Sensing\n\u00b6\n\n\nThe Triangular Chlorophyll Index (TCI) is widely used in remote sensing in the field of agricultural studies.\nA typical use case is e.g. the quantification of vegetation in an area for the purpose of land-use classification.\n\n\nThe TCI bases on the absorption maximum and thus a minimum of reflectance of chlorophyll at a wavelength of roughly\n670 nm. The stronger the minimum of reflectance is expressed in the spectrum under survey, the higher the TCI value.\n\n\nThe TCI is calculated according to the following formula\n1\n:\n\n\n\n\n\n  TCI =\n    1.2 \\cdot {\n      \\left( {R_{700nm}-R_{550nm}} \\right)\n    }\n    -1.5 \\cdot {\n      \\left( {R_{670nm}-R_{550nm}} \\right)\n    }\n    \\cdot {\n      \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}\n    }\n\n\n\n\n  TCI =\n    1.2 \\cdot {\n      \\left( {R_{700nm}-R_{550nm}} \\right)\n    }\n    -1.5 \\cdot {\n      \\left( {R_{670nm}-R_{550nm}} \\right)\n    }\n    \\cdot {\n      \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}\n    }\n\n\n\n\n\nHere, \nR_{550nm}\nR_{550nm}\n, \nR_{670nm}\nR_{670nm}\n and \nR_{700nm}\nR_{700nm}\n denote the reflectance for the wavelengths 550nm, 670nm and 700nm,\nrespectively. In order to deduce the reflectance values from the recorded reflected intensities a proper white\nbalance has to be provided.\n\n\n\n\n\n\n\n\n\n\nhttps://www.indexdatabase.de/db/i-single.php?id=392\n\u00a0\n\u21a9",
            "title": "Triangular Chlorophyll Index (TCI) in Spectral Remote Sensing"
        },
        {
            "location": "/methods/spectral_analysis_tci/#triangular-chlorophyll-index-tci-in-spectral-remote-sensing",
            "text": "The Triangular Chlorophyll Index (TCI) is widely used in remote sensing in the field of agricultural studies.\nA typical use case is e.g. the quantification of vegetation in an area for the purpose of land-use classification.  The TCI bases on the absorption maximum and thus a minimum of reflectance of chlorophyll at a wavelength of roughly\n670 nm. The stronger the minimum of reflectance is expressed in the spectrum under survey, the higher the TCI value.  The TCI is calculated according to the following formula 1 :   \n  TCI =\n    1.2 \\cdot {\n      \\left( {R_{700nm}-R_{550nm}} \\right)\n    }\n    -1.5 \\cdot {\n      \\left( {R_{670nm}-R_{550nm}} \\right)\n    }\n    \\cdot {\n      \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}\n    }  \n  TCI =\n    1.2 \\cdot {\n      \\left( {R_{700nm}-R_{550nm}} \\right)\n    }\n    -1.5 \\cdot {\n      \\left( {R_{670nm}-R_{550nm}} \\right)\n    }\n    \\cdot {\n      \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}\n    }   Here,  R_{550nm} R_{550nm} ,  R_{670nm} R_{670nm}  and  R_{700nm} R_{700nm}  denote the reflectance for the wavelengths 550nm, 670nm and 700nm,\nrespectively. In order to deduce the reflectance values from the recorded reflected intensities a proper white\nbalance has to be provided.      https://www.indexdatabase.de/db/i-single.php?id=392 \u00a0 \u21a9",
            "title": "Triangular Chlorophyll Index (TCI) in Spectral Remote Sensing"
        },
        {
            "location": "/methods/support_vector_machine/",
            "text": "Support Vector Machine\n\u00b6\n\n\nA Support Vector Machine (SVM) is a supervised learning model (machine learning) which can be used to classify image data. \nFor Parallelization we use a Linear SVM from \nApache Spark\n.\nSince we have more than two classes the One versus All (or \nOne vs. Rest\n ) Strategy is used.\n\n\n\n\nNote\n\n\n\n\nhttps://en.wikipedia.org/wiki/Support_vector_machine\n\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine\n\n\n\n\n\n\nLinear SVM\n\u00b6\n\n\nWe are given a training dataset of \nn\nn\n points of the form\n\n\n\n\n\n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)\n\n\n\n\n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)\n\n\n\n\n\nwhere the \ny_i\ny_i\n are either 1 or \u22121, each indicating the class to which\nthe point \n\\vec{x}_i\n\\vec{x}_i\n belongs. Each \n\\vec{x}_i\n\\vec{x}_i\n is a \np\np\n-dimensional\n\nreal\n vector. We want to find the\n\"maximum-margin hyperplane\" that divides the group of points\n\n\\vec{x}_i\n\\vec{x}_i\n for which \ny_i=1\ny_i=1\n from the group of points for which\n\ny_i=-1\ny_i=-1\n, which is defined so that the distance between the hyperplane\nand the nearest point \n\\vec{x}_i\n\\vec{x}_i\n from either group is maximized.\n\n\nAny \nhyperplane\n can be written as the set of\npoints \n\\vec{x}\n\\vec{x}\n satisfying\n\n\n\n\n\n  \\vec{w}\\cdot\\vec{x} - b=0\n\n\n\n\n  \\vec{w}\\cdot\\vec{x} - b=0\n\n\n\n\n\nwhere \n{\\vec{w}}\n{\\vec{w}}\n is the (not necessarily normalized) \nnormal\nvector\n to the hyperplane. This is much\nlike \nHesse normal form\n, except that\n\n{\\vec{w}}\n{\\vec{w}}\n is not necessarily a unit vector. The parameter\n\n\\tfrac{b}{\\|\\vec{w}\\|}\n\\tfrac{b}{\\|\\vec{w}\\|}\n determines the offset of the hyperplane from\nthe origin along the normal vector \n{\\vec{w}}\n{\\vec{w}}\n.\n\n\n\n\n\n\nFigure:\n\nMaximum-margin hyperplane and margins for an SVM trained with samples from two classes.\nSamples on the margin are called the support vectors.\n\n\n\n\nHard-margin\n\u00b6\n\n\nIf the training data are \nlinearly\nseparable\n, we can select two parallel\nhyperplanes that separate the two classes of data, so that the distance\nbetween them is as large as possible. The region bounded by these two\nhyperplanes is called the \\\"margin\\\", and the maximum-margin hyperplane\nis the hyperplane that lies halfway between them. These hyperplanes can\nbe described by the equations\n$$\n  \\vec{w}\\cdot\\vec{x} - b=1\n$$\n\n\nand\n\n\n\n\n\n  \\vec{w}\\cdot\\vec{x} - b=-1\n\n\n\n\n  \\vec{w}\\cdot\\vec{x} - b=-1\n\n\n\n\n\nGeometrically, the distance between these two hyperplanes is\n\n\\tfrac{2}{\\|\\vec{w}\\|}\n\\tfrac{2}{\\|\\vec{w}\\|}\n, so to maximize the distance between the planes\nwe want to minimize \n\\|\\vec{w}\\|\n\\|\\vec{w}\\|\n. As we also have to prevent data\npoints from falling into the margin, we add the following constraint:\nfor each \ni\ni\n either\n\n\n\n\n\n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1\n\n\n\n\n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1\n\n\n\n\n\nor\n\n\n\n\n\n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1\n\n\n\n\n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1\n\n\n\n\n\nThese constraints state that each data point must lie on the correct\nside of the margin.\n\n\nThis can be rewritten as:\n\n\n\n\n\n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)\n\n\n\n\n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)\n\n\n\n\n\nWe can put this together to get the optimization problem:\n\n\n\n\n\n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n\n\n\n\n\n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n\n\n\n\n\n\nThe \n\\vec w\n\\vec w\n and \nb\nb\n that solve this problem determine our classifier,\n$$\n  \\vec{x} \\mapsto sgn(\\vec{w} \\cdot \\vec{x} - b)\n$$\n\n\nAn easy-to-see but important consequence of this geometric description\nis that the max-margin hyperplane is completely determined by those\n\n\\vec{x}_i\n\\vec{x}_i\n which lie nearest to it. These \n\\vec{x}_i\n\\vec{x}_i\n are called\n\nsupport vectors.\n\n\nSoft-margin\n\u00b6\n\n\nTo extend SVM to cases in which the data are not linearly separable, we\nintroduce the \nhinge loss\n function:\n\n\n\n\n\n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)\n\n\n\n\n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)\n\n\n\n\n\nThis function is zero if the constraint in (1) is satisfied, in other\nwords, if \n\\vec{x}_i\n\\vec{x}_i\n lies on the correct side of the margin. For data\non the wrong side of the margin, the function\\'s value is proportional\nto the distance from the margin.\n\n\nWe then wish to minimize\n\n\n\n\n\n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,\n\n\n\n\n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,\n\n\n\n\n\nwhere the parameter \n\\lambda\n\\lambda\n determines the tradeoff between increasing\nthe margin-size and ensuring that the \n\\vec{x}_i\n\\vec{x}_i\n lie on the correct\nside of the margin. Thus, for sufficiently small values of \n\\lambda\n\\lambda\n,\nthe soft-margin SVM will behave identically to the hard-margin SVM if\nthe input data are linearly classifiable, but will still learn if a\nclassification rule is viable or not.\n\n\n\n\nTodo\n\n\nAdrian Klink: Add references, optimize description",
            "title": "Support Vector Machine"
        },
        {
            "location": "/methods/support_vector_machine/#support-vector-machine",
            "text": "A Support Vector Machine (SVM) is a supervised learning model (machine learning) which can be used to classify image data. \nFor Parallelization we use a Linear SVM from  Apache Spark .\nSince we have more than two classes the One versus All (or  One vs. Rest  ) Strategy is used.   Note   https://en.wikipedia.org/wiki/Support_vector_machine  https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine",
            "title": "Support Vector Machine"
        },
        {
            "location": "/methods/support_vector_machine/#linear-svm",
            "text": "We are given a training dataset of  n n  points of the form   \n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)  \n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)   where the  y_i y_i  are either 1 or \u22121, each indicating the class to which\nthe point  \\vec{x}_i \\vec{x}_i  belongs. Each  \\vec{x}_i \\vec{x}_i  is a  p p -dimensional real  vector. We want to find the\n\"maximum-margin hyperplane\" that divides the group of points \\vec{x}_i \\vec{x}_i  for which  y_i=1 y_i=1  from the group of points for which y_i=-1 y_i=-1 , which is defined so that the distance between the hyperplane\nand the nearest point  \\vec{x}_i \\vec{x}_i  from either group is maximized.  Any  hyperplane  can be written as the set of\npoints  \\vec{x} \\vec{x}  satisfying   \n  \\vec{w}\\cdot\\vec{x} - b=0  \n  \\vec{w}\\cdot\\vec{x} - b=0   where  {\\vec{w}} {\\vec{w}}  is the (not necessarily normalized)  normal\nvector  to the hyperplane. This is much\nlike  Hesse normal form , except that {\\vec{w}} {\\vec{w}}  is not necessarily a unit vector. The parameter \\tfrac{b}{\\|\\vec{w}\\|} \\tfrac{b}{\\|\\vec{w}\\|}  determines the offset of the hyperplane from\nthe origin along the normal vector  {\\vec{w}} {\\vec{w}} .    Figure: \nMaximum-margin hyperplane and margins for an SVM trained with samples from two classes.\nSamples on the margin are called the support vectors.",
            "title": "Linear SVM"
        },
        {
            "location": "/methods/support_vector_machine/#hard-margin",
            "text": "If the training data are  linearly\nseparable , we can select two parallel\nhyperplanes that separate the two classes of data, so that the distance\nbetween them is as large as possible. The region bounded by these two\nhyperplanes is called the \\\"margin\\\", and the maximum-margin hyperplane\nis the hyperplane that lies halfway between them. These hyperplanes can\nbe described by the equations\n$$\n  \\vec{w}\\cdot\\vec{x} - b=1\n$$  and   \n  \\vec{w}\\cdot\\vec{x} - b=-1  \n  \\vec{w}\\cdot\\vec{x} - b=-1   Geometrically, the distance between these two hyperplanes is \\tfrac{2}{\\|\\vec{w}\\|} \\tfrac{2}{\\|\\vec{w}\\|} , so to maximize the distance between the planes\nwe want to minimize  \\|\\vec{w}\\| \\|\\vec{w}\\| . As we also have to prevent data\npoints from falling into the margin, we add the following constraint:\nfor each  i i  either   \n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1  \n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1   or   \n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1  \n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1   These constraints state that each data point must lie on the correct\nside of the margin.  This can be rewritten as:   \n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)  \n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)   We can put this together to get the optimization problem:   \n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n  \n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n   The  \\vec w \\vec w  and  b b  that solve this problem determine our classifier,\n$$\n  \\vec{x} \\mapsto sgn(\\vec{w} \\cdot \\vec{x} - b)\n$$  An easy-to-see but important consequence of this geometric description\nis that the max-margin hyperplane is completely determined by those \\vec{x}_i \\vec{x}_i  which lie nearest to it. These  \\vec{x}_i \\vec{x}_i  are called support vectors.",
            "title": "Hard-margin"
        },
        {
            "location": "/methods/support_vector_machine/#soft-margin",
            "text": "To extend SVM to cases in which the data are not linearly separable, we\nintroduce the  hinge loss  function:   \n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)  \n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)   This function is zero if the constraint in (1) is satisfied, in other\nwords, if  \\vec{x}_i \\vec{x}_i  lies on the correct side of the margin. For data\non the wrong side of the margin, the function\\'s value is proportional\nto the distance from the margin.  We then wish to minimize   \n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,  \n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,   where the parameter  \\lambda \\lambda  determines the tradeoff between increasing\nthe margin-size and ensuring that the  \\vec{x}_i \\vec{x}_i  lie on the correct\nside of the margin. Thus, for sufficiently small values of  \\lambda \\lambda ,\nthe soft-margin SVM will behave identically to the hard-margin SVM if\nthe input data are linearly classifiable, but will still learn if a\nclassification rule is viable or not.   Todo  Adrian Klink: Add references, optimize description",
            "title": "Soft-margin"
        },
        {
            "location": "/scenarios/smartcity/",
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nSmart City\n\u00b6",
            "title": "Smart City"
        },
        {
            "location": "/scenarios/smartcity/#smart-city",
            "text": "",
            "title": "Smart City"
        },
        {
            "location": "/scenarios/disaster/",
            "text": "Responsible person for this section\n\n\n\n\nAlexander Groeschel\n\n\nBodo Bernsdorf\n\n\n\n\n\n\nDisaster Management\n\u00b6\n\n\n\n\nGeneral\n\u00b6\n\n\nDisasters and accidents always happen somewhere. Therefore, it is necessary to use geodata even in the case of small\naccident scenarios in order to assess access routes and their impact on the environment. Information support for the\nsafety authorities is necessary in many cases.\n\n\nSome examples\n\u00b6\n\n\nComplex traffic accidents require indications of conceivable access routes for the rescue services and, in particular,\nindications of departure routes for the rescue service in order to be able to supply patients quickly.\n\n\nLocal storms lead to blocked or flooded roads and destroyed infrastructure, which is why rescue forces must be informed\nabout the main areas of operation, access routes and safe areas for the potential accommodation of evacuees.\n\n\nLarge fires - for example in a recycling plant - produce highly contaminated flue gases which, depending on the weather\nconditions, remain on site (e. g. inversion weather conditions) or drift and diffuse. In addition to the question of the\nappropriate approaching route for emergency forces, it becomes necessary to warn the population in a short time. For\nthat, weather and meteorological data are of interest.\n\n\nLarge scale disasters also require a lot of spatial data at an early stage in order to be able to identify the already\nor potentially affected areas and assess the development of the situation.\n\n\nChallenges\n\u00b6\n\n\nIn many accidents, immediate action is required with very little time planning. The example scenario of a dangerous\ngoods accident illustrates this, but can be easily transferred, for example, to attacks or incidents in production\nplants with leaking harmful substances. In order to plan the measures at short notice, considerable amounts of data have\nto be processed in the shortest possible time and \"converted\" into targeted information for the heads of operations. The\nbasis for this is the investigation carried out by the fire brigades' management cycle, which in such cases are always\nheaded by the fire brigades. In this case, important data must be collected and interpreted for all further steps.\nWithout accurate information, only incomplete assessments of the situation and thus decisions on measures based on\nincomplete assumptions are possible.\n\n\nDepending on the dynamics of the situation, the operations manager must quickly assess the dangers for humans, animals,\nthe environment and material assets and decide how to proceed. For example, liquid and gaseous pollutants spread very\nquickly depending on weather conditions and geographical characteristics of the location. Often the outside temperature\ndetermines the formation of flammable atmospheres and reaction products, which drift into the environment in the form of\nnoxious gas clouds - very difficult to predict in the event of changes in wind direction. In addition, some substances\nare invisible to the human eye and/or odourless or cannot be detected or determined due to the hazard and shut-off\nlimits in accordance with the fire brigade regulations.\n\n\nData Sources\n\u00b6\n\n\nAs a basis for assessing the situation, considerable amounts of geodata are needed, the processing and provision of\nwhich is the core of the subject area. The basis for this is the OGC1-compliant geodata portals of the federal states\nand municipalities, which provide information on topography, the water network, the environment, population, sewers and\ndirect dischargers, for example. Until recently, however, this data was only updated in multi-annual cycles, which made\nit necessary to supplement current data sources. Since June 2015, the Sentinel 2 mission, as part of ESA's COPERNICUS\nsystem, has been delivering high spatial resolution satellite data in the visible and radar range with 14-day update\ncycles.\n\n\nIn addition to the geodata, an on-site investigation is necessary to determine the necessary measures. This can be\ncomplicated by various factors. For example, substance identification can be problematic in accidents involving\ndangerous goods transporters from a distance, as the prescribed vehicle signage is not always up-to-date and loading\ndocuments are located in the inaccessible driver's cab. This makes the acquisition of the latest data at the place of\nuse indispensable. This includes measurement data obtained via portable sensors (catalytic multi-warning and Ex devices,\nchemo-sensors or test tubes). In larger locations, terrestrial measuring vehicles (so-called NBC scanners) or the\nAnalytical Task Force (ATF) are used. Such vehicles have photoionization detectors (PID) or optical sensors such as the\nSIGIS2 system. However, these measurements are often complex and associated with dangers, for example in the case of a\npotentially 30,000 litre petrol-contaminated vehicle.\n\n\nNew Solution Concepts within the BigGIS Project\n\u00b6\n\n\nBigGIS is intended to enable an improved operational environment evaluation by the head of operations by combining the\ncurrently acquired mass data with equally extensive and complex archive data. To this end, a new approach of\nexploration, analysis, integration and visualisation must be developed for the described application scenarios. The goal\nis a much faster and more reliable solution for providing approach routes to the site, the location of supply areas for\nemergency personnel moving up, as well as the situation evaluation at the site of operation, including the detection of\npossible pollutants and the prediction of their temporal and spatial dispersion and the derivation of suitable measures.\n\n\nIn addition to the previously available geo and meteorological data, the BigGIS project will use optical, airborne data\nto record traffic infrastructure, existing buildings, protected goods and, if necessary, the identification of\nendangered persons. This data can be captured by a \nflying robot equipped with multi-spectral or hyperspectral\nsensors\n (similar to SIGIS2) and flying autonomously over the scene of the\naccident. These so-called UAS (Unmanned Aerial Systems) continuously send data over a broad spectrum (visible light up\nto infrared or far infrared) of the underlying air layers to BigGIS. Hyperspectral sensors often generate data volumes\nin the gigabyte range for each photo flight and application. BigGIS should be able to handle these data volumes and\noffer suitable APIs to perform data analysis and integration at scale.\n\n\nFurther images taken with the aid of the flying robots indicate the direction and velocity of propagation of pollutants\n(liquid, gaseous). This allows for forecasts of the diffusion and drift of the pollutants more accurately than with the\nusual approaches (Halpaap's club, MET or Memplex club etc.). In principle, it is possible to classify escaping\nsubstances or reaction products in pollutant pools or noxious gas clouds by means of absorption spectra in the infrared\nand far infrared range. Sensors such as Hygas, SIGIS 2 and CHARM are already available as ground stated or aviated\nsystems for that purpose. In the BigGIS project, \ninitial feasibility\nstudies\n were carried out to test the substance identification of simulated\nnoxious gas clouds with multi-spectral cameras for the visible range, which are suitable for use with small drones due\nto their relatively low weight.\n\n\nIn order to make efficient use of such drones, the BigGIS project is offers an implementation of \nautonomous, optimal\nflight planning\n by the drone itself.\n\n\nAt the same time, models are being developed that can \npredict the propagation of a detected gas\ncloud\n. The aim is to predict possible future scenarios for the spread of the pollutant,\nwind directions and strengths (e. g. taken from FeWIS), the geo-positions of neighbouring residential areas and their\nsurrounding structure, for example via satellite images, geographical characteristics of the accident area, depending on\nthe information on the spread of the pollutant, wind direction and strength (e. g. taken from FeWIS). The predicted\nscenario is to be presented to the operations manager in a compact, understandable way by means of new visual techniques\ndirectly with the (unsure) knowledge about the spread of the pollutants and thus enable decision support.",
            "title": "Disaster Management"
        },
        {
            "location": "/scenarios/disaster/#disaster-management",
            "text": "",
            "title": "Disaster Management"
        },
        {
            "location": "/scenarios/disaster/#general",
            "text": "Disasters and accidents always happen somewhere. Therefore, it is necessary to use geodata even in the case of small\naccident scenarios in order to assess access routes and their impact on the environment. Information support for the\nsafety authorities is necessary in many cases.",
            "title": "General"
        },
        {
            "location": "/scenarios/disaster/#some-examples",
            "text": "Complex traffic accidents require indications of conceivable access routes for the rescue services and, in particular,\nindications of departure routes for the rescue service in order to be able to supply patients quickly.  Local storms lead to blocked or flooded roads and destroyed infrastructure, which is why rescue forces must be informed\nabout the main areas of operation, access routes and safe areas for the potential accommodation of evacuees.  Large fires - for example in a recycling plant - produce highly contaminated flue gases which, depending on the weather\nconditions, remain on site (e. g. inversion weather conditions) or drift and diffuse. In addition to the question of the\nappropriate approaching route for emergency forces, it becomes necessary to warn the population in a short time. For\nthat, weather and meteorological data are of interest.  Large scale disasters also require a lot of spatial data at an early stage in order to be able to identify the already\nor potentially affected areas and assess the development of the situation.",
            "title": "Some examples"
        },
        {
            "location": "/scenarios/disaster/#challenges",
            "text": "In many accidents, immediate action is required with very little time planning. The example scenario of a dangerous\ngoods accident illustrates this, but can be easily transferred, for example, to attacks or incidents in production\nplants with leaking harmful substances. In order to plan the measures at short notice, considerable amounts of data have\nto be processed in the shortest possible time and \"converted\" into targeted information for the heads of operations. The\nbasis for this is the investigation carried out by the fire brigades' management cycle, which in such cases are always\nheaded by the fire brigades. In this case, important data must be collected and interpreted for all further steps.\nWithout accurate information, only incomplete assessments of the situation and thus decisions on measures based on\nincomplete assumptions are possible.  Depending on the dynamics of the situation, the operations manager must quickly assess the dangers for humans, animals,\nthe environment and material assets and decide how to proceed. For example, liquid and gaseous pollutants spread very\nquickly depending on weather conditions and geographical characteristics of the location. Often the outside temperature\ndetermines the formation of flammable atmospheres and reaction products, which drift into the environment in the form of\nnoxious gas clouds - very difficult to predict in the event of changes in wind direction. In addition, some substances\nare invisible to the human eye and/or odourless or cannot be detected or determined due to the hazard and shut-off\nlimits in accordance with the fire brigade regulations.",
            "title": "Challenges"
        },
        {
            "location": "/scenarios/disaster/#data-sources",
            "text": "As a basis for assessing the situation, considerable amounts of geodata are needed, the processing and provision of\nwhich is the core of the subject area. The basis for this is the OGC1-compliant geodata portals of the federal states\nand municipalities, which provide information on topography, the water network, the environment, population, sewers and\ndirect dischargers, for example. Until recently, however, this data was only updated in multi-annual cycles, which made\nit necessary to supplement current data sources. Since June 2015, the Sentinel 2 mission, as part of ESA's COPERNICUS\nsystem, has been delivering high spatial resolution satellite data in the visible and radar range with 14-day update\ncycles.  In addition to the geodata, an on-site investigation is necessary to determine the necessary measures. This can be\ncomplicated by various factors. For example, substance identification can be problematic in accidents involving\ndangerous goods transporters from a distance, as the prescribed vehicle signage is not always up-to-date and loading\ndocuments are located in the inaccessible driver's cab. This makes the acquisition of the latest data at the place of\nuse indispensable. This includes measurement data obtained via portable sensors (catalytic multi-warning and Ex devices,\nchemo-sensors or test tubes). In larger locations, terrestrial measuring vehicles (so-called NBC scanners) or the\nAnalytical Task Force (ATF) are used. Such vehicles have photoionization detectors (PID) or optical sensors such as the\nSIGIS2 system. However, these measurements are often complex and associated with dangers, for example in the case of a\npotentially 30,000 litre petrol-contaminated vehicle.",
            "title": "Data Sources"
        },
        {
            "location": "/scenarios/disaster/#new-solution-concepts-within-the-biggis-project",
            "text": "BigGIS is intended to enable an improved operational environment evaluation by the head of operations by combining the\ncurrently acquired mass data with equally extensive and complex archive data. To this end, a new approach of\nexploration, analysis, integration and visualisation must be developed for the described application scenarios. The goal\nis a much faster and more reliable solution for providing approach routes to the site, the location of supply areas for\nemergency personnel moving up, as well as the situation evaluation at the site of operation, including the detection of\npossible pollutants and the prediction of their temporal and spatial dispersion and the derivation of suitable measures.  In addition to the previously available geo and meteorological data, the BigGIS project will use optical, airborne data\nto record traffic infrastructure, existing buildings, protected goods and, if necessary, the identification of\nendangered persons. This data can be captured by a  flying robot equipped with multi-spectral or hyperspectral\nsensors  (similar to SIGIS2) and flying autonomously over the scene of the\naccident. These so-called UAS (Unmanned Aerial Systems) continuously send data over a broad spectrum (visible light up\nto infrared or far infrared) of the underlying air layers to BigGIS. Hyperspectral sensors often generate data volumes\nin the gigabyte range for each photo flight and application. BigGIS should be able to handle these data volumes and\noffer suitable APIs to perform data analysis and integration at scale.  Further images taken with the aid of the flying robots indicate the direction and velocity of propagation of pollutants\n(liquid, gaseous). This allows for forecasts of the diffusion and drift of the pollutants more accurately than with the\nusual approaches (Halpaap's club, MET or Memplex club etc.). In principle, it is possible to classify escaping\nsubstances or reaction products in pollutant pools or noxious gas clouds by means of absorption spectra in the infrared\nand far infrared range. Sensors such as Hygas, SIGIS 2 and CHARM are already available as ground stated or aviated\nsystems for that purpose. In the BigGIS project,  initial feasibility\nstudies  were carried out to test the substance identification of simulated\nnoxious gas clouds with multi-spectral cameras for the visible range, which are suitable for use with small drones due\nto their relatively low weight.  In order to make efficient use of such drones, the BigGIS project is offers an implementation of  autonomous, optimal\nflight planning  by the drone itself.  At the same time, models are being developed that can  predict the propagation of a detected gas\ncloud . The aim is to predict possible future scenarios for the spread of the pollutant,\nwind directions and strengths (e. g. taken from FeWIS), the geo-positions of neighbouring residential areas and their\nsurrounding structure, for example via satellite images, geographical characteristics of the accident area, depending on\nthe information on the spread of the pollutant, wind direction and strength (e. g. taken from FeWIS). The predicted\nscenario is to be presented to the operations manager in a compact, understandable way by means of new visual techniques\ndirectly with the (unsure) knowledge about the spread of the pollutants and thus enable decision support.",
            "title": "New Solution Concepts within the BigGIS Project"
        },
        {
            "location": "/scenarios/environment/",
            "text": "Environment\n\u00b6\n\n\n\n\nResponsible person for this section\n\n\n\n\nHannes M\u00fcller (LUBW)\n\n\nAndreas Abecker (Disy)\n\n\nJohannes Kutterer (Disy)\n\n\n\n\n\n\n\n\nIn the globalized world invasive species are a growing concern. Certain invasive species can lead to ecological damage\nand commercial losses and eventually health problem for humans and animals. The geographical nature of these phenomena\nadvise the analysis, evaluation and visualization with geographic information system (GIS).\n\n\n\n\nDrosophila suzukii (male)\n\n\n\nClearly recognizable is the dot on the wing. The spotted wing indicates that the picture\nshows an male individual. (Photo: Judy Gallagher, \nhttp://www.flickr.com/photos/52450054@N04/15675291741/\n,\n\nCreative Commons Attribution 2.0 Generic\n)\n\n\n\n\nOne invasive species which called attention in the last years is the Drosophila suzukii, commonly named the spotted wing\ndrosophila. The vinegar fly was widely observed in parts of Japan,Korea and China. From there the fly was spreaded\narround the world. In the last 10 years they could be observed in Swisserland and in South-West Germany (Rhine Vallay).\nIn other parts of the world e.g. the United States this spread has also been observed.\n\n\n\n\nVisualisation in Disy Cadenza\n\n\n\nDistribution of egg finds of the Drosophila suzukii in 2016 in south west Germany\n\n\n\n\nUnlike other Drosophila, it infests non-rotting and healthy fruits and is therefore of concern to fruit growers, such as\nvintners. The main commercial inpact are on summer fruit including cherries, blueberries, grapes, nectarines, pears,\nplums, pluots, peaches, raspberries, and strawberries. In the United States the yield loss estimates widely vary and\nreached 80% loss in some areas and crops \n1\n.\n\n\nFor the control of pests it is necessary to combine and analyze different information sources in real time.The BIGGIS\nproject adresses this challenge by covering the following aspects:\n\n\n\n\nCollecting data from several sources like the current distribution of the fly, wether conditions, elavation, landuse etc.\n\n\nData analysis of risk areas\n\n\nInteractive visualisation of risk areas and infestion likelyhood\n\n\n\n\n\n\n\n\n\n\n\n\nMark P. Bolda; Rachael E. Goodhue & Frank G. Zalom (2009). Spotted Wing Drosophila: Potential Economic Impact of\n  Newly Established Pest (\nPDF\n).\n  Giannini Foundation of Agricultural Economics, University of California.\u00a0\n\u21a9",
            "title": "Environment"
        },
        {
            "location": "/scenarios/environment/#environment",
            "text": "Responsible person for this section   Hannes M\u00fcller (LUBW)  Andreas Abecker (Disy)  Johannes Kutterer (Disy)     In the globalized world invasive species are a growing concern. Certain invasive species can lead to ecological damage\nand commercial losses and eventually health problem for humans and animals. The geographical nature of these phenomena\nadvise the analysis, evaluation and visualization with geographic information system (GIS).   Drosophila suzukii (male)  \nClearly recognizable is the dot on the wing. The spotted wing indicates that the picture\nshows an male individual. (Photo: Judy Gallagher,  http://www.flickr.com/photos/52450054@N04/15675291741/ , Creative Commons Attribution 2.0 Generic )   One invasive species which called attention in the last years is the Drosophila suzukii, commonly named the spotted wing\ndrosophila. The vinegar fly was widely observed in parts of Japan,Korea and China. From there the fly was spreaded\narround the world. In the last 10 years they could be observed in Swisserland and in South-West Germany (Rhine Vallay).\nIn other parts of the world e.g. the United States this spread has also been observed.   Visualisation in Disy Cadenza  \nDistribution of egg finds of the Drosophila suzukii in 2016 in south west Germany   Unlike other Drosophila, it infests non-rotting and healthy fruits and is therefore of concern to fruit growers, such as\nvintners. The main commercial inpact are on summer fruit including cherries, blueberries, grapes, nectarines, pears,\nplums, pluots, peaches, raspberries, and strawberries. In the United States the yield loss estimates widely vary and\nreached 80% loss in some areas and crops  1 .  For the control of pests it is necessary to combine and analyze different information sources in real time.The BIGGIS\nproject adresses this challenge by covering the following aspects:   Collecting data from several sources like the current distribution of the fly, wether conditions, elavation, landuse etc.  Data analysis of risk areas  Interactive visualisation of risk areas and infestion likelyhood       Mark P. Bolda; Rachael E. Goodhue & Frank G. Zalom (2009). Spotted Wing Drosophila: Potential Economic Impact of\n  Newly Established Pest ( PDF ).\n  Giannini Foundation of Agricultural Economics, University of California.\u00a0 \u21a9",
            "title": "Environment"
        }
    ]
}