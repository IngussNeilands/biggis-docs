{
    "docs": [
        {
            "location": "/",
            "text": "About BigGIS\n\u00b6\n\n\nBigGIS is a new generation of GIS that supports decision making in multiple\nscenarios which require processing of large and heterogeneous data sets.\n\n\nThe novelty lies in an integrated analytical approach to spatio-temporal\ndata, that are unstructured and from unreliable sources. The system provides\npredictive, prescriptive and visual tool integrated in a common analytical\npipeline.\n\n\n\n\n\n\n\n\nSmart City\n\n\nEnvironmental Management\n\n\nDisaster Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  #scenlist img {\n    height:32px;\n    vertical-align:middle\n  }\n\n\n\n\nThe project is evaluated on three scenarios:\n\n\n  \n Smart City\n\n  : urban heat islands, particulate matter\n  \n\n  \n Environmental management\n\n  : health threatening animals and plants\n  \n\n  \n Disaster control, civil protection\n\n  : air pollution, toxic chemicals\n\n\n\n\n\n\n\nWhy BigGIS?\n\u00b6\n\n\nCurrent GIS solution are mostly tackling big data related requirements in\nterms of data volume or data velocity. In the era of cloud computing,\nleveraging cloud-based resources is a widely adopted pattern. In addition,\nwith the advent of big data analytics, performing massively parallel\nanalytical tasks on large-scale data at rest or data in motion is as well\nbecoming a feasible approach shaping the design of today\u2019s GIS. Although\nscaling out enables GIS to tackle the aforementioned big data induced\nrequirements, there are still two major open issues. Firstly, dealing with\nvarying data types across multiple data sources (variety) lead to data and\nschema heterogeneity, e.g., to describe locations such as addresses, relative\nspatial relationships or different coordinates reference systems. Secondly,\nmodeling the inherent uncertainties in data (veracity), e.g., real-world\nnoise and erroneous values due to the nature of the data collecting process.\nBoth being crucial tasks in data management and analytics that directly\naffect the information retrieval and decision-making quality and moreover\nthe generated knowledge on human-side (value). By leveraging the the\ncontinuous refinement model, we present a holistic approach that explicitly\ndeals with all big data dimensions. By integrating the user in the\nprocess, computers can learn from the cognitive and perceptive skills of\nhuman analysis to create hidden connections between data and the problem\ndomain. This helps to decrease the noise and uncertainty and allows to\nbuild up trust in the analysis results on user side which will eventually\nlead to an increasing likelihood of relevant findings and generated\nknowledge.\n\n\nContact and Support\n\u00b6\n\n\n\n\n\n\n\n\nRole\n\n\nName\n\n\nE-mail\n\n\n\n\n\n\n\n\n\n\nContact person\n\n\nProf. Dr. Thomas Setzer\n\n\nsetzer@fzi.de\n\n\n\n\n\n\nProject coordination\n\n\nDr. Viliam Simko\n\n\nsimko@fzi.de",
            "title": "About BigGIS"
        },
        {
            "location": "/#about-biggis",
            "text": "BigGIS is a new generation of GIS that supports decision making in multiple\nscenarios which require processing of large and heterogeneous data sets.  The novelty lies in an integrated analytical approach to spatio-temporal\ndata, that are unstructured and from unreliable sources. The system provides\npredictive, prescriptive and visual tool integrated in a common analytical\npipeline.     Smart City  Environmental Management  Disaster Control            \n  #scenlist img {\n    height:32px;\n    vertical-align:middle\n  }  The project is evaluated on three scenarios: \n    Smart City \n  : urban heat islands, particulate matter\n   \n    Environmental management \n  : health threatening animals and plants\n   \n    Disaster control, civil protection \n  : air pollution, toxic chemicals",
            "title": "About BigGIS"
        },
        {
            "location": "/#why-biggis",
            "text": "Current GIS solution are mostly tackling big data related requirements in\nterms of data volume or data velocity. In the era of cloud computing,\nleveraging cloud-based resources is a widely adopted pattern. In addition,\nwith the advent of big data analytics, performing massively parallel\nanalytical tasks on large-scale data at rest or data in motion is as well\nbecoming a feasible approach shaping the design of today\u2019s GIS. Although\nscaling out enables GIS to tackle the aforementioned big data induced\nrequirements, there are still two major open issues. Firstly, dealing with\nvarying data types across multiple data sources (variety) lead to data and\nschema heterogeneity, e.g., to describe locations such as addresses, relative\nspatial relationships or different coordinates reference systems. Secondly,\nmodeling the inherent uncertainties in data (veracity), e.g., real-world\nnoise and erroneous values due to the nature of the data collecting process.\nBoth being crucial tasks in data management and analytics that directly\naffect the information retrieval and decision-making quality and moreover\nthe generated knowledge on human-side (value). By leveraging the the\ncontinuous refinement model, we present a holistic approach that explicitly\ndeals with all big data dimensions. By integrating the user in the\nprocess, computers can learn from the cognitive and perceptive skills of\nhuman analysis to create hidden connections between data and the problem\ndomain. This helps to decrease the noise and uncertainty and allows to\nbuild up trust in the analysis results on user side which will eventually\nlead to an increasing likelihood of relevant findings and generated\nknowledge.",
            "title": "Why BigGIS?"
        },
        {
            "location": "/#contact-and-support",
            "text": "Role  Name  E-mail      Contact person  Prof. Dr. Thomas Setzer  setzer@fzi.de    Project coordination  Dr. Viliam Simko  simko@fzi.de",
            "title": "Contact and Support"
        },
        {
            "location": "/biggis-github-repos/",
            "text": "List of Github Repositories\n\u00b6\n\n\n\n  \n\n    \n\n      \n\n      \n\n      \n\n      \n\n        \nedit\n\n      \n\n    \n\n    \n\n      \n\n        \n\n          \n\n            \n\n              {{ item.name }}\n            \n\n            \n\n              {{ item.description }}\n            \n\n            \n\n              {{ item.note }}\n            \n\n            \n\n              \n{{ item.created_at }}\n\n              \narchived\n\n              \n\n              \n\n                Open\n              \n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}\n\n\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'https://api.github.com/orgs/biggis-project/repos',\n    items_edit_url: 'https://github.com/biggis-project'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      if(this.isLoaded) return ''\n      if(this.items == null) return 'Loading ...'\n      return this.items\n    },\n    publicItems() {\n      return this.items.filter(x=>!x.private)\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n      try {\n        const resp = await axios(this.items_url)\n        this.items = resp.data.sort(sortByDate)\n      } catch(e) {\n        this.items = e.response.data.message\n      }\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Github Repositories"
        },
        {
            "location": "/biggis-github-repos/#list-of-github-repositories",
            "text": "edit \n       \n     \n     \n       \n         \n           \n             \n              {{ item.name }}\n             \n             \n              {{ item.description }}\n             \n             \n              {{ item.note }}\n             \n             \n               {{ item.created_at }} \n               archived \n               \n               \n                Open\n               \n             \n           \n         \n       \n     \n       \nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}      \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'https://api.github.com/orgs/biggis-project/repos',\n    items_edit_url: 'https://github.com/biggis-project'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      if(this.isLoaded) return ''\n      if(this.items == null) return 'Loading ...'\n      return this.items\n    },\n    publicItems() {\n      return this.items.filter(x=>!x.private)\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n      try {\n        const resp = await axios(this.items_url)\n        this.items = resp.data.sort(sortByDate)\n      } catch(e) {\n        this.items = e.response.data.message\n      }\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Github Repositories"
        },
        {
            "location": "/biggis-papers/",
            "text": "List of Papers\n\u00b6\n\n\n\n  \n\n    \n\n      \n\n      \n\n      \n\n      \n\n        \nfile_download\n\n      \n\n      \n\n        \nedit\n\n      \n\n    \n\n    \n\n      \n\n        \n\n          \n\n            \n\n              {{ item.title }}\n            \n\n            \n\n              \n{{author}}\n\n            \n\n            \n\n              {{ item.note }}\n              \n\n                {{ eventData(item) }}\n              \n\n            \n\n            \n\n              \n{{ item.date }}\n\n              \n\n              \n\n                \nopen_in_new\n\n              \n\n              \n\n                \ninsert_drive_file\n\n              \n\n              \n\n                \nopen_in_browser\n\n              \n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/papers.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/papers.json'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      return this.isLoaded ? '' : 'Loading ...';\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n    },\n    ensureArray(x) {\n      return Array.isArray(x) ? x : [x]\n    },\n    eventData(item) {\n      return [\n        item.event.title,\n        item.event.place,\n        item.event.info\n      ].filter(x => x).join(\", \")\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Papers"
        },
        {
            "location": "/biggis-papers/#list-of-papers",
            "text": "file_download \n       \n       \n         edit \n       \n     \n     \n       \n         \n           \n             \n              {{ item.title }}\n             \n             \n               {{author}} \n             \n             \n              {{ item.note }}\n               \n                {{ eventData(item) }}\n               \n             \n             \n               {{ item.date }} \n               \n               \n                 open_in_new \n               \n               \n                 insert_drive_file \n               \n               \n                 open_in_browser \n               \n             \n           \n         \n       \n     \n       \nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}     \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/papers.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/papers.json'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      return this.isLoaded ? '' : 'Loading ...';\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n    },\n    ensureArray(x) {\n      return Array.isArray(x) ? x : [x]\n    },\n    eventData(item) {\n      return [\n        item.event.title,\n        item.event.place,\n        item.event.info\n      ].filter(x => x).join(\", \")\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Papers"
        },
        {
            "location": "/biggis-presentations/",
            "text": "List of Presentations\n\u00b6\n\n\n\n  \n\n    \n\n      \n\n      \n\n      \n\n      \n\n        \nfile_download\n\n      \n\n      \n\n        \nedit\n\n      \n\n    \n\n    \n\n      \n\n        \n\n          \n\n            \n\n              {{ item.title }}\n            \n\n            \n\n              \n{{author}}\n\n            \n\n            \n\n              {{ item.note }}\n              \n\n                {{ eventData(item) }}\n              \n\n            \n\n            \n\n              \n{{ item.date }}\n\n              \n\n              \n\n                \nopen_in_new\n\n              \n\n              \n\n                \ninsert_drive_file\n\n              \n\n              \n\n                \nopen_in_browser\n\n              \n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/presentations.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/presentations.json'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      return this.isLoaded ? '' : 'Loading ...';\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n    },\n    ensureArray(x) {\n      return Array.isArray(x) ? x : [x]\n    },\n    eventData(item) {\n      return [\n        item.event.title,\n        item.event.place,\n        item.event.info\n      ].filter(x => x).join(\", \")\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Presentations"
        },
        {
            "location": "/biggis-presentations/#list-of-presentations",
            "text": "file_download \n       \n       \n         edit \n       \n     \n     \n       \n         \n           \n             \n              {{ item.title }}\n             \n             \n               {{author}} \n             \n             \n              {{ item.note }}\n               \n                {{ eventData(item) }}\n               \n             \n             \n               {{ item.date }} \n               \n               \n                 open_in_new \n               \n               \n                 insert_drive_file \n               \n               \n                 open_in_browser \n               \n             \n           \n         \n       \n     \n       \nth a * { float:right; color: white }\n.md-header a, .md-tabs a {color: white}\nhtml { font-size: 62.5%; } /* mkdocs vs vuetify fix */\n.comma-list > span:not(:last-child):after {\n  content: \", \";\n}     \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/presentations.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/presentations.json'\n  },\n  computed:{\n    isLoaded() {\n      return Array.isArray(this.items);\n    },\n    toolbarStatus() {\n      return this.isLoaded ? '' : 'Loading ...';\n    }\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(function(resp) { return resp.json()} );\n      this.items = json.sort(sortByDate);\n    },\n    ensureArray(x) {\n      return Array.isArray(x) ? x : [x]\n    },\n    eventData(item) {\n      return [\n        item.event.title,\n        item.event.place,\n        item.event.info\n      ].filter(x => x).join(\", \")\n    }\n  }\n});\nvueapp.loadItems() // async load\nvueapp.$el.style.display = 'block' // hack because html shows before vue init",
            "title": "List of Presentations"
        },
        {
            "location": "/biggis-press/",
            "text": "List of Press Releases\n\u00b6\n\n\n\nth a * { float:right; color: white }\n\n\n\n\n\n  \n\n    \n\n      \n\n        \n{{item.title}},\n\n        \n{{item.title}},\n\n        {{item.date}}\n      \n\n    \n\n  \n\n  \nLoading list ...\n\n\n\n\n\n\n\n\n\n\n\n\nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/press.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/press.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json())\n      this.items = json.sort(sortByDate)\n    }\n  }\n})\nvueapp.loadItems() // async load",
            "title": "List of Press Releases"
        },
        {
            "location": "/biggis-press/#list-of-press-releases",
            "text": "th a * { float:right; color: white }  \n   \n     \n       \n         {{item.title}}, \n         {{item.title}}, \n        {{item.date}}\n       \n     \n   \n   Loading list ...     \nconst vueapp = new Vue({\n  el: '#vueapp',\n  data: {\n    items: null,\n    items_url: 'http://biggis-project.eu/data/press.json',\n    items_edit_url: 'https://github.com/biggis-project/biggis-project.github.io/blob/master/data/press.json'\n  },\n  methods: {\n    async loadItems() {\n      const json = await fetch(this.items_url).then(response => response.json())\n      this.items = json.sort(sortByDate)\n    }\n  }\n})\nvueapp.loadItems() // async load",
            "title": "List of Press Releases"
        },
        {
            "location": "/consortium/",
            "text": "Project Consortium\n\u00b6\n\n\n\n\n\n\n\nProject Partners\n\u00b6\n\n\n\n\nFZI Forschungszentrum Informatik am KIT\n\n\nUniversit\u00e4t Konstanz\n\n\nHochschule Karlsruhe\n\n\nDisy Informationssysteme GmbH\n\n\nEXASOL AG\n\n\nEFTAS Fernerkundung Technologietransfer GmbH\n\n\nLandesanstalt f\u00fcr Umwelt Messungen und Naturschutz\n\n\n\n\nAssociated Partners\n\u00b6\n\n\n\n\nTHW Karlsruhe\n\n\nStadt Karlsruhe",
            "title": "Project Consortium"
        },
        {
            "location": "/consortium/#project-consortium",
            "text": "",
            "title": "Project Consortium"
        },
        {
            "location": "/consortium/#project-partners",
            "text": "FZI Forschungszentrum Informatik am KIT  Universit\u00e4t Konstanz  Hochschule Karlsruhe  Disy Informationssysteme GmbH  EXASOL AG  EFTAS Fernerkundung Technologietransfer GmbH  Landesanstalt f\u00fcr Umwelt Messungen und Naturschutz",
            "title": "Project Partners"
        },
        {
            "location": "/consortium/#associated-partners",
            "text": "THW Karlsruhe  Stadt Karlsruhe",
            "title": "Associated Partners"
        },
        {
            "location": "/contributing/",
            "text": "Contributing\n\u00b6\n\n\nWe value all kinds of contributions from the community, not just actual\ncode. If you do like to contribute actual code in the form of bug fixes, new\nfeatures or other patches this page gives you more info on how to do it.\n\n\nGit Branching Model\n\u00b6\n\n\nThe BigGIS team follows the standard practice of using the\n\nmaster\n branch as main integration branch.\n\n\nGit Commit Messages\n\u00b6\n\n\nWe follow the 'imperative present tense' style for commit messages.\n(e.g. \"Add new EnterpriseWidgetLoader instance\")\n\n\nIssue Tracking\n\u00b6\n\n\nIf you find a bug and would like to report it please go to the github\nissue tracker of a given sub-project and file an issue.\n\n\nPull Requests\n\u00b6\n\n\nIf you'd like to submit a code contribution please fork BigGIS and\nsend us pull request against the \nmaster\n branch. Like any other open\nsource project, we might ask you to go through some iterations of\ndiscussion and refinement before merging.\n\n\nContributing documentation\n\u00b6\n\n\nsee \nDocumentation Howto",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#contributing",
            "text": "We value all kinds of contributions from the community, not just actual\ncode. If you do like to contribute actual code in the form of bug fixes, new\nfeatures or other patches this page gives you more info on how to do it.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#git-branching-model",
            "text": "The BigGIS team follows the standard practice of using the master  branch as main integration branch.",
            "title": "Git Branching Model"
        },
        {
            "location": "/contributing/#git-commit-messages",
            "text": "We follow the 'imperative present tense' style for commit messages.\n(e.g. \"Add new EnterpriseWidgetLoader instance\")",
            "title": "Git Commit Messages"
        },
        {
            "location": "/contributing/#issue-tracking",
            "text": "If you find a bug and would like to report it please go to the github\nissue tracker of a given sub-project and file an issue.",
            "title": "Issue Tracking"
        },
        {
            "location": "/contributing/#pull-requests",
            "text": "If you'd like to submit a code contribution please fork BigGIS and\nsend us pull request against the  master  branch. Like any other open\nsource project, we might ask you to go through some iterations of\ndiscussion and refinement before merging.",
            "title": "Pull Requests"
        },
        {
            "location": "/contributing/#contributing-documentation",
            "text": "see  Documentation Howto",
            "title": "Contributing documentation"
        },
        {
            "location": "/docs-howto/",
            "text": "Documentation Howto\n\u00b6\n\n\nWe use \nmkdocs\n for documenting the project.\nThe goal is to make the documentation process as simple as possible,\nto allow versioning and to use pull requests for text reviews.\nI should edit the docs locally, preview it in a browser and then suggest a pull request.\n\n\nThe amount of formatting elements is deliberately small.\nApart from wiki, we try to keep each page self contained and to minimize\ninterlinking between pages because it only complicates reading of the docs.\n\n\nThe documentation is written as a set of Markdown files within the \ndocs/\n directory and after deployment\navailable as a static website: \nDocs Website\n.\n\n\nBefore building the docs\n\u00b6\n\n\nFirst of all, you need \npython 3\n and \npip3\n to be installed.\n(On some Linux distros, you need to use pip3 instead of pip)\n\n\nUsing pip, you need to install the following packages:\n\n\n\n\nmkdocs\n : Provides the executable command \nmkdocs\n.\n\n\nmkdocs-material\n : A material design theme. See also \nthis page\n.\n\n\npyembed-markdown\n : A markdown extension that allows for embedding Youtube videos in documents.\n                         See also \nthis page\n.\n\n\nmkdocs-awesome-pages-plugin\n : This plugin automatically generates the pages section from directory structure.\n  (For more info see \nthis github repository\n)\n\n\n\n\n\n\nHow to install as a user (recommended)\n\n\nYou can install the packages locally as a user into \n~/.local/\n\n\npip3 install --user mkdocs\npip3 install --user mkdocs-material\npip3 install --user pyembed-markdown\npip3 install --user mkdocs-awesome-pages-plugin\n\n\n\n\n\n\n\nHow to install system-wide as root (not recommended)\n\n\npip3 install mkdocs\npip3 install mkdocs-material\npip3 install pyembed-markdown\npip3 install mkdocs-awesome-pages-plugin\n\n\n\n\n\n\n\n\nHow to upgrade (as a user)\n\n\nMake sure you are using \nmkdocs version 0.17.2+\n.\n\npip3 install -U --user mkdocs\npip3 install -U --user mkdocs-material\npip3 install -U --user pyembed-markdown\npip3 install -U --user mkdocs-awesome-pages-plugin\n\n\n\n\n\nRecommended editor\n\u00b6\n\n\nSince we use the markdown format, you can use any plain text editor.\nHowever, we suggest to use \nIntelliJ IDEA\n\nwith the \nMarkdown Support plugin\n (both are free) which gives you:\n\n\n\n\nsyntax highlighting\n\n\npath completion of links such as image file names\n\n\nrefactoring, which is handy when renaming markdown files which are liked from other files\n\n\nfancy search\n\n\noutline of the document structure\n\n\nautomated simplified preview (which is not that important due to the mkdocs hot-reload)\n\n\n\n\nHow to edit\n\u00b6\n\n\nBefore editing the documentation, start the live-reloading docs server\nusing \nmkdocs serve\n within the project root directory.\nThen, open the page \nhttp://127.0.0.1:8000\n in your\nbrowser and watch your edits being reloaded automatically.\n\n\n1\nmkdocs serve\n\n\n\n\n\n\nINFO    -  Building documentation... \nINFO    -  Cleaning site directory \n[I 171024 15:03:51 server:283] Serving on http://127.0.0.1:8000\n[I 171024 15:03:51 handlers:60] Start watching changes\n\n\n\n\nYou can now edit the markdown documents within the \ndocs/\n directory.\n\n\nDeployment\n\u00b6\n\n\nUsing the command \nmkdocs gh-deploy\n we can generate a static \nDocs Website\n\nand deploy it automatically as a github page (served from \ngh-pages\n branch).\n\n\n\n\nInfo\n\n\nThe newly deployed version appears after few seconds.\n\n\n\n\n\n\nWarning\n\n\nThis operation is relevant only to the administrators of the \nbiggis-docs\n repository.\nAs a regular contributor, you should only preview your local changes and create\na pull request instead of directly deploying the built docs to the gh-pages branch.\n\n\n\n\nDocumentation layout\n\u00b6\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n  index.md    # The documentation homepage.\n  ...         # Other markdown pages, images and other files.\n\n\n\n\n\nFor the sake of simplicity, we use the following hierarchy inside \ndocs/\n:\n\n\n\n\nLevel 1\n : areas (directories) that appear in the main menu.\n\n\nLevel 2\n : pages (markdown files) that appear in the left side bar.\n\n\nLevel 3\n : headings (H1, H2, ...) that appear in the table of contents on the right\n\n\n\n\n\n\nWarning\n\n\nDo not use spaces in file names. Replace them with dashes \n-\n (or alternatively with underscores \n_\n).\nThis allows for easier refactoring because spaces are usually transformed to \n%20\n in markdown which looks weird.\n\n\n\n\nFormatting examples\n\u00b6\n\n\nSectioning, Headings and Table of Contents\n# Chapter\n\n## Section\n\n### Subsection\n\n\n\n\nTable of contents is generated automatically from the document structure.\nFootnotes\nSome text with a footnote[^LABEL]\n\n[^LABEL]: Text of the footnote\n\n\n\n\nSee also \nhttps://squidfunk.github.io/mkdocs-material/extensions/footnotes/\nCitations, Notes and Admonition\n!!! cite\n    Here comes the citation including authors, title, year, doi, url ...\n\n\n\n\nCite\nHere comes the citation including authors, title, year, doi, url ...\n!!! note\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n\n\n\n\nNote\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa.\nFor more options see \nhttps://squidfunk.github.io/mkdocs-material/extensions/admonition/\nCollapsible blocks\n??? \"Phasellus posuere in sem ut cursus\"\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n\n\n\n\nPhasellus posuere in sem ut cursus\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa.\nFor more information see \nhttps://facelessuser.github.io/pymdown-extensions/extensions/details/\nImages\nYou can include images into the documentation in the following format:\nSVG\n (scalable vectors).\nJPG\n (photos)\nPNG\n (raster graphics)\nIn contrast to scientific papers, it is not possible to create references to numbered figures in markdown.\n![Image \"alt\" description](path/to/image.svg)\n\n\n\n\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#images-and-media\nNote\nWhen editing a file e.g. \npath/to/ABC.md\n, store all related images in the same\nfolder (\npath/to/ABC\n). This way, different topics are better encapsulated.\nFigures with caption (on top)\nYou can create images with a nice looking caption and additional description.\n!!! info \"Figure: Here comes a single line title\"\n    ![](path/to/image.svg)\n\n    Here comes some additional multi-line text.\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n    Morbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis.\n\n\n\n\nFigure: Here comes a single line title\nHere comes some additional multi-line text.\nLorem ipsum dolor sit amet, consectetur adipiscing elit.\nMorbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis.\nTables\nFirst Header | Second Header | Third Header\n------------ | ------------- | ------------\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell\n\n\n\n\nFirst Header\nSecond Header\nThird Header\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#tables\nTables with alignment\nLeft         | Center        | Right\n---          |:--            |--:\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell\n\n\n\n\nLeft\nCenter\nRight\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nContent Cell\nSee also \nhttp://www.mkdocs.org/user-guide/writing-your-docs/#tables\nMathematical Formulas\nFormula are generated using \nMathJax\n, which is similar to LaTeX.\nSee also this \nquick reference\n.\n$$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$\n\n\n\n\n\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\n\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\nSource Code with Code Highlighting\nCode can be displayed inline like this:\n`print 1+{variable}`\n\n\n\n\nOr it can be displayed in a code block with optional syntax highlighting if the language is specified.\n```python\ndef my_function():\n    \"just a test\"\n    print 8/2 \n```\n\n\n\n\ndef\n \nmy_function\n():\n\n    \n\"just a test\"\n\n    \nprint\n \n8\n/\n2\n \n\n\n\nOther useful langauge codes are:\nsh\n : source code written in shell script\nconsole\n: commands written to console, e.g.: \n$ cat /dev/urandom\njson\n: data in JSON format\njavascript\n: JavaScript source code\nSmart Symbols\nSome smart symbols: -->,  <--, 1st, 2nd, 1/4\n\n\n\n\nSome smart symbols: \u2192,  \u2190, 1\nst\n, 2\nnd\n, \u00bc\nSee also \nhttps://facelessuser.github.io/pymdown-extensions/extensions/smartsymbols/\nSequence diagrams\n```sequence\nTitle: Example sequence diagram\nA->B: Sync call\nB-->A: Sync return\nA->C: Another sync call\nC->>D: Async call\nD-->>C: Async return\n```\n\n\n\nTitle: Example sequence diagram\nA->B: Sync call\nB-->A: Sync return\nA->C: Another sync call\nC->>D: Async call\nD-->>C: Async return\n\n\nEmbedded Youtube Videos\n[!embed](https://www.youtube.com/watch?v=QQKVzZpXTpQ)\n\n\n\n\n\n\nFor more information see \nhttps://pyembed.github.io/usage/markdown/\nHTML (please only in special cases)\nIn special cases, you can also use raw HTML in your document.\n <style>.special img {height:32px; vertical-align:middle}</style>\n <div class=\"special\">\n   [![](https://www.gstatic.com/webp/gallery3/5.png)](https://developers.google.com/speed/webp/gallery2)\n   Click on this icon\n </div>\n\n\n\n\n.special img {height:32px; vertical-align:middle}\n\n \n\n   \n\n   Click on this icon",
            "title": "Documentation Howto"
        },
        {
            "location": "/docs-howto/#documentation-howto",
            "text": "We use  mkdocs  for documenting the project.\nThe goal is to make the documentation process as simple as possible,\nto allow versioning and to use pull requests for text reviews.\nI should edit the docs locally, preview it in a browser and then suggest a pull request.  The amount of formatting elements is deliberately small.\nApart from wiki, we try to keep each page self contained and to minimize\ninterlinking between pages because it only complicates reading of the docs.  The documentation is written as a set of Markdown files within the  docs/  directory and after deployment\navailable as a static website:  Docs Website .",
            "title": "Documentation Howto"
        },
        {
            "location": "/docs-howto/#before-building-the-docs",
            "text": "First of all, you need  python 3  and  pip3  to be installed.\n(On some Linux distros, you need to use pip3 instead of pip)  Using pip, you need to install the following packages:   mkdocs  : Provides the executable command  mkdocs .  mkdocs-material  : A material design theme. See also  this page .  pyembed-markdown  : A markdown extension that allows for embedding Youtube videos in documents.\n                         See also  this page .  mkdocs-awesome-pages-plugin  : This plugin automatically generates the pages section from directory structure.\n  (For more info see  this github repository )    How to install as a user (recommended)  You can install the packages locally as a user into  ~/.local/  pip3 install --user mkdocs\npip3 install --user mkdocs-material\npip3 install --user pyembed-markdown\npip3 install --user mkdocs-awesome-pages-plugin    How to install system-wide as root (not recommended)  pip3 install mkdocs\npip3 install mkdocs-material\npip3 install pyembed-markdown\npip3 install mkdocs-awesome-pages-plugin    How to upgrade (as a user)  Make sure you are using  mkdocs version 0.17.2+ . pip3 install -U --user mkdocs\npip3 install -U --user mkdocs-material\npip3 install -U --user pyembed-markdown\npip3 install -U --user mkdocs-awesome-pages-plugin",
            "title": "Before building the docs"
        },
        {
            "location": "/docs-howto/#recommended-editor",
            "text": "Since we use the markdown format, you can use any plain text editor.\nHowever, we suggest to use  IntelliJ IDEA \nwith the  Markdown Support plugin  (both are free) which gives you:   syntax highlighting  path completion of links such as image file names  refactoring, which is handy when renaming markdown files which are liked from other files  fancy search  outline of the document structure  automated simplified preview (which is not that important due to the mkdocs hot-reload)",
            "title": "Recommended editor"
        },
        {
            "location": "/docs-howto/#how-to-edit",
            "text": "Before editing the documentation, start the live-reloading docs server\nusing  mkdocs serve  within the project root directory.\nThen, open the page  http://127.0.0.1:8000  in your\nbrowser and watch your edits being reloaded automatically.  1 mkdocs serve   INFO    -  Building documentation... \nINFO    -  Cleaning site directory \n[I 171024 15:03:51 server:283] Serving on http://127.0.0.1:8000\n[I 171024 15:03:51 handlers:60] Start watching changes  You can now edit the markdown documents within the  docs/  directory.",
            "title": "How to edit"
        },
        {
            "location": "/docs-howto/#deployment",
            "text": "Using the command  mkdocs gh-deploy  we can generate a static  Docs Website \nand deploy it automatically as a github page (served from  gh-pages  branch).   Info  The newly deployed version appears after few seconds.    Warning  This operation is relevant only to the administrators of the  biggis-docs  repository.\nAs a regular contributor, you should only preview your local changes and create\na pull request instead of directly deploying the built docs to the gh-pages branch.",
            "title": "Deployment"
        },
        {
            "location": "/docs-howto/#documentation-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n  index.md    # The documentation homepage.\n  ...         # Other markdown pages, images and other files.  For the sake of simplicity, we use the following hierarchy inside  docs/ :   Level 1  : areas (directories) that appear in the main menu.  Level 2  : pages (markdown files) that appear in the left side bar.  Level 3  : headings (H1, H2, ...) that appear in the table of contents on the right    Warning  Do not use spaces in file names. Replace them with dashes  -  (or alternatively with underscores  _ ).\nThis allows for easier refactoring because spaces are usually transformed to  %20  in markdown which looks weird.",
            "title": "Documentation layout"
        },
        {
            "location": "/docs-howto/#formatting-examples",
            "text": "Sectioning, Headings and Table of Contents # Chapter\n\n## Section\n\n### Subsection  Table of contents is generated automatically from the document structure. Footnotes Some text with a footnote[^LABEL]\n\n[^LABEL]: Text of the footnote  See also  https://squidfunk.github.io/mkdocs-material/extensions/footnotes/ Citations, Notes and Admonition !!! cite\n    Here comes the citation including authors, title, year, doi, url ...  Cite Here comes the citation including authors, title, year, doi, url ... !!! note\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.  Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa. For more options see  https://squidfunk.github.io/mkdocs-material/extensions/admonition/ Collapsible blocks ??? \"Phasellus posuere in sem ut cursus\"\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.  Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa. For more information see  https://facelessuser.github.io/pymdown-extensions/extensions/details/ Images You can include images into the documentation in the following format: SVG  (scalable vectors). JPG  (photos) PNG  (raster graphics) In contrast to scientific papers, it is not possible to create references to numbered figures in markdown. ![Image \"alt\" description](path/to/image.svg)  See also  http://www.mkdocs.org/user-guide/writing-your-docs/#images-and-media Note When editing a file e.g.  path/to/ABC.md , store all related images in the same\nfolder ( path/to/ABC ). This way, different topics are better encapsulated. Figures with caption (on top) You can create images with a nice looking caption and additional description. !!! info \"Figure: Here comes a single line title\"\n    ![](path/to/image.svg)\n\n    Here comes some additional multi-line text.\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n    Morbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis.  Figure: Here comes a single line title Here comes some additional multi-line text.\nLorem ipsum dolor sit amet, consectetur adipiscing elit.\nMorbi et iaculis mi, ut interdum risus. Nulla facilisis viverra felis tincidunt sagittis. Tables First Header | Second Header | Third Header\n------------ | ------------- | ------------\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell  First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell See also  http://www.mkdocs.org/user-guide/writing-your-docs/#tables Tables with alignment Left         | Center        | Right\n---          |:--            |--:\nContent Cell | Content Cell  | Content Cell\nContent Cell | Content Cell  | Content Cell  Left Center Right Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell See also  http://www.mkdocs.org/user-guide/writing-your-docs/#tables Mathematical Formulas Formula are generated using  MathJax , which is similar to LaTeX.\nSee also this  quick reference . $$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$  \n\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \n\\frac{n!}{k!(n-k)!} = \\binom{n}{k} Source Code with Code Highlighting Code can be displayed inline like this: `print 1+{variable}`  Or it can be displayed in a code block with optional syntax highlighting if the language is specified. ```python\ndef my_function():\n    \"just a test\"\n    print 8/2 \n```  def   my_function (): \n     \"just a test\" \n     print   8 / 2    Other useful langauge codes are: sh  : source code written in shell script console : commands written to console, e.g.:  $ cat /dev/urandom json : data in JSON format javascript : JavaScript source code Smart Symbols Some smart symbols: -->,  <--, 1st, 2nd, 1/4  Some smart symbols: \u2192,  \u2190, 1 st , 2 nd , \u00bc See also  https://facelessuser.github.io/pymdown-extensions/extensions/smartsymbols/ Sequence diagrams ```sequence\nTitle: Example sequence diagram\nA->B: Sync call\nB-->A: Sync return\nA->C: Another sync call\nC->>D: Async call\nD-->>C: Async return\n```  Title: Example sequence diagram\nA->B: Sync call\nB-->A: Sync return\nA->C: Another sync call\nC->>D: Async call\nD-->>C: Async return  Embedded Youtube Videos [!embed](https://www.youtube.com/watch?v=QQKVzZpXTpQ)   For more information see  https://pyembed.github.io/usage/markdown/ HTML (please only in special cases) In special cases, you can also use raw HTML in your document.  <style>.special img {height:32px; vertical-align:middle}</style>\n <div class=\"special\">\n   [![](https://www.gstatic.com/webp/gallery3/5.png)](https://developers.google.com/speed/webp/gallery2)\n   Click on this icon\n </div>  .special img {height:32px; vertical-align:middle} \n  \n    \n   Click on this icon",
            "title": "Formatting examples"
        },
        {
            "location": "/architecture/arch-overview/",
            "text": "Overview\n\u00b6\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/biggis-infrastructure\n\n\n\n\n\n\nThe BigGIS architecture, as depicted in the picture above, consists of several layers that are \nbriefly discussed in the following.\n\n\nModelling\n\u00b6\n\n\nPipelines in BigGIS are modelled leveraging \nStreamPipes\n. StreamPipes allows to \ntransform and analyse sensor landscape and other data streams with less programming effort. We \nextend StreamPipes to support geo-spatial data analytics, e.g. raster data.\n\n\nAnalytics\n\u00b6\n\n\nTo process and analyze the geo-spatial data, BigGIS relies on multiple big data analytics \nframeworks. While \nApache Flink\n is mainly used for sensor data,\nsome application use cases need to process geo-spatial raster or vector data in batches as well. \nThus, we integrate \nApache Spark\n featuring \nGeoTrellis\n into our architectural design in order to process geographic data. GeoTrellis provides a \nnumber of operations to manipulate raster data including map algebra operations. In addition, we \nprovide other data science notebooks (RStudio, Jupyter) to exploratively analyze the data.\n\n\nMiddleware & Connectors\n\u00b6\n\n\nApache Kafka\n is used as the primary message broker. It handles the \ncommunication between the data processing elements, i.e. nodes, within the analytics pipelines. \nBesides, \nActiveMQ\n is another message broker which can be used in \naddition to Kafka. Currently, the main purpose is to provide an endpoint for the websocket \nconnections required by the real-time dashboard of the StreamPipes UI.\n\n\nStorage Backends\n\u00b6\n\n\nInternally, BigGIS uses a variety of different storage backends for designated purposes.\n\n\n\n\nHDFS\n for GeoTrellis catalog and Spark jobs.\n\n\nExasol\n for fast access of stored data.\n\n\nCouchDB\n for pipelines, users and visualizations created in the dashboard.\n\n\nRDF4J\n (formerly Sesame) as a semantic backend of StreamPipes.\n\n\n\n\nContainer Management\n\u00b6\n\n\nRunning these containers in a distributed manner requires a wide variety of technologies, that must be integrated and\nmanaged throughout their lifecycle. To easily deploy our containers, our infrastructure is designed to run on\n\nRancher\n as our container management platform. Rancher enables organizations to run and manage\nDocker and Kubernetes in production, providing four major components, that are:\n\n\n\n\nInfrastructure Orchestration\n\n\nContainer Orchestration and Scheduling\n\n\nApplication Catalog\n\n\nAuthentication\n\n\n\n\nThat way, it is fairly easy to set up a distributed Rancher cluster in a couple of minutes (see \nthe official \ndocumentation\n for more information).\n\n\nInfrastructure\n\u00b6\n\n\nBigGIS infrastructure leverages \nbwCloud Infrastructure-as-a-Service\n (IaaS) offer powered by\nOpenstack. See the \nsection bwCloud\n for additional information about the infrastructure setup.",
            "title": "Overview"
        },
        {
            "location": "/architecture/arch-overview/#overview",
            "text": "Note  Related repository is  https://github.com/biggis-project/biggis-infrastructure    The BigGIS architecture, as depicted in the picture above, consists of several layers that are \nbriefly discussed in the following.",
            "title": "Overview"
        },
        {
            "location": "/architecture/arch-overview/#modelling",
            "text": "Pipelines in BigGIS are modelled leveraging  StreamPipes . StreamPipes allows to \ntransform and analyse sensor landscape and other data streams with less programming effort. We \nextend StreamPipes to support geo-spatial data analytics, e.g. raster data.",
            "title": "Modelling"
        },
        {
            "location": "/architecture/arch-overview/#analytics",
            "text": "To process and analyze the geo-spatial data, BigGIS relies on multiple big data analytics \nframeworks. While  Apache Flink  is mainly used for sensor data,\nsome application use cases need to process geo-spatial raster or vector data in batches as well. \nThus, we integrate  Apache Spark  featuring  GeoTrellis  into our architectural design in order to process geographic data. GeoTrellis provides a \nnumber of operations to manipulate raster data including map algebra operations. In addition, we \nprovide other data science notebooks (RStudio, Jupyter) to exploratively analyze the data.",
            "title": "Analytics"
        },
        {
            "location": "/architecture/arch-overview/#middleware-connectors",
            "text": "Apache Kafka  is used as the primary message broker. It handles the \ncommunication between the data processing elements, i.e. nodes, within the analytics pipelines. \nBesides,  ActiveMQ  is another message broker which can be used in \naddition to Kafka. Currently, the main purpose is to provide an endpoint for the websocket \nconnections required by the real-time dashboard of the StreamPipes UI.",
            "title": "Middleware &amp; Connectors"
        },
        {
            "location": "/architecture/arch-overview/#storage-backends",
            "text": "Internally, BigGIS uses a variety of different storage backends for designated purposes.   HDFS  for GeoTrellis catalog and Spark jobs.  Exasol  for fast access of stored data.  CouchDB  for pipelines, users and visualizations created in the dashboard.  RDF4J  (formerly Sesame) as a semantic backend of StreamPipes.",
            "title": "Storage Backends"
        },
        {
            "location": "/architecture/arch-overview/#container-management",
            "text": "Running these containers in a distributed manner requires a wide variety of technologies, that must be integrated and\nmanaged throughout their lifecycle. To easily deploy our containers, our infrastructure is designed to run on Rancher  as our container management platform. Rancher enables organizations to run and manage\nDocker and Kubernetes in production, providing four major components, that are:   Infrastructure Orchestration  Container Orchestration and Scheduling  Application Catalog  Authentication   That way, it is fairly easy to set up a distributed Rancher cluster in a couple of minutes (see \nthe official  documentation  for more information).",
            "title": "Container Management"
        },
        {
            "location": "/architecture/arch-overview/#infrastructure",
            "text": "BigGIS infrastructure leverages  bwCloud Infrastructure-as-a-Service  (IaaS) offer powered by\nOpenstack. See the  section bwCloud  for additional information about the infrastructure setup.",
            "title": "Infrastructure"
        },
        {
            "location": "/architecture/components/",
            "text": "Components\n\u00b6\n\n\n\n\nTodo\n\n\n\n\nsome info about the structure and hierarchical composition of our Docker images.\n\n\n\n\n\n\n\n\nall sources should be on github\n\n\nimages should be hosted on dockerhub\n\n\nlist of docker images that should be available:\n\n\nHDFS (should use all bwCloud resources available to BigGIS)\n\n\nKafka with Zookeeper (overview of queues needed)\n\n\nFlink\n\n\nSpark\n\n\nGeotrellis libraries (part of the Spark container?)\n\n\nAccumulo with Geomesa (or Geowave)\n\n\nStreamPipes\n\n\nExasolution\n\n\nExasolution should support Accumulo(Geomesa/Geowave) through virtual schema\n\n\nExasolution should support indexing using 2D, 3D, 4D space filling curves (lat,lon,time,elevation)\n\n\n\n\n\n\nGeo-Server\n\n\nmit Plugin f\u00fcr Accumulo\n\n\nzur Transformation von Formaten\n\n\nauch als Datenquelle f\u00fcr Cadenza",
            "title": "Components"
        },
        {
            "location": "/architecture/components/#components",
            "text": "Todo   some info about the structure and hierarchical composition of our Docker images.     all sources should be on github  images should be hosted on dockerhub  list of docker images that should be available:  HDFS (should use all bwCloud resources available to BigGIS)  Kafka with Zookeeper (overview of queues needed)  Flink  Spark  Geotrellis libraries (part of the Spark container?)  Accumulo with Geomesa (or Geowave)  StreamPipes  Exasolution  Exasolution should support Accumulo(Geomesa/Geowave) through virtual schema  Exasolution should support indexing using 2D, 3D, 4D space filling curves (lat,lon,time,elevation)    Geo-Server  mit Plugin f\u00fcr Accumulo  zur Transformation von Formaten  auch als Datenquelle f\u00fcr Cadenza",
            "title": "Components"
        },
        {
            "location": "/architecture/platform-bwcloud/",
            "text": "bwCloud\n\u00b6\n\n\n\n\nWithin the scope of BigGIS, we build our platform on top of the \nbwCloud\n offering. The\nBaden-W\u00fcrttemberg Cloud (bwCloud) provides virtual machines (servers) for members of science- and research institutions\nin Baden-W\u00fcrttemberg (e.g. students and staff-members) much like Amazon\u2019s EC2. Building on the openstack-platform the\ninfrastructure is currently operated by four sites in Baden-W\u00fcrttemberg: the Universities of Mannheim, Karlsruhe, Ulm\nand Freiburg.\n\n\nInfrastructure\n\u00b6\n\n\nCurrently, we run a total of \nnine\n Virtual Machines (VM), serving different kinds of purposes, that are providing (1)\na single-node Rancher Testing environment, (2) a multi-node Rancher Cluster environment, and (3) infrastructure\nsupporting services (e.g. private Docker registry for sensitive images, OpenVPN server to connect to the virtual private\ncloud).\n\n\n\n\n\n\n\n\n\n\nm1.xlarge\n\n\nm1.large\n\n\nm1.medium\n\n\n\n\n\n\n\n\n\n\nOS\n\n\nUbuntu 16.04\n\n\nUbuntu 16.04\n\n\nUbuntu 16.04\n\n\n\n\n\n\nCPU\n\n\n8 vCPU\n\n\n4 vCPU\n\n\n2 vCPU\n\n\n\n\n\n\nMemory\n\n\n8 GB\n\n\n8 GB\n\n\n4 GB\n\n\n\n\n\n\nDisk\n\n\n50 GB\n\n\n50 GB\n\n\n50 GB\n\n\n\n\n\n\n#VM\n\n\n1\n\n\n7\n\n\n1\n\n\n\n\n\n\nUsage\n\n\nRancher Dev/Testing\n\n\nRancher Server, Rancher Cluster (5 VM), Docker Registry\n\n\nOpenVPN\n\n\n\n\n\n\n\n\nRancher is deployed as a set of Docker containers. Running Rancher involves launching at least two containers. One\ncontainer as the management \nserver\n and another container on a node as an \nagent\n.\n\n\n\n\nFigure: Container-based Rancher Setup in bwCloud.\n\n\n\n\n\n\nWhile the server runs on a single VM, we set up a single node agent for development and testing environment as well as a\nfive node cluster environment to run the BigGIS components in a distributed manner (see figure below). Both, the\ndev/testing as well as the cluster environment make use of a the network file system (NFS) in order to overcome the\nproblem of strictly coupeling a service to a node. While this is only mandatory in the cluster environment, it makes\nsense to setup the dev/testing environment as an exact clone to have the same workflows within Rancher.",
            "title": "bwCloud"
        },
        {
            "location": "/architecture/platform-bwcloud/#bwcloud",
            "text": "Within the scope of BigGIS, we build our platform on top of the  bwCloud  offering. The\nBaden-W\u00fcrttemberg Cloud (bwCloud) provides virtual machines (servers) for members of science- and research institutions\nin Baden-W\u00fcrttemberg (e.g. students and staff-members) much like Amazon\u2019s EC2. Building on the openstack-platform the\ninfrastructure is currently operated by four sites in Baden-W\u00fcrttemberg: the Universities of Mannheim, Karlsruhe, Ulm\nand Freiburg.",
            "title": "bwCloud"
        },
        {
            "location": "/architecture/platform-bwcloud/#infrastructure",
            "text": "Currently, we run a total of  nine  Virtual Machines (VM), serving different kinds of purposes, that are providing (1)\na single-node Rancher Testing environment, (2) a multi-node Rancher Cluster environment, and (3) infrastructure\nsupporting services (e.g. private Docker registry for sensitive images, OpenVPN server to connect to the virtual private\ncloud).      m1.xlarge  m1.large  m1.medium      OS  Ubuntu 16.04  Ubuntu 16.04  Ubuntu 16.04    CPU  8 vCPU  4 vCPU  2 vCPU    Memory  8 GB  8 GB  4 GB    Disk  50 GB  50 GB  50 GB    #VM  1  7  1    Usage  Rancher Dev/Testing  Rancher Server, Rancher Cluster (5 VM), Docker Registry  OpenVPN     Rancher is deployed as a set of Docker containers. Running Rancher involves launching at least two containers. One\ncontainer as the management  server  and another container on a node as an  agent .   Figure: Container-based Rancher Setup in bwCloud.    While the server runs on a single VM, we set up a single node agent for development and testing environment as well as a\nfive node cluster environment to run the BigGIS components in a distributed manner (see figure below). Both, the\ndev/testing as well as the cluster environment make use of a the network file system (NFS) in order to overcome the\nproblem of strictly coupeling a service to a node. While this is only mandatory in the cluster environment, it makes\nsense to setup the dev/testing environment as an exact clone to have the same workflows within Rancher.",
            "title": "Infrastructure"
        },
        {
            "location": "/architecture/semantics/",
            "text": "Semantics\n\u00b6\n\n\nDue to the heterogeneity of \ndata sources\n within BigGIS, an explicit semantics is required that\nprovides meaningful descriptions of formats, syntax and semantics of each source. For that purpose, we have developed a\n\nsemantic data management\n platform that enables five star meta data including shared\nvocabularies and background knowledge from concept that are available as Linked Open Data. Using the knowledge base\nretrieved from the semantic data management platform, we allow users of the platform to add additional meta data to JSON\nmessages of environmental observations. Bases on these annotations, we provide \nsemantic message\nenrichment\n on-the-fly and add explicit semantics to the key-value pairs of the\nobservation, including provenance, measured quantities, units and data types. The semantically enriched messages are the\nbasis for further evaluation and transformation. One example for further processing of a semantically enriched data\nstream is \nshape constraint validation\n, which we demonstrate in the last subsection.\n\n\nSemantic Data Management\n\u00b6\n\n\nThe first step towards unified and meaningful descriptions of observations is the semantic data management platform. We\naim to have these descriptions in an open, machine processable and interlinked format, or in short, as five star data:\n\n\n\n\nFigure: From Open Data to Five Star Data\n\n\n\n\n\n\nFor that purpose we introduce Linked Data Wiki (LD-Wiki), a MediaWiki bases knowledge management platform. The core of\nLD-Wiki is the Linked Data Management Module (LDaMM) for updating, querying, reasoning, linking and rule execution on\nsemantic statements in both, LOD and local storage. The architecture of LD-Wiki is as follows:\n\n\n\n\nFigure: Architecture of Linked Data Wiki\n\n\n\n\n\n\nLD-Wiki enables easy reuse of properties of well-known entities and context knowledge from LOD. As an example, the\ncommon knowledge of the concept of a city is already described in LOD sources like schema.org, WikiData or DBPedia. To\nfind instances of this concept in LOD, we link the category in LD-Wiki to the according LOD concepts:\n\n\n\n\nFigure: Linking concepts from LOD to LD-Wiki\n\n\n\n\n\n\nWhen creating new instances of this category withing LD-Wiki, existing instances of the related concepts in LOD with \na similar label can be employed and the values of their properties can be reused in the context of LD-Wiki:\n\n\n\n\nFigure: Linking entities from LOD to LD-Wiki\n\n\n\n\n\n\nSemantic Message Enrichment\n\u00b6\n\n\nThe second step towards unified and meaningful descriptions of observations is the annotation of observation messages.\nWith these annotations, we add explicit semantics to the key-value pairs of a JSON message:\n\n\n\n\nFigure: Explicit Semantics for JSON messages\n\n\n\n\n\n\nIn the backend, the annotations are linked to shared concepts for unified and meaningful interpretation of observations\nfrom heterogeneous sensors. Based on these annotations, an uplifting of non-semantic data streams from heterogeneous\nobservation stations with explicit semantics can be performed on-the-fly:\n\n\n\n\nFigure: Semantic uplifting of JSON messages to JSON-LD based on meta data\n\n\n\n\n\n\nThe semantic enrichment process creates a new message with a list of values. Each \nvalue\nvalue\n of the new message is assigned\nfor each member \nn\nn\n of a message with the according explicit semantics of meta data \nm\nm\n as\n\n\n\n\n\n    \\sum_{n=0}^N \\sum_{m=0}^M value_{n,m} := \n    \\begin{cases}\n        member_{n,val} \\circ meta_{m,val}       & \\text{if } member_{n,key} = meta_{m,key}\\\\\n        \\emptyset                                                               & \\text{otherwise}\n    \\end{cases}\n\n\n\n\n    \\sum_{n=0}^N \\sum_{m=0}^M value_{n,m} := \n    \\begin{cases}\n        member_{n,val} \\circ meta_{m,val}       & \\text{if } member_{n,key} = meta_{m,key}\\\\\n        \\emptyset                                                               & \\text{otherwise}\n    \\end{cases}\n\n\n\n\n\nwith \nmember\nmember\n being the set of \nN\nN\n members of a concrete message without any meta data and \nmeta\nmeta\n being the set of meta\ndata for a class of messages with explicit semantics of \nM\nM\n known types of observations. Each element of both sets\nconsists of \nkey\nkey\n and \nvalue\nvalue\n. If \nkey_{n}\nkey_{n}\n of a member element is equal to \nkey_{m}\nkey_{m}\n of a meta data element,\n\nvalue_{n}\nvalue_{n}\n of member element is combined with \nvalue_{m}\nvalue_{m}\n of the meta data element using the binary operator \n\\circ\n\\circ\n\nin order to add explicit semantics to the original observation. The functionality of the binary operator \n\\circ\n\\circ\n is defined as\n\n\n\n\n\n    f(member, meta) = \n    \\begin{cases}\n        data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{string\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{number\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{bool\\right\\} \\\\\n            object(member, meta)                                                & \\quad  \\text{if } member \\in \\left\\{object\\right\\} \\\\\n            \\sum_{n=0}^N member_{n} \\circ meta_{n}          & \\quad  \\text{if } member \\in \\left\\{array\\right\\} \\\\\n        \\emptyset                                                                       & \\quad  \\text{if } member \\in \\left\\{null\\right\\}\n    \\end{cases}\n\n\n\n\n    f(member, meta) = \n    \\begin{cases}\n        data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{string\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{number\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{bool\\right\\} \\\\\n            object(member, meta)                                                & \\quad  \\text{if } member \\in \\left\\{object\\right\\} \\\\\n            \\sum_{n=0}^N member_{n} \\circ meta_{n}          & \\quad  \\text{if } member \\in \\left\\{array\\right\\} \\\\\n        \\emptyset                                                                       & \\quad  \\text{if } member \\in \\left\\{null\\right\\}\n    \\end{cases}\n\n\n\n\n\nThe function \ndata(member, meta)\ndata(member, meta)\n adds the data type property of \nmeta\nmeta\n to the new message and includes the literal\nvalue of \nmember\nmember\n in conjunction with the XSD data type associated with \nmeta\nmeta\n. The function \nobject(member, meta)\nobject(member, meta)\n\nadds the object property of \nmeta\nmeta\n to the new message and includes an URI reference to the object in \nmember\nmember\n.\nAs the value of an array is nothing else but a list of \nN\nN\n values, we can simply execute the function \nf(member, meta)\nf(member, meta)\n\non each element \nn\nn\n of that list. However, as we identify the meta data for each member by its key, there can only be\na single data type property or object property which is applied to all elements of an array. Different types of values\nwithin a single array are not supported.\n\n\nAs an example, we use the JSON messages of \nLUBW stations\n and push them to a \nmessage broker:\n\n\n\n\nListing: Example of a LUBW observation as JSON\n\n\n{\n\n  \n\"no2\"\n:\n \n61\n,\n\n  \n\"ozn\"\n:\n \n10\n,\n\n  \n\"luqx\"\n:\n \n0\n,\n\n  \n\"latitude\"\n:\n \n48.18169\n,\n\n  \n\"heigth\"\n:\n \n510\n,\n\n  \n\"so2\"\n:\n \n0\n,\n\n  \n\"station\"\n:\n \n\"DEBY189\"\n,\n\n  \n\"pm10\"\n:\n \n0\n,\n\n  \n\"timestamp\"\n:\n \n1516191751218\n,\n\n  \n\"longitude\"\n:\n \n11.46445\n\n\n}\n\n\n\n\n\n\n\n\n\nFigure: Example for LUBW data stream\n\n\n\n\n\n\nAnother example are the JSON messages of \nsenseBox-based weather stations\n,\nwhich are pushed to another topic of the message broker:\n\n\n\n\nListing: Example of a senseBox observation as JSON\n\n\n{\n\n  \n\"title\"\n:\n \n\"Temperatur\"\n,\n\n  \n\"unit\"\n:\n \n\"\u00b0C\"\n,\n\n  \n\"sensorType\"\n:\n \n\"HDC1008\"\n,\n\n  \n\"icon\"\n:\n \n\"osem-thermometer\"\n,\n\n  \n\"_id\"\n:\n \n\"59ec966d49f6f80011c1239a\"\n,\n\n  \n\"lastMeasurement\"\n:\n \n{\n\n    \n\"value\"\n:\n \n\"7.98\"\n,\n\n    \n\"createdAt\"\n:\n \n\"2018-01-18T13:02:14.330Z\"\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\n\n\nFigure: Example for senseBox data stream\n\n\n\n\n\n\nBy employing the explicit meta data, we gain a new message stream of JSON-LD messages with explicit semantics of each\nobservation:\n\n\n\n\nFigure: Example for semantically enriched data stream\n\n\n\n\n\n\nSemantic Data Validation\n\u00b6\n\n\nOnce we have a data stream of observation messages from heterogeneous data sources but with explicit semantics, we can\nalso perform a semantic validation for all messages. For these validations, we again employ our annotation platform to\ndefine data shapes (patterns) that should be fulfilled by all observation messages, regardless from which observation\nstation they are retrieved:\n\n\n\n\nFigure: Shape constraint annotation\n\n\n\n\n\n\nApplying these shapes on the JSON-LD observation messages tells us immediately whether the observation is conform to the\ndefined shape or not:\n\n\n\n\nFigure: Shape constraint validation of JSON-LD messages",
            "title": "Semantics"
        },
        {
            "location": "/architecture/semantics/#semantics",
            "text": "Due to the heterogeneity of  data sources  within BigGIS, an explicit semantics is required that\nprovides meaningful descriptions of formats, syntax and semantics of each source. For that purpose, we have developed a semantic data management  platform that enables five star meta data including shared\nvocabularies and background knowledge from concept that are available as Linked Open Data. Using the knowledge base\nretrieved from the semantic data management platform, we allow users of the platform to add additional meta data to JSON\nmessages of environmental observations. Bases on these annotations, we provide  semantic message\nenrichment  on-the-fly and add explicit semantics to the key-value pairs of the\nobservation, including provenance, measured quantities, units and data types. The semantically enriched messages are the\nbasis for further evaluation and transformation. One example for further processing of a semantically enriched data\nstream is  shape constraint validation , which we demonstrate in the last subsection.",
            "title": "Semantics"
        },
        {
            "location": "/architecture/semantics/#semantic-data-management",
            "text": "The first step towards unified and meaningful descriptions of observations is the semantic data management platform. We\naim to have these descriptions in an open, machine processable and interlinked format, or in short, as five star data:   Figure: From Open Data to Five Star Data    For that purpose we introduce Linked Data Wiki (LD-Wiki), a MediaWiki bases knowledge management platform. The core of\nLD-Wiki is the Linked Data Management Module (LDaMM) for updating, querying, reasoning, linking and rule execution on\nsemantic statements in both, LOD and local storage. The architecture of LD-Wiki is as follows:   Figure: Architecture of Linked Data Wiki    LD-Wiki enables easy reuse of properties of well-known entities and context knowledge from LOD. As an example, the\ncommon knowledge of the concept of a city is already described in LOD sources like schema.org, WikiData or DBPedia. To\nfind instances of this concept in LOD, we link the category in LD-Wiki to the according LOD concepts:   Figure: Linking concepts from LOD to LD-Wiki    When creating new instances of this category withing LD-Wiki, existing instances of the related concepts in LOD with \na similar label can be employed and the values of their properties can be reused in the context of LD-Wiki:   Figure: Linking entities from LOD to LD-Wiki",
            "title": "Semantic Data Management"
        },
        {
            "location": "/architecture/semantics/#semantic-message-enrichment",
            "text": "The second step towards unified and meaningful descriptions of observations is the annotation of observation messages.\nWith these annotations, we add explicit semantics to the key-value pairs of a JSON message:   Figure: Explicit Semantics for JSON messages    In the backend, the annotations are linked to shared concepts for unified and meaningful interpretation of observations\nfrom heterogeneous sensors. Based on these annotations, an uplifting of non-semantic data streams from heterogeneous\nobservation stations with explicit semantics can be performed on-the-fly:   Figure: Semantic uplifting of JSON messages to JSON-LD based on meta data    The semantic enrichment process creates a new message with a list of values. Each  value value  of the new message is assigned\nfor each member  n n  of a message with the according explicit semantics of meta data  m m  as   \n    \\sum_{n=0}^N \\sum_{m=0}^M value_{n,m} := \n    \\begin{cases}\n        member_{n,val} \\circ meta_{m,val}       & \\text{if } member_{n,key} = meta_{m,key}\\\\\n        \\emptyset                                                               & \\text{otherwise}\n    \\end{cases}  \n    \\sum_{n=0}^N \\sum_{m=0}^M value_{n,m} := \n    \\begin{cases}\n        member_{n,val} \\circ meta_{m,val}       & \\text{if } member_{n,key} = meta_{m,key}\\\\\n        \\emptyset                                                               & \\text{otherwise}\n    \\end{cases}   with  member member  being the set of  N N  members of a concrete message without any meta data and  meta meta  being the set of meta\ndata for a class of messages with explicit semantics of  M M  known types of observations. Each element of both sets\nconsists of  key key  and  value value . If  key_{n} key_{n}  of a member element is equal to  key_{m} key_{m}  of a meta data element, value_{n} value_{n}  of member element is combined with  value_{m} value_{m}  of the meta data element using the binary operator  \\circ \\circ \nin order to add explicit semantics to the original observation. The functionality of the binary operator  \\circ \\circ  is defined as   \n    f(member, meta) = \n    \\begin{cases}\n        data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{string\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{number\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{bool\\right\\} \\\\\n            object(member, meta)                                                & \\quad  \\text{if } member \\in \\left\\{object\\right\\} \\\\\n            \\sum_{n=0}^N member_{n} \\circ meta_{n}          & \\quad  \\text{if } member \\in \\left\\{array\\right\\} \\\\\n        \\emptyset                                                                       & \\quad  \\text{if } member \\in \\left\\{null\\right\\}\n    \\end{cases}  \n    f(member, meta) = \n    \\begin{cases}\n        data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{string\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{number\\right\\} \\\\\n            data(member, meta)                                                  & \\quad  \\text{if } member \\in \\left\\{bool\\right\\} \\\\\n            object(member, meta)                                                & \\quad  \\text{if } member \\in \\left\\{object\\right\\} \\\\\n            \\sum_{n=0}^N member_{n} \\circ meta_{n}          & \\quad  \\text{if } member \\in \\left\\{array\\right\\} \\\\\n        \\emptyset                                                                       & \\quad  \\text{if } member \\in \\left\\{null\\right\\}\n    \\end{cases}   The function  data(member, meta) data(member, meta)  adds the data type property of  meta meta  to the new message and includes the literal\nvalue of  member member  in conjunction with the XSD data type associated with  meta meta . The function  object(member, meta) object(member, meta) \nadds the object property of  meta meta  to the new message and includes an URI reference to the object in  member member .\nAs the value of an array is nothing else but a list of  N N  values, we can simply execute the function  f(member, meta) f(member, meta) \non each element  n n  of that list. However, as we identify the meta data for each member by its key, there can only be\na single data type property or object property which is applied to all elements of an array. Different types of values\nwithin a single array are not supported.  As an example, we use the JSON messages of  LUBW stations  and push them to a \nmessage broker:   Listing: Example of a LUBW observation as JSON  { \n   \"no2\" :   61 , \n   \"ozn\" :   10 , \n   \"luqx\" :   0 , \n   \"latitude\" :   48.18169 , \n   \"heigth\" :   510 , \n   \"so2\" :   0 , \n   \"station\" :   \"DEBY189\" , \n   \"pm10\" :   0 , \n   \"timestamp\" :   1516191751218 , \n   \"longitude\" :   11.46445  }     Figure: Example for LUBW data stream    Another example are the JSON messages of  senseBox-based weather stations ,\nwhich are pushed to another topic of the message broker:   Listing: Example of a senseBox observation as JSON  { \n   \"title\" :   \"Temperatur\" , \n   \"unit\" :   \"\u00b0C\" , \n   \"sensorType\" :   \"HDC1008\" , \n   \"icon\" :   \"osem-thermometer\" , \n   \"_id\" :   \"59ec966d49f6f80011c1239a\" , \n   \"lastMeasurement\" :   { \n     \"value\" :   \"7.98\" , \n     \"createdAt\" :   \"2018-01-18T13:02:14.330Z\" \n   }  }     Figure: Example for senseBox data stream    By employing the explicit meta data, we gain a new message stream of JSON-LD messages with explicit semantics of each\nobservation:   Figure: Example for semantically enriched data stream",
            "title": "Semantic Message Enrichment"
        },
        {
            "location": "/architecture/semantics/#semantic-data-validation",
            "text": "Once we have a data stream of observation messages from heterogeneous data sources but with explicit semantics, we can\nalso perform a semantic validation for all messages. For these validations, we again employ our annotation platform to\ndefine data shapes (patterns) that should be fulfilled by all observation messages, regardless from which observation\nstation they are retrieved:   Figure: Shape constraint annotation    Applying these shapes on the JSON-LD observation messages tells us immediately whether the observation is conform to the\ndefined shape or not:   Figure: Shape constraint validation of JSON-LD messages",
            "title": "Semantic Data Validation"
        },
        {
            "location": "/architecture/stream-pipes/",
            "text": "StreamPipes\n\u00b6\n\n\n\n\nResponsible person for this section\n\n\nMatthias Frank\n\n\n\n\n\n\nWith StreamPipes, it is possible to build distributed pipelines to process your data in real-time without development\neffort by providing an easy-to-use graphical user interface on top of existing stream processing frameworks.\nPipelines consist of data streams, real-time event processors and data sinks. StreamPipes provides a system that\ncan be used by domain experts with little to no technical understanding to explore and analyse their data.\nThe whole system is designed in a way that it is flexible and can be easily extended by adding data streams,\ndata processors or data sinks. A community edition of StreamPipes is available under the Apache Licence. \n\n\n\n\nFeatures Overview\n\n\nOfficial Documentation\n\n\n\n\nBigGIS extensions\n\u00b6\n\n\nWithin the BigGIS project, we added several GIS-specific extensions to the StreamPipes platform.\n\n\n\n\n\n\nClimate data\n\n\n\n\n\n\nSenseBoxAdapter\n\n\nprovides the semantic description for the externally created SenseBox data stream\nfor the integration into StreamPipes\n\n\n\n\n\n\nSenseboxMetadataEnricher\n\n\nenriches each message in the SenseBox measurements data stream with meta-data (location, OpenSenseMap-Id)\n\n\n\n\n\n\n\n\n\n\nRaster processing using geotrellis.\n\n\n\n\n\n\nRasterDataEndlessSource\n\n\ngenerates an endless Kafka stream of rasterdata messages to easily test and debug\nother rasterdata processing components\n\n\n\n\n\n\nRasterDataAdapter\n\n\nprovides the semantic description for the RasterDataEndlessSource",
            "title": "StreamPipes"
        },
        {
            "location": "/architecture/stream-pipes/#streampipes",
            "text": "Responsible person for this section  Matthias Frank    With StreamPipes, it is possible to build distributed pipelines to process your data in real-time without development\neffort by providing an easy-to-use graphical user interface on top of existing stream processing frameworks.\nPipelines consist of data streams, real-time event processors and data sinks. StreamPipes provides a system that\ncan be used by domain experts with little to no technical understanding to explore and analyse their data.\nThe whole system is designed in a way that it is flexible and can be easily extended by adding data streams,\ndata processors or data sinks. A community edition of StreamPipes is available under the Apache Licence.    Features Overview  Official Documentation",
            "title": "StreamPipes"
        },
        {
            "location": "/architecture/stream-pipes/#biggis-extensions",
            "text": "Within the BigGIS project, we added several GIS-specific extensions to the StreamPipes platform.    Climate data    SenseBoxAdapter  provides the semantic description for the externally created SenseBox data stream\nfor the integration into StreamPipes    SenseboxMetadataEnricher  enriches each message in the SenseBox measurements data stream with meta-data (location, OpenSenseMap-Id)      Raster processing using geotrellis.    RasterDataEndlessSource  generates an endless Kafka stream of rasterdata messages to easily test and debug\nother rasterdata processing components    RasterDataAdapter  provides the semantic description for the RasterDataEndlessSource",
            "title": "BigGIS extensions"
        },
        {
            "location": "/data-sources/",
            "text": "Datasets in BigGIS\n\u00b6\n\n\nThis section describes publicly available datasets that we used thoughout the project.",
            "title": "Datasets in BigGIS"
        },
        {
            "location": "/data-sources/#datasets-in-biggis",
            "text": "This section describes publicly available datasets that we used thoughout the project.",
            "title": "Datasets in BigGIS"
        },
        {
            "location": "/data-sources/atmosphere/dwd/",
            "text": "DWD (Meteorological Data)\n\u00b6\n\n\nThe DWD (Deutscher Wetterdienst) provides very detailed weather and climate data for the many decades, at the moment\nfrom 78 weather stations. We refer the interested reader to a very detailed documentation about the available data which\nis provided \nhere\n as well as\n\ndetailed information about the possible sensor\nvalues\n.\nAll available data are transferred and stored in the BigGIS database.",
            "title": "DWD (Meteorological Data)"
        },
        {
            "location": "/data-sources/atmosphere/dwd/#dwd-meteorological-data",
            "text": "The DWD (Deutscher Wetterdienst) provides very detailed weather and climate data for the many decades, at the moment\nfrom 78 weather stations. We refer the interested reader to a very detailed documentation about the available data which\nis provided  here  as well as detailed information about the possible sensor\nvalues .\nAll available data are transferred and stored in the BigGIS database.",
            "title": "DWD (Meteorological Data)"
        },
        {
            "location": "/data-sources/atmosphere/lubw/",
            "text": "LUBW Stations\n\u00b6\n\n\nAir Measurements\n\u00b6\n\n\nThe LUBW mangages 50-80 aktive measurement stations across Baden-W\u00fcrttemberg, which provide hourly information on three\nmain air pollutants (see \nFigure 1\n):\n\n\n\n\nNitrogen oxides\n\n\nOzone \n\n\nParticular matter (Feinstaub)\n\n\n\n\nThe data is public and can be downloaded from the official web-page:\n\nhttps://www.lubw.baden-wuerttemberg.de/luft/messwerte-immissionswerte#karte\n\n\n\n\nFigure 1: Webpage LUBW with distribution of air measurement stations.\n\n\n\n\n\n\nLUBW REST API\n\u00b6\n\n\nAir measurment data collected by the LUBW (Nirtogen oxides, Ozone, Particular Matter) is also provided by a REST-Service\nfor the BigGIS project. The service is not public and delivers the current measurement value of each measurement\nstation and each pollutant in json format. The service also allows to aquire data on air pollutant from Bavaria,\nThuringia, Saxony-Anhalt and Schleswig-holstein.\n\n\nData on air pollutants across different states of germany can be acquired by the Federal Environmental Agency:\n\nhttps://www.umweltbundesamt.de/daten/luftbelastung/aktuelle-luftdaten#/start?s=q64FAA==&_k=ep8c63\n\n\nMeteorological Station at FZI\n\u00b6\n\n\nIn January 2017 a meteorological station was mounted at the roof of FZI for measuring air pressure, temperature, wind\nspeed, precipitation, humidity and global radiation  (see \nFigure 2\n). The measurements serve for calibrating and validating the sense\nboxes. The measurement device will be dismounted in April 2018.\n\n\n\n\nFigure 2: Meteorological Station at FZI",
            "title": "LUBW Stations"
        },
        {
            "location": "/data-sources/atmosphere/lubw/#lubw-stations",
            "text": "",
            "title": "LUBW Stations"
        },
        {
            "location": "/data-sources/atmosphere/lubw/#air-measurements",
            "text": "The LUBW mangages 50-80 aktive measurement stations across Baden-W\u00fcrttemberg, which provide hourly information on three\nmain air pollutants (see  Figure 1 ):   Nitrogen oxides  Ozone   Particular matter (Feinstaub)   The data is public and can be downloaded from the official web-page: https://www.lubw.baden-wuerttemberg.de/luft/messwerte-immissionswerte#karte   Figure 1: Webpage LUBW with distribution of air measurement stations.",
            "title": "Air Measurements"
        },
        {
            "location": "/data-sources/atmosphere/lubw/#lubw-rest-api",
            "text": "Air measurment data collected by the LUBW (Nirtogen oxides, Ozone, Particular Matter) is also provided by a REST-Service\nfor the BigGIS project. The service is not public and delivers the current measurement value of each measurement\nstation and each pollutant in json format. The service also allows to aquire data on air pollutant from Bavaria,\nThuringia, Saxony-Anhalt and Schleswig-holstein.  Data on air pollutants across different states of germany can be acquired by the Federal Environmental Agency: https://www.umweltbundesamt.de/daten/luftbelastung/aktuelle-luftdaten#/start?s=q64FAA==&_k=ep8c63",
            "title": "LUBW REST API"
        },
        {
            "location": "/data-sources/atmosphere/lubw/#meteorological-station-at-fzi",
            "text": "In January 2017 a meteorological station was mounted at the roof of FZI for measuring air pressure, temperature, wind\nspeed, precipitation, humidity and global radiation  (see  Figure 2 ). The measurements serve for calibrating and validating the sense\nboxes. The measurement device will be dismounted in April 2018.   Figure 2: Meteorological Station at FZI",
            "title": "Meteorological Station at FZI"
        },
        {
            "location": "/data-sources/atmosphere/sensebox-fzi/",
            "text": "SenseBox-based Weather Stations (FZI)\n\u00b6\n\n\n\n\nTodo\n\n\nJulian\n\n\n\n\n\n\n\n\nsupport two different modes of data transmission: WLAN and LoRa\n\n\n\n\nLoRa is preferred for longer range and easier deployment (no site-specific configuration)\n\n\n\n\n\n\n\n\nLoRa gateways\n\n\n\n\nTwo LoRa gateways should be deployed, one at FZI, the other at LUBW-KA (TODO:clarify)\n\n\nGateways are a \nKickstarter project\n,\n    and have not yet been delivered as of 2017-12-15. \nLast status\n\n    on Kickstarter is that the gateways are ready to ship (2017-11-06)\n\n\nJulian handles Kickstarter and deployment\n\n\n\n\n\n\n\n\nData handling\n\n\n\n\na common Kafka queue for events from both transports\n\n\none Kafka message per transmitted event, containing all measurements (temperature, humidity, air pressure, internal temperature, light, UV radiation)\n\n\ntransport specific adapters for \nWLAN\n\n    and \nLoRa\n\n\nshould be handled in a stream-processing way (pipeline modeled using StreamPipes)\n\n\nData source and metadata enrichment exist as \nStreamPipes components\n\n\n\n\n\n\nshould be stored within BigGIS database i.e. Accumulo/Exasolution (sensor,lat,lon,ts)\n\n\na \nFlink job\n\n    that persists the events from the Kafka queue into MySQL or PostgreSQL exists,\n    could be extended to support other JDBC databases\n\n\n\n\n\n\nshould be sent to \nhttps://opensensemap.org/\n (TODO:Jochen)\n\n\nOutlier filtering node (kafka \u2192 flink \u2192 kafka)\n\n\n\n\n\n\n\n\nDeployment\n\n\n\n\n34 LoRa sensor units should be deployed, Julian handles locations, external organizations etc.\n\n\n\n\n\n\n\n\nWeb-based mobile-friendly app\n\u00b6\n\n\n\n\nQR code contains stations id and URL that leads to public web\n\n\nthe web page contains info about the station and the project\n\n\nadmin can click and change station information (or register a new station)\n\n\nlat/lon is taken from the phone (HTML5 geolocation api)\n\n\nadmin can add additional parameters (placement details)",
            "title": "SenseBox-based Weather Stations (FZI)"
        },
        {
            "location": "/data-sources/atmosphere/sensebox-fzi/#sensebox-based-weather-stations-fzi",
            "text": "Todo  Julian     support two different modes of data transmission: WLAN and LoRa   LoRa is preferred for longer range and easier deployment (no site-specific configuration)     LoRa gateways   Two LoRa gateways should be deployed, one at FZI, the other at LUBW-KA (TODO:clarify)  Gateways are a  Kickstarter project ,\n    and have not yet been delivered as of 2017-12-15.  Last status \n    on Kickstarter is that the gateways are ready to ship (2017-11-06)  Julian handles Kickstarter and deployment     Data handling   a common Kafka queue for events from both transports  one Kafka message per transmitted event, containing all measurements (temperature, humidity, air pressure, internal temperature, light, UV radiation)  transport specific adapters for  WLAN \n    and  LoRa  should be handled in a stream-processing way (pipeline modeled using StreamPipes)  Data source and metadata enrichment exist as  StreamPipes components    should be stored within BigGIS database i.e. Accumulo/Exasolution (sensor,lat,lon,ts)  a  Flink job \n    that persists the events from the Kafka queue into MySQL or PostgreSQL exists,\n    could be extended to support other JDBC databases    should be sent to  https://opensensemap.org/  (TODO:Jochen)  Outlier filtering node (kafka \u2192 flink \u2192 kafka)     Deployment   34 LoRa sensor units should be deployed, Julian handles locations, external organizations etc.",
            "title": "SenseBox-based Weather Stations (FZI)"
        },
        {
            "location": "/data-sources/atmosphere/sensebox-fzi/#web-based-mobile-friendly-app",
            "text": "QR code contains stations id and URL that leads to public web  the web page contains info about the station and the project  admin can click and change station information (or register a new station)  lat/lon is taken from the phone (HTML5 geolocation api)  admin can add additional parameters (placement details)",
            "title": "Web-based mobile-friendly app"
        },
        {
            "location": "/data-sources/atmosphere/wunderground/",
            "text": "Wunderground Dataset\n\u00b6\n\n\nTo create models that support the visual analysis of the urban heat island effect, it is crucial to select suitable data\nsets that can be analyzed and transformed into useful visualizations. Data always must be preprocessed to clean and\ntransform the data to improve the quality to avoid misinterpretations of the consecutive visualizations and to enable\nusers to draw valid and helpful conclusions. Therefore, data processing functions as central part of any analysis system\nof underlying complex data. During this process, we follow the chronological order of the well known KDD pipeline.\n\n\nData Description\n\u00b6\n\n\nWe use temperature data from the private Weather Underground Station Network (WUSN) that has a better spatial\ndistribution of stations than station networks from public authorities like the German Weather Service (DWD). The WUSN\nis a crowd-sourcing platform that provides meteorological data from its participating users to gather high-resolution\ndata. Unfortunately, the WUSN dataset often contains invalid values that must be handled by cleaning, regression or\ndeletion with KNIME. To create a feature vector that contains multiple different data that are part of the complex\nvariable composition we found the meteorological variables that are provided by the GWS (DWD) to be useful. The DWD data\nunderlies certain quality standards and is more reliable than the data from WUSN, which also provides meteorological\nrecords. Weather cannot be simply be in uenced by human compared to surface characteristics.\n\n\nWeather Underground Station Network\n\u00b6\n\n\nIn Germany, the WUSN consists of 25298 stations (see Figure 1 and Figure 2) (1832 stations within city borders) that\nprovide multivariate data with a high temporal resolution of up to 2-3 measurements per hour (see Table 1 and Figure 3\nfor more details). In the preprocessing step, we replaced missing and invalid (out of possible range) values for every\nstation by using the average value of the previous and next recording to keep valuable information. We filtered all\nother meteorological attributes that are provided since temperature is sufficient to decide whether the location of a\nstation is warmer than the surrounding. We decide to average the available temperature data hourly which results in a\ndataset containing about 13,6 million data rows for the year of 2016. Unfortunately, some stations have values missing\nfor the range of multiple months, which doesn't affect the visualizations but are important when exploring the\nvisualization and interpreting the results.\n\n\n\n\nFigure 1: Distribution of weather stations in and around Germany.\n\n\n\n\n\n\n\n\nFigure 2: Distribution of weather stations in and around Germany displaed as voronoi diagram.\n\n\n\n\n\n\n\n\nFigure 3: Each weather station is represented by a single row.\n\n\nIt can clearly be seen that the total amount of weather stations is strongly increasing from the beginning of 2015.\n\n\n\n\n\n\n\nTable 1: Description of the data provided from the wunderground network stations.\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nUHI Index\n\n\nServes as indicator whether this station is a hotspot (local UHI) at this location and point in time\n\n\n\n\n\n\nStation ID\n\n\nUnique WU Station Identifier\n\n\n\n\n\n\nLocation\n\n\nGeographic position of the station consisting of longitude and latitude\n\n\n\n\n\n\nTime\n\n\nTimestamp of the recording with minute-by-minute precision\n\n\n\n\n\n\nTemperature\n\n\nProvided in Celsius values with an accuracy of one decimal place\n\n\n\n\n\n\nSurrounding Temp.\n\n\nBilinear interpolated temperature of WU neighbor stations using the distance to the central station as weighting factor\n\n\n\n\n\n\nNeighbor Stations\n\n\nList containing Station IDs up to 10 nearby stations\n\n\n\n\n\n\n\n\n\n\nFor the identification of local urban heat islands (hotspots), we considered the temporal and spatial attributes of the\nWU stations. First, for every station that lies inside of the city borders, the closest (up to 10 stations) neighbor\nstations where identified. Then, a temperature value for the respective surrounding of each city WU station was\ncalculated using a bilinear strategy considering the distance as weighting factor for the temperature value.",
            "title": "Wunderground Dataset"
        },
        {
            "location": "/data-sources/atmosphere/wunderground/#wunderground-dataset",
            "text": "To create models that support the visual analysis of the urban heat island effect, it is crucial to select suitable data\nsets that can be analyzed and transformed into useful visualizations. Data always must be preprocessed to clean and\ntransform the data to improve the quality to avoid misinterpretations of the consecutive visualizations and to enable\nusers to draw valid and helpful conclusions. Therefore, data processing functions as central part of any analysis system\nof underlying complex data. During this process, we follow the chronological order of the well known KDD pipeline.",
            "title": "Wunderground Dataset"
        },
        {
            "location": "/data-sources/atmosphere/wunderground/#data-description",
            "text": "We use temperature data from the private Weather Underground Station Network (WUSN) that has a better spatial\ndistribution of stations than station networks from public authorities like the German Weather Service (DWD). The WUSN\nis a crowd-sourcing platform that provides meteorological data from its participating users to gather high-resolution\ndata. Unfortunately, the WUSN dataset often contains invalid values that must be handled by cleaning, regression or\ndeletion with KNIME. To create a feature vector that contains multiple different data that are part of the complex\nvariable composition we found the meteorological variables that are provided by the GWS (DWD) to be useful. The DWD data\nunderlies certain quality standards and is more reliable than the data from WUSN, which also provides meteorological\nrecords. Weather cannot be simply be in uenced by human compared to surface characteristics.",
            "title": "Data Description"
        },
        {
            "location": "/data-sources/atmosphere/wunderground/#weather-underground-station-network",
            "text": "In Germany, the WUSN consists of 25298 stations (see Figure 1 and Figure 2) (1832 stations within city borders) that\nprovide multivariate data with a high temporal resolution of up to 2-3 measurements per hour (see Table 1 and Figure 3\nfor more details). In the preprocessing step, we replaced missing and invalid (out of possible range) values for every\nstation by using the average value of the previous and next recording to keep valuable information. We filtered all\nother meteorological attributes that are provided since temperature is sufficient to decide whether the location of a\nstation is warmer than the surrounding. We decide to average the available temperature data hourly which results in a\ndataset containing about 13,6 million data rows for the year of 2016. Unfortunately, some stations have values missing\nfor the range of multiple months, which doesn't affect the visualizations but are important when exploring the\nvisualization and interpreting the results.   Figure 1: Distribution of weather stations in and around Germany.     Figure 2: Distribution of weather stations in and around Germany displaed as voronoi diagram.     Figure 3: Each weather station is represented by a single row.  It can clearly be seen that the total amount of weather stations is strongly increasing from the beginning of 2015.    Table 1: Description of the data provided from the wunderground network stations.     Variable  Description      UHI Index  Serves as indicator whether this station is a hotspot (local UHI) at this location and point in time    Station ID  Unique WU Station Identifier    Location  Geographic position of the station consisting of longitude and latitude    Time  Timestamp of the recording with minute-by-minute precision    Temperature  Provided in Celsius values with an accuracy of one decimal place    Surrounding Temp.  Bilinear interpolated temperature of WU neighbor stations using the distance to the central station as weighting factor    Neighbor Stations  List containing Station IDs up to 10 nearby stations      For the identification of local urban heat islands (hotspots), we considered the temporal and spatial attributes of the\nWU stations. First, for every station that lies inside of the city borders, the closest (up to 10 stations) neighbor\nstations where identified. Then, a temperature value for the respective surrounding of each city WU station was\ncalculated using a bilinear strategy considering the distance as weighting factor for the temperature value.",
            "title": "Weather Underground Station Network"
        },
        {
            "location": "/data-sources/biosphere/vitimeteo/",
            "text": "Drosophila Suzukii Observations (Vitimeteo)\n\u00b6\n\n\nThe website \nVitiMeteo\n provides informations for winemakers. The site is provided by the Staatliches  Freiburg.\n\n\n\n\nFigure 1: Screenshot of \nVitiMeteo\n webpage: Showing reported egg finds\n\n\n\n\n\n\nIn the data provided by VitiMeteo are, among other things, observations of the spread of Drosophila suzukii. This data\nconsists of trap findings of Drosophila suzukii as well as percentage information about how many berries were infested\nin a sample taken at the station. Additionally, there is percentage information about how many eggs were found in a\nsample. This percentage can be over 100%, if there are more egg findings than berries in a sample. These observations\nare collected from 867 stations non-uniformly spread over Baden-Wuerttemberg.\n\n\nSee also \ninvasive-species",
            "title": "Drosophila Suzukii Observations (Vitimeteo)"
        },
        {
            "location": "/data-sources/biosphere/vitimeteo/#drosophila-suzukii-observations-vitimeteo",
            "text": "The website  VitiMeteo  provides informations for winemakers. The site is provided by the Staatliches  Freiburg.   Figure 1: Screenshot of  VitiMeteo  webpage: Showing reported egg finds    In the data provided by VitiMeteo are, among other things, observations of the spread of Drosophila suzukii. This data\nconsists of trap findings of Drosophila suzukii as well as percentage information about how many berries were infested\nin a sample taken at the station. Additionally, there is percentage information about how many eggs were found in a\nsample. This percentage can be over 100%, if there are more egg findings than berries in a sample. These observations\nare collected from 867 stations non-uniformly spread over Baden-Wuerttemberg.  See also  invasive-species",
            "title": "Drosophila Suzukii Observations (Vitimeteo)"
        },
        {
            "location": "/data-sources/ground/landcover/",
            "text": "Digital ortho-images (Aerial photographs)\n\u00b6\n\n\nDigital ortho-images were aquired across different flight campaigns between 2013 and 2015 building a mosaic that covers\nBaden-W\u00fcrttemberg entirely (see \nFigure 1 A\n). The georeferenced images come with 4 spectral bands (blue, green, red,\ninfra-red) and a spatial resolution of 20cm (see \nFigure 1 B\n). The non-public dataset was provided by the State\nInstitute for Environment of Baden-W\u00fcrttemberg (LUBW) and State Agency for Spatial Information and Rural Development\nBaden-W\u00fcrttemberg (LGL).\n\n\n\n\nFigure 1: Digital ortho-image example\n\n\n\n\nA)\n Overview on image aquisition years,\n\nB)\n False color composite (R= infra-red, G=green, B=blue) of digital ortho-images with 20cm spatial resolution.",
            "title": "Digital ortho-images (Aerial photographs)"
        },
        {
            "location": "/data-sources/ground/landcover/#digital-ortho-images-aerial-photographs",
            "text": "Digital ortho-images were aquired across different flight campaigns between 2013 and 2015 building a mosaic that covers\nBaden-W\u00fcrttemberg entirely (see  Figure 1 A ). The georeferenced images come with 4 spectral bands (blue, green, red,\ninfra-red) and a spatial resolution of 20cm (see  Figure 1 B ). The non-public dataset was provided by the State\nInstitute for Environment of Baden-W\u00fcrttemberg (LUBW) and State Agency for Spatial Information and Rural Development\nBaden-W\u00fcrttemberg (LGL).   Figure 1: Digital ortho-image example   A)  Overview on image aquisition years, B)  False color composite (R= infra-red, G=green, B=blue) of digital ortho-images with 20cm spatial resolution.",
            "title": "Digital ortho-images (Aerial photographs)"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/",
            "text": "Lightweight Camera Systems\n\u00b6\n\n\n\n\nFigure 1: Sony QX1 mounted on UAV Dual Camera Gimbal together with FLIR\n\n\n\n\n\n\nRGB camera information\n\u00b6\n\n\n\n\nTodo\n\n\nBodo\n\n\n\n\nAs RGB camera systems the SONY Alpha series was used in the\nBigGIS-Project. These consumer market cameras are used due to the fact\nthat the optical specification fits to the need of the project and that\nthey are wildly used to be mounted to a UAV/UAS. After using a Sony\nAlpha 1 camera in the first flying campaign the project switched to the\nSony QX1 ILCE (Alpha series) because of several reasons which are tested\nin the second campaign:\n\n\n\n\nNo camera body (reduction of camera weight)\n\n\nWiFi connection (no straps / cables which can influence the gimbal position)\n\n\nExchangeable objectives (lenses) for a perfect fit to the focal length of the IR camera\n\n\n\n\nSpecifications and relation to the flight height over ground:\n\n\n\n\n\n\n\n\nSONY ILCE QX 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensor format\n\n\n23,2\n\n\n15,4\n\n\n[mm\u00b2] according to manufacturer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5456\n\n\n3632\n\n\npixel  according to manufacturer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4,25\n\n\n4,24\n\n\nsize of pixels in \u00b5m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfocal length (mm)\n\n\n28,0\n\n\n28,0\n\n\n28,0\n\n\n28,0\n\n\n\n\n19,0\n\n\n19,0\n\n\n19,0\n\n\n19,0\n\n\n\n\n\n\npixel size (\u03bcm)\n\n\n4,3\n\n\n4,3\n\n\n4,3\n\n\n4,3\n\n\n\n\n4,3\n\n\n4,3\n\n\n4,3\n\n\n4,3\n\n\n\n\n\n\ncruising altitude (m)\n\n\n25,0\n\n\n50,0\n\n\n75,0\n\n\n100,0\n\n\n\n\n25,0\n\n\n50,0\n\n\n75,0\n\n\n100,0\n\n\n\n\n\n\nground resolution (cm)\n\n\n0,4\n\n\n0,8\n\n\n1,1\n\n\n1,5\n\n\n\n\n0,6\n\n\n1,1\n\n\n1,7\n\n\n2,2\n\n\n\n\n\n\nground coverage X (m)\n\n\n20,7\n\n\n41,4\n\n\n62,1\n\n\n82,8\n\n\n\n\n30,5\n\n\n61,0\n\n\n91,5\n\n\n122,0\n\n\n\n\n\n\nground Coverage  Y (m)\n\n\n13,8\n\n\n27,6\n\n\n41,3\n\n\n55,1\n\n\n\n\n20,3\n\n\n40,6\n\n\n60,9\n\n\n81,2\n\n\n\n\n\n\n\n\nTo program the Sony QX1 one can use Sony Capture API, which is an C++\nimplementation with the following features:\n\n\n\n\nBased on SONY DSC-QX10\n\n\nLINUX / Python Wrapper\n\n\nSSDP Client to establish the WiFi connection\n\n\nHTTP commands\n\n\nJSON format\n\n\n\n\nFurther information:\n\nhttps://www.sony.de/electronics/wechselobjektivkameras/ilce-qx1-body-kit\n\n\n\n\nFigure 2: Sony QX1 mounted on UAV Dual Camera Gimbal together with FLIR\n\n\n\n\n\n\nThermal IR camera information\n\u00b6\n\n\n\n\nTodo\n\n\nBodo\n\n\n\n\nLike with the RGB camera the project used two types of IR cameras which\ncould easily be mounted on a UAV. In the first flying campaign we\nperformed a test using the OPTRIS PI LightWeight camera which is a very\nsmall thermal image camera with appropriate specifications:\n\n\n\n\nOperation system: Linux\n\n\nFull radiometric IR inspection\n\n\nRadiometric data saved on a CuBOX industry computer during the flight\n\n\nOptical resolution 640 x 480 pixel\n\n\nWeight (camera + companion PC): 325 g\n\n\nSize 46 x 56 x 90 mm\n\n\nSpectral range 7,5 to 13 \u00b5m\n\n\n\n\nThe possibility to safe real radiometric data was of special interest\nfor the later on analysis. During the work it got clear, that the real\nspecification (calibration etc.) for the thermal calculation could only\nbe accessed by using the original OPTRIS software. It was encrypted and\nthe data was stored in an additional line in the thermal images. OPTRIS\ngave no access to that line which make it difficult to transfer the data\nto an image processing software.\n\n\nTherefore, the second campaign was performed by using a FLIR Vue Pro R,\nwere the \u201cR\u201d stands for \u201cradiometric\u201d. The Vue Pro also was designed for\nUAV applications. Two facts speaks for the Vue Pro: 1.) There is a full\naccess to the raw radiometric data and 2.) there are a choice of three\ndifferent lenses (9 mm, 13 mm, 19 mm).\n\n\n\n\nOperation system: Linux\n\n\nFull radiometric IR inspection\n\n\nRadiometric data saved as raw data on any companion PC during the flight\n\n\nOptical resolution 640 x 512 pixel\n\n\nWeight (only camera): 92,1 \u2013 113,4 g depending on configuration\n\n\nSize 44,4 x 57,4 x 67 mm\n\n\nSpectral range 7,5 to 13,5 \u00b5m\n\n\n\n\nThe FLIR Vue pro R was used together with the so called TeAx Thermal\nCapture device which stores the radiometric raw data on a storage device\nlike a USB stick, a memory chip or a companion PC. Within the project\nthe camera was combined with a ODROID PC. The features of the\nThermalCapture device:\n\n\n\n\nStore Digital RAW data on a storage device\n\n\nNo transmission errors in images\n\n\nProvides position and time (GPS) based on UAV downlink\n\n\nWeigh 45 g including housing\n\n\nEasy post-production due to free ThermalCapture software\n\n\n\n\nRAW Data is transformed into images during post-processing with the help\nof included software ThermoViewer. The needed parameters for conversion\ncan be detected automatically. Those parameters can be changed\nafterwards manually by the user, to receive the result for every case.\n\n\nOf special interest is the possibility to choose from three different\nfocal lengths / lenses. So it could be fitted to the focal length of the\nSONY QX1 which ensure a nearly similar ground coverage:\n\n\n\n\n\n\n\n\nFLIR Vue Pro R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensor format\n\n\n10,9\n\n\n8,70\n\n\n[mm\u00b2] according to manufacturer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n640\n\n\n512\n\n\npixel  according to manufacturer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17\n\n\n17\n\n\nsize of pixels in \u00b5m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfocal length (mm)\n\n\n13,0\n\n\n13,0\n\n\n13,0\n\n\n13,0\n\n\n\n\n9,0\n\n\n9,0\n\n\n9,0\n\n\n9,0\n\n\n\n\n\n\npixel size (\u03bcm)\n\n\n17,0\n\n\n17,0\n\n\n17,0\n\n\n17,0\n\n\n\n\n17,0\n\n\n17,0\n\n\n17,0\n\n\n17,0\n\n\n\n\n\n\ncruising altitude (m)\n\n\n25,0\n\n\n50,0\n\n\n75,0\n\n\n100,0\n\n\n\n\n25,0\n\n\n50,0\n\n\n75,0\n\n\n100,0\n\n\n\n\n\n\nground resolution (cm)\n\n\n3,3\n\n\n6,5\n\n\n9,8\n\n\n13,1\n\n\n\n\n4,7\n\n\n9,4\n\n\n14,2\n\n\n18,9\n\n\n\n\n\n\nground coverage X (m)\n\n\n20,9\n\n\n41,8\n\n\n62,8\n\n\n83,7\n\n\n\n\n30,2\n\n\n60,4\n\n\n90,7\n\n\n120,9\n\n\n\n\n\n\nground coverage  Y (m)\n\n\n16,7\n\n\n33,5\n\n\n50,2\n\n\n67,0\n\n\n\n\n24,2\n\n\n48,4\n\n\n72,5\n\n\n96,7\n\n\n\n\n\n\n\n\nWithin the project the lens with the 13 mm focal length was chosen for\nit has nearly the same ground coverage them the 28 mm lens of the SONY\nQX1.\n\n\nFurther information:\n\n\nOptris PI LightWeight: \nhttps://www.optris.de/pi-lightweight-netbox\n\n\nFLIR Vue Pro R: \nhttp://www.flir.de/suas/content/?id=70728\n\n\nTaAx Technology ThermalCapture Grabber:\n\nhttp://thermalcapture.com/thermalcapture-grabber-usb/\n\n\n\n\nFigure 3: FLIR Vue Pro R with TEAX Thermal Capture Grabber USB\n\n\n\n\n\n\nHyperspectral camera information\n\u00b6\n\n\n\n\nTodo\n\n\nAlex\n\n\n\n\nThe reason to choose the Cubert UHD 186 Firefly as a specific\nhyperspectral camera depends on the scenario. It was aimed to detect\n\u201cchemicals\u201d in a smoke cloud during a fire event. This proof of concept\nshould give a realistic scenario with a CBRN incident: A fire produces\nsmoke and with help of the convective process a chemical is distributed\ninto the environment.\n\n\nDue to the fact that it was not allowed to choose toxic / dangerous\nchemicals it was decided to use a fire training smoke\n(1,2-Propylenglycol) for a standard smoke machine. The training smoke\nwas contaminated with at least 40% of chlorophyll from the food industry\nwhich usually is uses as food colorant and a not toxic substance.\n\n\nTherefore, an appropriate hyperspectral camera had to be able to find\nchlorophyll in the smoke to what the Firefly is able.\n\n\nSpecifications:\n\n\n\n\nwavelength range: 450nm \u2013 950nm 125 channels\n\n\nresolution: 8 nm @ 532 nm sampling 4 nm\n\n\npan resolution: 1 Megapixel 2500 spectra / cube\n\n\nelongated version, weight 490g\n\n\npower consumption 8W @ 12V\n\n\n\n\nTo mount the camera to an UAV ist was necessary to build a special\ngimbal. In both flight campaign only vertical photographs was\ncollected.\n\n\nFurther information:\n\n\nToday\u2019s model: S 185 Hyperspectral SE FireflEYE:\n\nhttp://cubert-gmbh.com/uhd-185-firefly/\n\n\n\n\nFigure 4: Hyperspectral Cubert UHD 186 Firefly",
            "title": "Lightweight Camera Systems"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/#lightweight-camera-systems",
            "text": "Figure 1: Sony QX1 mounted on UAV Dual Camera Gimbal together with FLIR",
            "title": "Lightweight Camera Systems"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/#rgb-camera-information",
            "text": "Todo  Bodo   As RGB camera systems the SONY Alpha series was used in the\nBigGIS-Project. These consumer market cameras are used due to the fact\nthat the optical specification fits to the need of the project and that\nthey are wildly used to be mounted to a UAV/UAS. After using a Sony\nAlpha 1 camera in the first flying campaign the project switched to the\nSony QX1 ILCE (Alpha series) because of several reasons which are tested\nin the second campaign:   No camera body (reduction of camera weight)  WiFi connection (no straps / cables which can influence the gimbal position)  Exchangeable objectives (lenses) for a perfect fit to the focal length of the IR camera   Specifications and relation to the flight height over ground:     SONY ILCE QX 1                           Sensor format  23,2  15,4  [mm\u00b2] according to manufacturer           5456  3632  pixel  according to manufacturer           4,25  4,24  size of pixels in \u00b5m                      focal length (mm)  28,0  28,0  28,0  28,0   19,0  19,0  19,0  19,0    pixel size (\u03bcm)  4,3  4,3  4,3  4,3   4,3  4,3  4,3  4,3    cruising altitude (m)  25,0  50,0  75,0  100,0   25,0  50,0  75,0  100,0    ground resolution (cm)  0,4  0,8  1,1  1,5   0,6  1,1  1,7  2,2    ground coverage X (m)  20,7  41,4  62,1  82,8   30,5  61,0  91,5  122,0    ground Coverage  Y (m)  13,8  27,6  41,3  55,1   20,3  40,6  60,9  81,2     To program the Sony QX1 one can use Sony Capture API, which is an C++\nimplementation with the following features:   Based on SONY DSC-QX10  LINUX / Python Wrapper  SSDP Client to establish the WiFi connection  HTTP commands  JSON format   Further information: https://www.sony.de/electronics/wechselobjektivkameras/ilce-qx1-body-kit   Figure 2: Sony QX1 mounted on UAV Dual Camera Gimbal together with FLIR",
            "title": "RGB camera information"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/#thermal-ir-camera-information",
            "text": "Todo  Bodo   Like with the RGB camera the project used two types of IR cameras which\ncould easily be mounted on a UAV. In the first flying campaign we\nperformed a test using the OPTRIS PI LightWeight camera which is a very\nsmall thermal image camera with appropriate specifications:   Operation system: Linux  Full radiometric IR inspection  Radiometric data saved on a CuBOX industry computer during the flight  Optical resolution 640 x 480 pixel  Weight (camera + companion PC): 325 g  Size 46 x 56 x 90 mm  Spectral range 7,5 to 13 \u00b5m   The possibility to safe real radiometric data was of special interest\nfor the later on analysis. During the work it got clear, that the real\nspecification (calibration etc.) for the thermal calculation could only\nbe accessed by using the original OPTRIS software. It was encrypted and\nthe data was stored in an additional line in the thermal images. OPTRIS\ngave no access to that line which make it difficult to transfer the data\nto an image processing software.  Therefore, the second campaign was performed by using a FLIR Vue Pro R,\nwere the \u201cR\u201d stands for \u201cradiometric\u201d. The Vue Pro also was designed for\nUAV applications. Two facts speaks for the Vue Pro: 1.) There is a full\naccess to the raw radiometric data and 2.) there are a choice of three\ndifferent lenses (9 mm, 13 mm, 19 mm).   Operation system: Linux  Full radiometric IR inspection  Radiometric data saved as raw data on any companion PC during the flight  Optical resolution 640 x 512 pixel  Weight (only camera): 92,1 \u2013 113,4 g depending on configuration  Size 44,4 x 57,4 x 67 mm  Spectral range 7,5 to 13,5 \u00b5m   The FLIR Vue pro R was used together with the so called TeAx Thermal\nCapture device which stores the radiometric raw data on a storage device\nlike a USB stick, a memory chip or a companion PC. Within the project\nthe camera was combined with a ODROID PC. The features of the\nThermalCapture device:   Store Digital RAW data on a storage device  No transmission errors in images  Provides position and time (GPS) based on UAV downlink  Weigh 45 g including housing  Easy post-production due to free ThermalCapture software   RAW Data is transformed into images during post-processing with the help\nof included software ThermoViewer. The needed parameters for conversion\ncan be detected automatically. Those parameters can be changed\nafterwards manually by the user, to receive the result for every case.  Of special interest is the possibility to choose from three different\nfocal lengths / lenses. So it could be fitted to the focal length of the\nSONY QX1 which ensure a nearly similar ground coverage:     FLIR Vue Pro R                           Sensor format  10,9  8,70  [mm\u00b2] according to manufacturer           640  512  pixel  according to manufacturer           17  17  size of pixels in \u00b5m                      focal length (mm)  13,0  13,0  13,0  13,0   9,0  9,0  9,0  9,0    pixel size (\u03bcm)  17,0  17,0  17,0  17,0   17,0  17,0  17,0  17,0    cruising altitude (m)  25,0  50,0  75,0  100,0   25,0  50,0  75,0  100,0    ground resolution (cm)  3,3  6,5  9,8  13,1   4,7  9,4  14,2  18,9    ground coverage X (m)  20,9  41,8  62,8  83,7   30,2  60,4  90,7  120,9    ground coverage  Y (m)  16,7  33,5  50,2  67,0   24,2  48,4  72,5  96,7     Within the project the lens with the 13 mm focal length was chosen for\nit has nearly the same ground coverage them the 28 mm lens of the SONY\nQX1.  Further information:  Optris PI LightWeight:  https://www.optris.de/pi-lightweight-netbox  FLIR Vue Pro R:  http://www.flir.de/suas/content/?id=70728  TaAx Technology ThermalCapture Grabber: http://thermalcapture.com/thermalcapture-grabber-usb/   Figure 3: FLIR Vue Pro R with TEAX Thermal Capture Grabber USB",
            "title": "Thermal IR camera information"
        },
        {
            "location": "/data-sources/ground/lightweight-cams/#hyperspectral-camera-information",
            "text": "Todo  Alex   The reason to choose the Cubert UHD 186 Firefly as a specific\nhyperspectral camera depends on the scenario. It was aimed to detect\n\u201cchemicals\u201d in a smoke cloud during a fire event. This proof of concept\nshould give a realistic scenario with a CBRN incident: A fire produces\nsmoke and with help of the convective process a chemical is distributed\ninto the environment.  Due to the fact that it was not allowed to choose toxic / dangerous\nchemicals it was decided to use a fire training smoke\n(1,2-Propylenglycol) for a standard smoke machine. The training smoke\nwas contaminated with at least 40% of chlorophyll from the food industry\nwhich usually is uses as food colorant and a not toxic substance.  Therefore, an appropriate hyperspectral camera had to be able to find\nchlorophyll in the smoke to what the Firefly is able.  Specifications:   wavelength range: 450nm \u2013 950nm 125 channels  resolution: 8 nm @ 532 nm sampling 4 nm  pan resolution: 1 Megapixel 2500 spectra / cube  elongated version, weight 490g  power consumption 8W @ 12V   To mount the camera to an UAV ist was necessary to build a special\ngimbal. In both flight campaign only vertical photographs was\ncollected.  Further information:  Today\u2019s model: S 185 Hyperspectral SE FireflEYE: http://cubert-gmbh.com/uhd-185-firefly/   Figure 4: Hyperspectral Cubert UHD 186 Firefly",
            "title": "Hyperspectral camera information"
        },
        {
            "location": "/data-sources/ground/nasa-elevation/",
            "text": "NASA Elevation Profile\n\u00b6\n\n\nInformation taken from \nhttps://asterweb.jpl.nasa.gov/gdem.asp\n:\n\n\nThe Ministry of Economy, Trade, and Industry (METI) of Japan and the United States National Aeronautics and Space\nAdministration (NASA) jointly announced the release of the Advanced Spaceborne Thermal Emission and Reflection\nRadiometer (ASTER) Global Digital Elevation Model Version 2 (GDEM V2) on October 17, 2011.\n\n\nThe first version of the ASTER GDEM, released in June 2009, was generated using stereo-pair images collected by the\nASTER instrument onboard Terra. ASTER GDEM coverage spans from 83 degrees north latitude to 83 degrees south,\nencompassing 99 percent of Earth's landmass.\n\n\nThe improved GDEM V2 adds 260,000 additional stereo-pairs, improving coverage and reducing the occurrence of artifacts.\nThe refined production algorithm provides improved spatial resolution, increased horizontal and vertical accuracy, and\nsuperior water body coverage and detection. The ASTER GDEM V2 maintains the GeoTIFF format and the same gridding and\ntile structure as V1, with 30-meter postings and 1 x 1 degree tiles.\n\n\nVersion 2 shows significant improvements over the previous release. However, users are advised that the data contains\nanomalies and artifacts that will impede effectiveness for use in certain applications. The data are provided \"as is,\"\nand neither NASA nor METI/Japan Space Systems (J-spacesystems) will be responsible for any damages resulting from use of\nthe data.\n\n\nAs a contribution from METI and NASA to the Global Earth Observation System of Systems (GEOSS), ASTER GDEM V2 data are\navailable free of charge to users worldwide from the Land Processes Distributed Active Archive Center (LP DAAC) and\nJ-spacesystems.\n\n\nBelow is an example image derived from ASTER GDEM V1 globe map data sets. The full data set can be downloaded from the\nlinks above.\n\n\n\n\nFigure 1: Example image derived from ASTER GDEM V1 globe map showing the global elevation profile.",
            "title": "NASA Elevation Profile"
        },
        {
            "location": "/data-sources/ground/nasa-elevation/#nasa-elevation-profile",
            "text": "Information taken from  https://asterweb.jpl.nasa.gov/gdem.asp :  The Ministry of Economy, Trade, and Industry (METI) of Japan and the United States National Aeronautics and Space\nAdministration (NASA) jointly announced the release of the Advanced Spaceborne Thermal Emission and Reflection\nRadiometer (ASTER) Global Digital Elevation Model Version 2 (GDEM V2) on October 17, 2011.  The first version of the ASTER GDEM, released in June 2009, was generated using stereo-pair images collected by the\nASTER instrument onboard Terra. ASTER GDEM coverage spans from 83 degrees north latitude to 83 degrees south,\nencompassing 99 percent of Earth's landmass.  The improved GDEM V2 adds 260,000 additional stereo-pairs, improving coverage and reducing the occurrence of artifacts.\nThe refined production algorithm provides improved spatial resolution, increased horizontal and vertical accuracy, and\nsuperior water body coverage and detection. The ASTER GDEM V2 maintains the GeoTIFF format and the same gridding and\ntile structure as V1, with 30-meter postings and 1 x 1 degree tiles.  Version 2 shows significant improvements over the previous release. However, users are advised that the data contains\nanomalies and artifacts that will impede effectiveness for use in certain applications. The data are provided \"as is,\"\nand neither NASA nor METI/Japan Space Systems (J-spacesystems) will be responsible for any damages resulting from use of\nthe data.  As a contribution from METI and NASA to the Global Earth Observation System of Systems (GEOSS), ASTER GDEM V2 data are\navailable free of charge to users worldwide from the Land Processes Distributed Active Archive Center (LP DAAC) and\nJ-spacesystems.  Below is an example image derived from ASTER GDEM V1 globe map data sets. The full data set can be downloaded from the\nlinks above.   Figure 1: Example image derived from ASTER GDEM V1 globe map showing the global elevation profile.",
            "title": "NASA Elevation Profile"
        },
        {
            "location": "/data-sources/ground/nasa-modis/",
            "text": "MODIS Dataset\n\u00b6\n\n\nOverview\n\u00b6\n\n\nThe Land Surface Temperature (LST) and Emissivity daily data are retrieved at 1km pixels by the generalized split-window\nalgorithm and at 6km grids by the day/night algorithm. In the split-window algorithm, emissivities in bands 31 and 32\nare estimated from land cover types, atmospheric column water vapor and lower boundary air surface temperature are\nseparated into tractable sub-ranges for optimal retrieval. In the day/night algorithm, daytime and nighttime LSTs and\nsurface emissivities are retrieved from pairs of day and night MODIS observations in seven TIR bands. The\nproduct\n1\n is comprised of LSTs, quality assessment, observation time, view angles, and emissivities.\n\n\nDetails\n\u00b6\n\n\nLST Data is available with 1km grid resolution, all others in 5km resolution, typical size of a single capture is around\n1350x2000km. MODIS provides 1-2 captures per day and location (this is due to the polar Orbit of the two MODIS\nSatelites). The data is provided as either Near-Real-Time (NRT) data with a delay time of 30min-3h or as Tile Data wich\nis available within 2-3 Days. The Tile Data can be accessed via a coordinate-based Bounding-Box request, NRT Data is\navailable via FTP. All data is stored in the HDF4 file format which contains a two dimensional array of data points\ncontaining the value, coordinates (lat, lon), record time and Quality index (used for masking cloud covered areas)\n\n\nLST Data is available since 2002 in full capability and since 1999 with reduced frequency (single satellite operation\nuntil 2002). For the BigGIS Project, we from the University of Konstanz, made this data available to all project\npartners. For this we created a crawler to collect the published Tile Data, preprocessed the published HDF4 data and\nmade the MODIS data available in our database since 2016.\n\n\n\n\nFigure 1: Example distribution of available MODIS LST data in Germany.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://modis.gsfc.nasa.gov/data/dataprod/mod11.php\n \"Last Accessed on 26.01.2018.\"\u00a0\n\u21a9",
            "title": "MODIS Dataset"
        },
        {
            "location": "/data-sources/ground/nasa-modis/#modis-dataset",
            "text": "",
            "title": "MODIS Dataset"
        },
        {
            "location": "/data-sources/ground/nasa-modis/#overview",
            "text": "The Land Surface Temperature (LST) and Emissivity daily data are retrieved at 1km pixels by the generalized split-window\nalgorithm and at 6km grids by the day/night algorithm. In the split-window algorithm, emissivities in bands 31 and 32\nare estimated from land cover types, atmospheric column water vapor and lower boundary air surface temperature are\nseparated into tractable sub-ranges for optimal retrieval. In the day/night algorithm, daytime and nighttime LSTs and\nsurface emissivities are retrieved from pairs of day and night MODIS observations in seven TIR bands. The\nproduct 1  is comprised of LSTs, quality assessment, observation time, view angles, and emissivities.",
            "title": "Overview"
        },
        {
            "location": "/data-sources/ground/nasa-modis/#details",
            "text": "LST Data is available with 1km grid resolution, all others in 5km resolution, typical size of a single capture is around\n1350x2000km. MODIS provides 1-2 captures per day and location (this is due to the polar Orbit of the two MODIS\nSatelites). The data is provided as either Near-Real-Time (NRT) data with a delay time of 30min-3h or as Tile Data wich\nis available within 2-3 Days. The Tile Data can be accessed via a coordinate-based Bounding-Box request, NRT Data is\navailable via FTP. All data is stored in the HDF4 file format which contains a two dimensional array of data points\ncontaining the value, coordinates (lat, lon), record time and Quality index (used for masking cloud covered areas)  LST Data is available since 2002 in full capability and since 1999 with reduced frequency (single satellite operation\nuntil 2002). For the BigGIS Project, we from the University of Konstanz, made this data available to all project\npartners. For this we created a crawler to collect the published Tile Data, preprocessed the published HDF4 data and\nmade the MODIS data available in our database since 2016.   Figure 1: Example distribution of available MODIS LST data in Germany.        https://modis.gsfc.nasa.gov/data/dataprod/mod11.php  \"Last Accessed on 26.01.2018.\"\u00a0 \u21a9",
            "title": "Details"
        },
        {
            "location": "/data-sources/ground/sentinel2/",
            "text": "Sentinel 2\n\u00b6\n\n\n\n\nTodo\n\n\nAdrian (EFTAS)\n: optimize text\n\n\n\n\nUsed in \nLanduse classification\n as input to classify landcover for the landuse analysis\n\n\nDescription\n\u00b6\n\n\nSentinel 2 dataset is multispectral satellite raster data with 13 bands in the visible (RGB), near infrared (NIR) and\nshort wave infrared (SWIR) spectrum. It can be updated approximately every 5 days. The Sentinel-2A and 2B satellites are\noperated by the European Space Agency (ESA) as part of the Copernicus Programme.\n\n\nsee: \nhttps://en.wikipedia.org/wiki/Sentinel-2\n\n\nSource of the data is the ESA Sentinel Portal\n\u00b6\n\n\n\n\nESA: \nhttps://sentinel.esa.int/web/sentinel/missions/sentinel-2/data-products\n\n\nSentinel Tiles - \nSentinel-2 tiling grid kml\n\n\nAccess to Sentinel Data - \nhttps://sentinel.esa.int/web/sentinel/sentinel-data-access\n\n\nCopernicus Open Access Hub - \nhttps://scihub.copernicus.eu/dhus/#/home\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative Sources\n\u00b6\n\n\n\n\nAWS: \nhttps://aws.amazon.com/de/public-datasets/sentinel-2/\n\n\n\n\nExample download (Tile 32UMU - 2016-05-05 - bands B2,B3,B4,B8 - Blue Green Red NIR )\n\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B02.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B03.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B04.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B08.jp2\n\n\n\nSentinel 2 bands and resolutions\n\u00b6\n\n\n\n\n\n\n\n\nBand\n\n\nBandname\n\n\nCentral Wavelength (\u00b5m)\n\n\nResolution (m)\n\n\nBandwidth (nm)\n\n\n\n\n\n\n\n\n\n\n1\n\n\nCoastal aerosol\n\n\n0.443\n\n\n60\n\n\n20\n\n\n\n\n\n\n2\n\n\nBlue\n\n\n0.490\n\n\n10\n\n\n65\n\n\n\n\n\n\n3\n\n\nGreen\n\n\n0.560\n\n\n10\n\n\n35\n\n\n\n\n\n\n4\n\n\nRed\n\n\n0.665\n\n\n10\n\n\n30\n\n\n\n\n\n\n5\n\n\nVegetation Red Edge\n\n\n0.705\n\n\n20\n\n\n15\n\n\n\n\n\n\n6\n\n\nVegetation Red Edge\n\n\n0.740\n\n\n20\n\n\n15\n\n\n\n\n\n\n7\n\n\nVegetation Red Edge\n\n\n0.783\n\n\n20\n\n\n20\n\n\n\n\n\n\n8\n\n\nNIR\n\n\n0.842\n\n\n10\n\n\n115\n\n\n\n\n\n\n8A\n\n\nNarrow NIR\n\n\n0.865\n\n\n20\n\n\n20\n\n\n\n\n\n\n9\n\n\nWater vapour\n\n\n0.945\n\n\n60\n\n\n20\n\n\n\n\n\n\n10\n\n\nSWIR \u2013 Cirrus\n\n\n1.375\n\n\n60\n\n\n20\n\n\n\n\n\n\n11\n\n\nSWIR\n\n\n1.610\n\n\n20\n\n\n90\n\n\n\n\n\n\n12\n\n\nSWIR\n\n\n2.190\n\n\n20\n\n\n180\n\n\n\n\n\n\n\n\n10m bands\n\u00b6\n\n\n\n\n\n\n\n\nBand\n\n\nBandname\n\n\nCentral Wavelength (\u00b5m)\n\n\nResolution (m)\n\n\nBandwidth (nm)\n\n\n\n\n\n\n\n\n\n\n2\n\n\nBlue\n\n\n0.490\n\n\n10\n\n\n65\n\n\n\n\n\n\n3\n\n\nGreen\n\n\n0.560\n\n\n10\n\n\n35\n\n\n\n\n\n\n4\n\n\nRed\n\n\n0.665\n\n\n10\n\n\n30\n\n\n\n\n\n\n8\n\n\nNIR\n\n\n0.842\n\n\n10\n\n\n115\n\n\n\n\n\n\n\n\n20m bands\n\u00b6\n\n\n\n\n\n\n\n\nBand\n\n\nBandname\n\n\nCentral Wavelength (\u00b5m)\n\n\nResolution (m)\n\n\nBandwidth (nm)\n\n\n\n\n\n\n\n\n\n\n5\n\n\nVegetation Red Edge\n\n\n0.705\n\n\n20\n\n\n15\n\n\n\n\n\n\n6\n\n\nVegetation Red Edge\n\n\n0.740\n\n\n20\n\n\n15\n\n\n\n\n\n\n7\n\n\nVegetation Red Edge\n\n\n0.783\n\n\n20\n\n\n20\n\n\n\n\n\n\n8A\n\n\nNarrow NIR\n\n\n0.865\n\n\n20\n\n\n20\n\n\n\n\n\n\n11\n\n\nSWIR\n\n\n1.610\n\n\n20\n\n\n90\n\n\n\n\n\n\n12\n\n\nSWIR\n\n\n2.190\n\n\n20\n\n\n180\n\n\n\n\n\n\n\n\n60m bands\n\u00b6\n\n\n\n\n\n\n\n\nBand\n\n\nBandname\n\n\nCentral Wavelength (\u00b5m)\n\n\nResolution (m)\n\n\nBandwidth (nm)\n\n\n\n\n\n\n\n\n\n\n1\n\n\nCoastal aerosol\n\n\n0.443\n\n\n60\n\n\n20\n\n\n\n\n\n\n9\n\n\nWater vapour\n\n\n0.945\n\n\n60\n\n\n20\n\n\n\n\n\n\n10\n\n\nSWIR \u2013 Cirrus\n\n\n1.375\n\n\n60\n\n\n20",
            "title": "Sentinel 2"
        },
        {
            "location": "/data-sources/ground/sentinel2/#sentinel-2",
            "text": "Todo  Adrian (EFTAS)\n: optimize text   Used in  Landuse classification  as input to classify landcover for the landuse analysis",
            "title": "Sentinel 2"
        },
        {
            "location": "/data-sources/ground/sentinel2/#description",
            "text": "Sentinel 2 dataset is multispectral satellite raster data with 13 bands in the visible (RGB), near infrared (NIR) and\nshort wave infrared (SWIR) spectrum. It can be updated approximately every 5 days. The Sentinel-2A and 2B satellites are\noperated by the European Space Agency (ESA) as part of the Copernicus Programme.  see:  https://en.wikipedia.org/wiki/Sentinel-2",
            "title": "Description"
        },
        {
            "location": "/data-sources/ground/sentinel2/#source-of-the-data-is-the-esa-sentinel-portal",
            "text": "ESA:  https://sentinel.esa.int/web/sentinel/missions/sentinel-2/data-products  Sentinel Tiles -  Sentinel-2 tiling grid kml  Access to Sentinel Data -  https://sentinel.esa.int/web/sentinel/sentinel-data-access  Copernicus Open Access Hub -  https://scihub.copernicus.eu/dhus/#/home",
            "title": "Source of the data is the ESA Sentinel Portal"
        },
        {
            "location": "/data-sources/ground/sentinel2/#alternative-sources",
            "text": "AWS:  https://aws.amazon.com/de/public-datasets/sentinel-2/   Example download (Tile 32UMU - 2016-05-05 - bands B2,B3,B4,B8 - Blue Green Red NIR ) wget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B02.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B03.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B04.jp2\nwget http://sentinel-s2-l1c.s3.amazonaws.com/tiles/32/U/MU/2016/5/5/0/B08.jp2",
            "title": "Alternative Sources"
        },
        {
            "location": "/data-sources/ground/sentinel2/#sentinel-2-bands-and-resolutions",
            "text": "Band  Bandname  Central Wavelength (\u00b5m)  Resolution (m)  Bandwidth (nm)      1  Coastal aerosol  0.443  60  20    2  Blue  0.490  10  65    3  Green  0.560  10  35    4  Red  0.665  10  30    5  Vegetation Red Edge  0.705  20  15    6  Vegetation Red Edge  0.740  20  15    7  Vegetation Red Edge  0.783  20  20    8  NIR  0.842  10  115    8A  Narrow NIR  0.865  20  20    9  Water vapour  0.945  60  20    10  SWIR \u2013 Cirrus  1.375  60  20    11  SWIR  1.610  20  90    12  SWIR  2.190  20  180",
            "title": "Sentinel 2 bands and resolutions"
        },
        {
            "location": "/data-sources/ground/sentinel2/#10m-bands",
            "text": "Band  Bandname  Central Wavelength (\u00b5m)  Resolution (m)  Bandwidth (nm)      2  Blue  0.490  10  65    3  Green  0.560  10  35    4  Red  0.665  10  30    8  NIR  0.842  10  115",
            "title": "10m bands"
        },
        {
            "location": "/data-sources/ground/sentinel2/#20m-bands",
            "text": "Band  Bandname  Central Wavelength (\u00b5m)  Resolution (m)  Bandwidth (nm)      5  Vegetation Red Edge  0.705  20  15    6  Vegetation Red Edge  0.740  20  15    7  Vegetation Red Edge  0.783  20  20    8A  Narrow NIR  0.865  20  20    11  SWIR  1.610  20  90    12  SWIR  2.190  20  180",
            "title": "20m bands"
        },
        {
            "location": "/data-sources/ground/sentinel2/#60m-bands",
            "text": "Band  Bandname  Central Wavelength (\u00b5m)  Resolution (m)  Bandwidth (nm)      1  Coastal aerosol  0.443  60  20    9  Water vapour  0.945  60  20    10  SWIR \u2013 Cirrus  1.375  60  20",
            "title": "60m bands"
        },
        {
            "location": "/data-sources/ground/uba-air/",
            "text": "Umweltbundesamt Air Pollution\n\u00b6\n\n\nThe Umweltbundesamt (UBA) provides data about the concentration of fine dust and other pollutants\n(CO/SO\n2\n/NO\n2\n etc). General documentation from the UBA can be found\n\nhere\n. For Germany,\naround 320 stations are installed. PM10 fine dust concentrations are available as daily averages, CO as 8-hour averages,\nall others as 1-hour averages. As the gathered data is directly available as CSV using formatted URLs, we saved all\navailable data in our database. Furthermore, every new added measurements are automatically added to our database.\n\n\n\n\nPicture 1: Subset of one of our metadata tables to store the UBA air pollution data.",
            "title": "Umweltbundesamt Air Pollution"
        },
        {
            "location": "/data-sources/ground/uba-air/#umweltbundesamt-air-pollution",
            "text": "The Umweltbundesamt (UBA) provides data about the concentration of fine dust and other pollutants\n(CO/SO 2 /NO 2  etc). General documentation from the UBA can be found here . For Germany,\naround 320 stations are installed. PM10 fine dust concentrations are available as daily averages, CO as 8-hour averages,\nall others as 1-hour averages. As the gathered data is directly available as CSV using formatted URLs, we saved all\navailable data in our database. Furthermore, every new added measurements are automatically added to our database.   Picture 1: Subset of one of our metadata tables to store the UBA air pollution data.",
            "title": "Umweltbundesamt Air Pollution"
        },
        {
            "location": "/data-sources/socio-economic/envirocar/",
            "text": "Envirocar Dataset\n\u00b6\n\n\nThe used dataset contains approximately around 1.7 million data points. Each data record contains 24 attributes\nreflecting sensor values of the vehicle (e.g. speed, rpm, ...) as well as a CO\n2\n estimation. Each data point\nis part of a trip, which is described by a trajectory. While there are 5734 trips, a set of the trips' trajectories can\nfurthermore be associated with a sensor. There are 160 registered sensors which may be directly associated wit a\nvehicle. Additional information about the vehicle (type, etc.) is provided as well.\n\n\n\n\nPicture 1: Distribution of trips per sensor (german).\n\n\n\n\n\n\n\n\nPicture 2: Distribution of trips per month in 2016 and 2017 (german).",
            "title": "Envirocar Dataset"
        },
        {
            "location": "/data-sources/socio-economic/envirocar/#envirocar-dataset",
            "text": "The used dataset contains approximately around 1.7 million data points. Each data record contains 24 attributes\nreflecting sensor values of the vehicle (e.g. speed, rpm, ...) as well as a CO 2  estimation. Each data point\nis part of a trip, which is described by a trajectory. While there are 5734 trips, a set of the trips' trajectories can\nfurthermore be associated with a sensor. There are 160 registered sensors which may be directly associated wit a\nvehicle. Additional information about the vehicle (type, etc.) is provided as well.   Picture 1: Distribution of trips per sensor (german).     Picture 2: Distribution of trips per month in 2016 and 2017 (german).",
            "title": "Envirocar Dataset"
        },
        {
            "location": "/data-sources/socio-economic/landuse/",
            "text": "Land use data\n\u00b6\n\n\n\n\nTodo\n\n\nHannes (LUBW)\n\n\n\n\nATKIS\n\u00b6\n\n\n\n\nATKIS land use data (multiple options possible)\n\n\nshapefiles in a directory\n\n\ndata in Accumulo/Exasolution\n\n\n\n\n\n\n\n\nALKIS\n\u00b6",
            "title": "Land use data"
        },
        {
            "location": "/data-sources/socio-economic/landuse/#land-use-data",
            "text": "Todo  Hannes (LUBW)",
            "title": "Land use data"
        },
        {
            "location": "/data-sources/socio-economic/landuse/#atkis",
            "text": "ATKIS land use data (multiple options possible)  shapefiles in a directory  data in Accumulo/Exasolution",
            "title": "ATKIS"
        },
        {
            "location": "/data-sources/socio-economic/landuse/#alkis",
            "text": "",
            "title": "ALKIS"
        },
        {
            "location": "/data-sources/socio-economic/newyork-taxi/",
            "text": "New York Taxi Drives\n\u00b6\n\n\n\n\nTodo\n\n\nMatthias Frank (FZI)\n\n\n\n\n\n\nNew York taxi drives\n\n\n2GB/month \u2192 for years 2009-2015 potentially ~160GB of storage space\n\n\nmultiple options possible - TODO:Matthias\n\n\nbunch of CSV files in a directories organized per year\n\n\npoints stored in Accumulo\n\n\npoints stored in Exasolution",
            "title": "New York Taxi Drives"
        },
        {
            "location": "/data-sources/socio-economic/newyork-taxi/#new-york-taxi-drives",
            "text": "Todo  Matthias Frank (FZI)    New York taxi drives  2GB/month \u2192 for years 2009-2015 potentially ~160GB of storage space  multiple options possible - TODO:Matthias  bunch of CSV files in a directories organized per year  points stored in Accumulo  points stored in Exasolution",
            "title": "New York Taxi Drives"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/",
            "text": "Traffic Incidents\n\u00b6\n\n\nFoundations\n\u00b6\n\n\nThe traffic incidents contained for various analytical use cases within the BigGIS projects are gathered via the Bing API (Documentation is online: \nhttps://msdn.microsoft.com/en-us/library/hh441726.aspx\n). The JSON API provides data in real time about traffic incidents (Accidents, traffic congestion, construction etc.). It supports requests using either bounding boxes or route location codes. For our data gathering, we used bounding boxes worldwide and gathered the data in near real time in our database. The API returns coordinates (lat, lon), type of incident, severity (slow-down, all lanes closed etc.), estimated delay as well as start time and estimated end time. All gathered worldwide Bing Traffic Data is available in the BigGIS Database hosted at the University of Konstanz. The table values values are documented below. \n\n\nStatistics\n\u00b6\n\n\nThe database contains 3,7 million entries, each containing a unique ID, location information, start and end time as well as describing meta information such as a textual description, the type of incident and the severity. \n\n\nIncident Types\n\u00b6\n\n\nInteger value in Range 1-11 with the following mappings:\n\n\n1: Accident\n\n\n2: Congestion\n\n\n3: DisabledVehicle\n\n\n4: MassTransit\n\n\n5: Miscellaneous\n\n\n6: OtherNews\n\n\n7: PlannedEvent\n\n\n8: RoadHazard\n\n\n9: Construction\n\n\n10: Alert\n\n\n11: Weather\n\n\nSeverity\n\u00b6\n\n\nInteger value in Range 1-4:\n\n\n1: LowImpact\n\n\n2: Minor\n\n\n3: Moderate\n\n\n4: Serious",
            "title": "Traffic Incidents"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#traffic-incidents",
            "text": "",
            "title": "Traffic Incidents"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#foundations",
            "text": "The traffic incidents contained for various analytical use cases within the BigGIS projects are gathered via the Bing API (Documentation is online:  https://msdn.microsoft.com/en-us/library/hh441726.aspx ). The JSON API provides data in real time about traffic incidents (Accidents, traffic congestion, construction etc.). It supports requests using either bounding boxes or route location codes. For our data gathering, we used bounding boxes worldwide and gathered the data in near real time in our database. The API returns coordinates (lat, lon), type of incident, severity (slow-down, all lanes closed etc.), estimated delay as well as start time and estimated end time. All gathered worldwide Bing Traffic Data is available in the BigGIS Database hosted at the University of Konstanz. The table values values are documented below.",
            "title": "Foundations"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#statistics",
            "text": "The database contains 3,7 million entries, each containing a unique ID, location information, start and end time as well as describing meta information such as a textual description, the type of incident and the severity.",
            "title": "Statistics"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#incident-types",
            "text": "Integer value in Range 1-11 with the following mappings:  1: Accident  2: Congestion  3: DisabledVehicle  4: MassTransit  5: Miscellaneous  6: OtherNews  7: PlannedEvent  8: RoadHazard  9: Construction  10: Alert  11: Weather",
            "title": "Incident Types"
        },
        {
            "location": "/data-sources/socio-economic/traffic-incidents/#severity",
            "text": "Integer value in Range 1-4:  1: LowImpact  2: Minor  3: Moderate  4: Serious",
            "title": "Severity"
        },
        {
            "location": "/demos/",
            "text": "About demos\n\u00b6\n\n\nIn this section you can find demos of various BigGIS components.\n\n\nEach demo consists of:\n\n\n\n\nA short introduction of the problem being solved.\n\n\nReferences to related methods, algorithms and models (from section \nMethods\n)\n\n\nExplanation of related code samples.\n\n\nStep-by-step tutorial, how the demo can be executed.\n\n\n\n\nDemos to be included later\n\u00b6\n\n\n\n\nExasol: demo of using a spatial index\n\n\nExasol: demo of using a virtual schema\n\n\nExasol: demo of using R-connector",
            "title": "About demos"
        },
        {
            "location": "/demos/#about-demos",
            "text": "In this section you can find demos of various BigGIS components.  Each demo consists of:   A short introduction of the problem being solved.  References to related methods, algorithms and models (from section  Methods )  Explanation of related code samples.  Step-by-step tutorial, how the demo can be executed.",
            "title": "About demos"
        },
        {
            "location": "/demos/#demos-to-be-included-later",
            "text": "Exasol: demo of using a spatial index  Exasol: demo of using a virtual schema  Exasol: demo of using R-connector",
            "title": "Demos to be included later"
        },
        {
            "location": "/demos/firefighting/",
            "text": "Firefighting\n\u00b6\n\n\nMotivation\n\u00b6\n\n\nIn the last decades the methods of firefighting have become a lot more effective. Still, the access to a sufficient\namount of water is often limited. Fighting a major fire requires several thousand liters of water per minute and so the\nwater tanks of firetrucks are empty in a matter of minutes. Here the real challenge begins: \nfinding hydrants as\nquickly as possible\n, which can supply a sufficient amount of water and at the same time are close by, while taking\ninto account:\n\n\n\n\nthe height difference, \n\n\nreducing water pressure. \n\n\n\n\nWe developed a tool that can help solving this complex situation. OpenStreetMap data is used as basis data for our\ncalculations.  The aim of our tool is to help the fire officer in charge, to make the best decision to provide the\nnecessary amount of water as quickly as possible.\n\n\nFeatures\n\u00b6\n\n\nAfter marking an incident scene on a map, the most suitable hydrant is shown on the map. In addition, the position for\nthe line of hoses is marked as well as the position of potentially needed mobile water pumps, providing the right\npressure in hilly surroundings.\n\n\nOn a side bar on the right you can interactively add the amount of water needed. The user can also choose between three\ndifferent varieties to get water, taking into account other possibilities for hydrants, hoses and pumps. After having\nchosen a solution, the corresponding elevation profile will be shown on the side bar, indicating the incident scene, the\nhydrant and the position and distance of the individual pumps.\n\n\nIt is possible to mark certain items on the map as blocked, e.g. hydrants, roads etc. leading to a new calculation of\nthe best solution without using the blocked item. This is very often necessary as hydrants might be blocked by a parking\ncar or roads are not accessible because of construction sites.",
            "title": "Firefighting"
        },
        {
            "location": "/demos/firefighting/#firefighting",
            "text": "",
            "title": "Firefighting"
        },
        {
            "location": "/demos/firefighting/#motivation",
            "text": "In the last decades the methods of firefighting have become a lot more effective. Still, the access to a sufficient\namount of water is often limited. Fighting a major fire requires several thousand liters of water per minute and so the\nwater tanks of firetrucks are empty in a matter of minutes. Here the real challenge begins:  finding hydrants as\nquickly as possible , which can supply a sufficient amount of water and at the same time are close by, while taking\ninto account:   the height difference,   reducing water pressure.    We developed a tool that can help solving this complex situation. OpenStreetMap data is used as basis data for our\ncalculations.  The aim of our tool is to help the fire officer in charge, to make the best decision to provide the\nnecessary amount of water as quickly as possible.",
            "title": "Motivation"
        },
        {
            "location": "/demos/firefighting/#features",
            "text": "After marking an incident scene on a map, the most suitable hydrant is shown on the map. In addition, the position for\nthe line of hoses is marked as well as the position of potentially needed mobile water pumps, providing the right\npressure in hilly surroundings.  On a side bar on the right you can interactively add the amount of water needed. The user can also choose between three\ndifferent varieties to get water, taking into account other possibilities for hydrants, hoses and pumps. After having\nchosen a solution, the corresponding elevation profile will be shown on the side bar, indicating the incident scene, the\nhydrant and the position and distance of the individual pumps.  It is possible to mark certain items on the map as blocked, e.g. hydrants, roads etc. leading to a new calculation of\nthe best solution without using the blocked item. This is very often necessary as hydrants might be blocked by a parking\ncar or roads are not accessible because of construction sites.",
            "title": "Features"
        },
        {
            "location": "/demos/gas-detect/",
            "text": "Responsible person for this section\n\n\nAlexander Groeschel\n\n\n\n\nGas Cloud Detection\n\u00b6\n\n\n\n\nChlorophyll-Erkennung im Befliegungsexperiment\n\n\nUnsichtbare Schadgaswolke (IR-Bereich)\n\n\nHei\u00dfe oder kalte Gase/Gegenst\u00e4nde leicht zu erkennen\n\n\nGase mit Umgebungstemperatur im IR-Bild nicht zu erkennen.\n\n\nSubtraktion von georeferenzierten und zus\u00e4tzlich positionskorrigierten Bildern aus Zeitreihen macht kleine \n  Abweichungen in Bildern sichtbar \u2192 Wolken unsichtbarer Gase im Bild sichtbar\n\n\n\n\nBefliegungskampagne am 15./16.07.17\n\u00b6\n\n\n\n\nAnalyse von Gaswolken aus der Luft\n\n\nTools:\n\n\nIR/RGB-Kamera\n\n\nHyperspektralkamera\n\n\n\n\n\n\n\n\n\n\nEtablierung einer Funkstrecke\n\n\n\u00dcbertragung von Flugplan-Daten/Bildergebnissen\n\n\n\n\n\n\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nDisaster Management\n\n\nSmart City",
            "title": "Gas Cloud Detection"
        },
        {
            "location": "/demos/gas-detect/#gas-cloud-detection",
            "text": "Chlorophyll-Erkennung im Befliegungsexperiment  Unsichtbare Schadgaswolke (IR-Bereich)  Hei\u00dfe oder kalte Gase/Gegenst\u00e4nde leicht zu erkennen  Gase mit Umgebungstemperatur im IR-Bild nicht zu erkennen.  Subtraktion von georeferenzierten und zus\u00e4tzlich positionskorrigierten Bildern aus Zeitreihen macht kleine \n  Abweichungen in Bildern sichtbar \u2192 Wolken unsichtbarer Gase im Bild sichtbar",
            "title": "Gas Cloud Detection"
        },
        {
            "location": "/demos/gas-detect/#befliegungskampagne-am-15160717",
            "text": "Analyse von Gaswolken aus der Luft  Tools:  IR/RGB-Kamera  Hyperspektralkamera      Etablierung einer Funkstrecke  \u00dcbertragung von Flugplan-Daten/Bildergebnissen",
            "title": "Befliegungskampagne am 15./16.07.17"
        },
        {
            "location": "/demos/gas-detect/#related-scenarios",
            "text": "Disaster Management  Smart City",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/gas-predict/",
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nGas Cloud Prediction\n\u00b6\n\n\n\n\nModeling of gas clouds and their dispersion over time.\n\n\nJulian's bachelor student (maybe text from his bc-thesis?)",
            "title": "Gas Cloud Prediction"
        },
        {
            "location": "/demos/gas-predict/#gas-cloud-prediction",
            "text": "Modeling of gas clouds and their dispersion over time.  Julian's bachelor student (maybe text from his bc-thesis?)",
            "title": "Gas Cloud Prediction"
        },
        {
            "location": "/demos/heatstress/",
            "text": "Heatstress Routing App\n\u00b6\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/path-optimizer\n\n\n\n\nRelated Scenarios: \nSmart City\n\n\nThe back-end exposes a simple REST-API, that can be used for routing or to find\nthe optimal point in time. The API can be accessed on \nhttp://localhost:8080/heatstressrouting/api/v1\n.\nJSON is supported as the only output format.\n\n\nThe following sections describe the API in detail.\n\n\nServer information\n\u00b6\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/info\n\n\nReturns some information about the running service, e.g. the supported area and time range\n\n\nParameters\n\u00b6\n\n\nThe \n/info\n site takes no parameters.\n\n\nReturns\n\u00b6\n\n\nReturns some information about the running service (see sample response below):\n\n\n\n\nbbox\n: the bounding box of the area supported by the service as an array of \n[min_lat, min_lng, max_lat, max_lng]\n.\n\n\ntime_range\n: the time range supported by the service, given as time stamps of the form \n2014-08-23T00:00\n.\n\n\nplace_types\n: a list of place types supported by the optimal time api\n\n\n\n\nExample\n\u00b6\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/info\n\n\n\nSample Response:\n\n{\n\n  \n\"service\"\n:\n \n\"heat stress routing\"\n,\n\n  \n\"version\"\n:\n \n\"0.0.1-SNAPSHOT\"\n,\n\n  \n\"build_time\"\n:\n \n\"2016-09-27T07:50:42Z\"\n,\n\n  \n\"bbox\"\n:\n \n[\n\n    \n48.99\n,\n\n    \n8.385\n,\n\n    \n49.025\n,\n\n    \n8.435\n\n  \n],\n\n  \n\"time_range\"\n:\n \n{\n\n    \n\"from\"\n:\n \n\"2014-08-23T00:00\"\n,\n\n    \n\"to\"\n:\n \n\"2016-02-23T23:00\"\n\n  \n},\n\n  \n\"place_types\"\n:\n \n[\n\n    \n\"bakery\"\n,\n\n    \n\"taxi\"\n,\n\n    \n\"post_office\"\n,\n\n    \n\"ice_cream\"\n,\n\n    \n\"dentist\"\n,\n\n    \n\"post_box\"\n,\n\n    \n\"supermarket\"\n,\n\n    \n\"toilets\"\n,\n\n    \n\"bank\"\n,\n\n    \n\"cafe\"\n,\n\n    \n\"police\"\n,\n\n    \n\"doctors\"\n,\n\n    \n\"pharmacy\"\n,\n\n    \n\"drinking_water\"\n,\n\n    \n\"atm\"\n,\n\n    \n\"clinic\"\n,\n\n    \n\"kiosk\"\n,\n\n    \n\"hospital\"\n,\n\n    \n\"chemist\"\n,\n\n    \n\"fast_food\"\n\n  \n]\n\n\n}\n\n\n\n\nRouting\n\u00b6\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/routing\n\n\nComputes the optimal route (regarding heat stress) between a start and a destination at a given time.\n\n\nParameters\n\u00b6\n\n\nThe \n/routing\n api supports the following parameter (some are optional):\n\n\n\n\nstart\n: the start point as pair of a latitude value and longitude value (in that order)\n  seperated by a comma, e.g. \nstart=49.0118083,8.4251357\n.\n\n\ndestination\n: the destination as pair of a latitude value and longitude value (in that order)\n  separated by a comma, e.g. \ndestination=49.0126868,8.4065707\n. \n\n\ntime\n: the date and time the optimal route should be searched for; a time stamp of the form\n  \nYYYY-MM-DDTHH:MM:SS\n, e.g. \ntime=2015-08-31T10:00:00\n. The value must be in the time range\n  returned by \n/info\n (see \nabove\n).\n\n\nweighting\n (optional): the weightings to be used; a comma seperated list of the supported\n  weightings (\nshortest\n, \nheatindex\n and \ntemperature\n), e.g. \nweighting=shortest,heatindex,temperature\n;\n  the default is \nweighting=shortest,heatindex\n; the results for the \nshortest\n weighting are always\n  returned, even if the value is omited in the weighings list.\n\n\n\n\nReturns\n\u00b6\n\n\nThe path and some other information for each of the weightings:\n\n\n\n\nstatus\n: the status of the request; \nOK\n is everthing is okay, \nBAD_REQUEST\n if a invalid request was send or \nINTERNAL_SERVER_ERROR\n if an internal error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\nresults\n: the results for each weighting:\n\n\nweighting\n: the weighting used for that result (see parameter \nweighting\n above).\n\n\nstart\n: the coordinates of the start point as array of \n[lat, lng]\n.\n\n\ndestination\n: the coordinates of the destination as array of \n[lat, lng]\n.\n\n\ndistance\n: the length of the route in meter.\n\n\nduration\n: the walking time in milli seconds.\n\n\nroute_weights\n: the route weights of the selected weightings for the route.\n\n\npath\n: the geometry of the path found; an array of points, were each point is an array of [lat, lng]`.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/routing?start=49.0118083,8.4251357&destination=49.0126868,8.4065707&time=2015-08-31T10:00:00&weighting=shortest,heatindex,temperature\n\n\n\nSample Response:\n\n{\n\n  \n\"status\"\n:\n \n\"OK\"\n,\n\n  \n\"status_code\"\n:\n \n200\n,\n\n  \n\"results\"\n:\n \n{\n\n    \n\"shortest\"\n:\n \n{\n\n      \n\"weighting\"\n:\n \n\"shortest\"\n,\n\n      \n\"start\"\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \n\"destination\"\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \n\"distance\"\n:\n \n1698.2989202985977\n,\n\n      \n\"duration\"\n:\n \n1222740\n,\n\n      \n\"route_weights\"\n:\n \n{\n\n        \n\"temperature\"\n:\n \n50903.955833052285\n,\n\n        \n\"heatindex\"\n:\n \n50892.20496302502\n,\n\n        \n\"shortest\"\n:\n \n1698.2989202985977\n\n      \n},\n\n      \n\"path\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \n\"heatindex\"\n:\n \n{\n\n      \n\"weighting\"\n:\n \n\"heatindex\"\n,\n\n      \n\"start\"\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \n\"destination\"\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \n\"distance\"\n:\n \n1901.8839202985973\n,\n\n      \n\"duration\"\n:\n \n1369323\n,\n\n      \n\"route_weights\"\n:\n \n{\n\n        \n\"temperature\"\n:\n \n51868.74807902536\n,\n\n        \n\"heatindex\"\n:\n \n51098.277424417196\n,\n\n        \n\"shortest\"\n:\n \n1901.8839202985978\n\n      \n},\n\n      \n\"path\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \n\"temperature\"\n:\n \n{\n\n      \n\"weighting\"\n:\n \n\"temperature\"\n,\n\n      \n\"start\"\n:\n \n[\n\n        \n49.0118083\n,\n\n        \n8.4251357\n\n      \n],\n\n      \n\"destination\"\n:\n \n[\n\n        \n49.0126868\n,\n\n        \n8.4065707\n\n      \n],\n\n      \n\"distance\"\n:\n \n1901.8839202985973\n,\n\n      \n\"duration\"\n:\n \n1369323\n,\n\n      \n\"route_weights\"\n:\n \n{\n\n        \n\"temperature\"\n:\n \n51868.74807902536\n,\n\n        \n\"heatindex\"\n:\n \n51098.277424417196\n,\n\n        \n\"shortest\"\n:\n \n1901.8839202985978\n\n      \n},\n\n      \n\"path\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.01225359765262\n,\n\n          \n8.425994591995952\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.01272775130067\n,\n\n          \n8.406514897340614\n\n        \n]\n\n      \n]\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\nOptimal time\n\u00b6\n\n\nURL:\n \nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime\n\n\nPerformce a nearby search for a given start point and computes for every place that fulfills\na specified criterion an optimal point in time, i.e. the time with the minimal heat stress.\n\n\nParameters\n\u00b6\n\n\nThe \n/optimaltime\n api supports the following parameter (some are optional):\n\n\n\n\nstart\n: the start point as pair of a latitude value and longitude value (in that order) seperated by a comma, e.g. \nstart=49.0118083,8.4251357\n. \n\n\ntime\n: the date and time the optimal route should be searched for; a time stamp of the form \nYYYY-MM-DDTHH:MM:SS\n, e.g. \ntime=2015-08-31T10:00:00\n. The value must be in the time range returned by \n/info\n (see \nabove\n).\n\n\nplace_type\n: the place type to search for; a comma seperated list of supported place types, e.g. \nplace_type=supermarket,chemist\n; a complete list of supported place list can be queried using the \ninfo\n api (see \nabove\n). Currently the following place tyes are supported: \nbakery\n, \ntaxi\n, \npost_office\n, \nice_cream\n, \ndentist\n, \npost_box\n, \nsupermarket\n, \ntoilets\n, \nbank\n, \ncafe\n, \npolice\n, \ndoctors\n, \npharmacy\n, \ndrinking_water\n, \natm\n, \nclinic\n, \nkiosk\n, \nhospital\n, \nchemist\n, \nfast_food\n. The place types are mapped to the corresponding \nshop\n respectively \namenity\n tags.\n\n\nmax_results\n (optional): the maximum number of results to consider for the nearby search (an positive integer), e.g. \nmax_results=10\n; the default value is 5.\n\n\nmax_distance\n (optional): the maximum direct distance (as the crow flies) between the start point and the place in meter, e.g. \nmax_distance=500.0\n; the default value is 1000.0 meter.\n\n\ntime_buffer\n (optional): the minimum time needed at the place (in minutes), i.e. the optimal time is chossen so that the place is opened for a least \ntime_buffer\n when the user arrives, e.g. \ntime_buffer=30\n; the default value is 15 miniutes.\n\n\nearliest_time\n (optional): the earliest desired time, either a time stamp, e.g. \nearliest_time=2015-08-31T09:00\n or the string \nnull\n (case is ignored); the default value is \nnull\n. If both \nearliest_time\n and \nlatest_time\n are specified, \nearliest_time\n must be before \nlatest_time\n.\n\n\nlatest_time\n (optional): the latest desired time, either a time stamp, e.g. \nlatest_time=2015-08-31T17:00\n or the string \nnull\n (case is ignored); the default value is \nnull\n. If both \nearliest_time\n and \nlatest_time\n are specified, \nearliest_time\n must be before \nlatest_time\n; \nlatest_time\n must be after \ntime\n.\n\n\n\n\nReturns\n\u00b6\n\n\nThe optimal point in time for each place found in the specified radius ranked by the optimal-value:\n\n\n\n\nstatus\n: the status of the request; \nOK\n if everthing is okay, \nNO_REULTS\n if not results were found, \nBAD_REQUEST\n if a invalid request was send to the server or \nINTERNAL_SERVER_ERROR\n if an internal server error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\n\n\nresults\n: the result for each place found during the nearby search:\n\n\n\n\nrank\n: the rank of the place according to the optimal value (were 1 is the best rank).\n\n\nname\n: the name of the place.\n\n\nosm_id\n: the \nOpenStreetMap Node ID\n of the place.\n\n\nlocation\n: the coordinates of the places as an array of \n[lat, lng]\n.\n\n\nopening_hours\n: the opening hours of the place; the format specification can be found \nhere\n.\n\n\noptimal_time\n: the optimal point in time found for that place, e.g. \n2015-08-31T20:00\n\n\noptimal_value\n: the optimal value found for the place; the value considering the heat stress acording to steadman's heatindex \n(Steadmean, 1979)\n as well as the distance between the start and the place.\n\n\ndistance\n: the length of the optimal path (see \nRouting\n above) from the start to the place in meter.\n\n\nduration\n: the time needed to walk from the start to the place (in milli seconds).\n\n\npath_optimal\n: the geometry of the optimal path (see \nRouting\n above).\n\n\ndistance_shortest\n: the length of the shortest path between the start and the place (in meter).\n\n\nduration_shortest\n: the time needed to walk the shortest path between the start and the place (in milli seconds).\n\n\npath_optimal\n: the geometry of the shortest path (see \nRouting\n above).\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nSample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime?start=49.0118083,8.4251357&time=2015-08-31T10:00:00&place_type=supermarket&max_distance=1000&max_results=5&time_buffer=15&earliest_time=2015-08-31T09:00:00&latest_time=2015-08-31T20:00:00\n\n\n\nSample Response:\n\n{\n\n  \n\"status\"\n:\n \n\"OK\"\n,\n\n  \n\"status_code\"\n:\n \n200\n,\n\n  \n\"results\"\n:\n \n[\n\n    \n{\n\n      \n\"rank\"\n:\n \n1\n,\n\n      \n\"name\"\n:\n \n\"Rewe City\"\n,\n\n      \n\"osm_id\"\n:\n \n897615202\n,\n\n      \n\"location\"\n:\n \n[\n\n        \n49.0096613\n,\n\n        \n8.4237272\n\n      \n],\n\n      \n\"opening_hours\"\n:\n \n\"Mo-Sa 07:00-22:00; Su,PH off\"\n,\n\n      \n\"optimal_time\"\n:\n \n\"2015-08-31T20:00\"\n,\n\n      \n\"optimal_value\"\n:\n \n12515.36230258099\n,\n\n      \n\"distance\"\n:\n \n539.1839746027457\n,\n\n      \n\"duration\"\n:\n \n388207\n,\n\n      \n\"path_optimal\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00954480942009\n,\n\n          \n8.423681942364334\n\n        \n]\n\n      \n],\n\n      \n\"distance_shortest\"\n:\n \n468.99728441805115\n,\n\n      \n\"duration_shortest\"\n:\n \n337669\n,\n\n      \n\"path_shortest\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00954480942009\n,\n\n          \n8.423681942364334\n\n        \n]\n\n      \n]\n\n    \n},\n\n    \n{\n\n      \n\"rank\"\n:\n \n2\n,\n\n      \n\"name\"\n:\n \n\"Oststadt Super-Bio-Markt\"\n,\n\n      \n\"osm_id\"\n:\n \n931682116\n,\n\n      \n\"location\"\n:\n \n[\n\n        \n49.009433\n,\n\n        \n8.4234214\n\n      \n],\n\n      \n\"opening_hours\"\n:\n \n\"Mo-Fr 09:00-13:00,14:00-18:30; Sa 09:00-13:00\"\n,\n\n      \n\"optimal_time\"\n:\n \n\"2015-08-31T18:09:19.199\"\n,\n\n      \n\"optimal_value\"\n:\n \n14318.962937267655\n,\n\n      \n\"distance\"\n:\n \n473.346750294328\n,\n\n      \n\"duration\"\n:\n \n340801\n,\n\n      \n\"path_optimal\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00944708743373\n,\n\n          \n8.4235711322383\n\n        \n]\n\n      \n],\n\n      \n\"distance_shortest\"\n:\n \n473.346750294328\n,\n\n      \n\"duration_shortest\"\n:\n \n340801\n,\n\n      \n\"path_shortest\"\n:\n \n[\n\n        \n[\n\n          \n49.01190564077309\n,\n\n          \n8.4250437301107\n\n        \n],\n\n        \n[\n\n          \n49.011967867880344\n,\n\n          \n8.425196821060705\n\n        \n],\n\n        \n//\n \npoints\n \nomitted\n \n...\n\n        \n[\n\n          \n49.00944708743373\n,\n\n          \n8.4235711322383\n\n        \n]\n\n      \n]\n\n    \n}\n\n  \n]\n\n\n}\n\n\n\n\nError messages\n\u00b6\n\n\nIf an error occurs, e.g. because a bade request were send to the server or an internal server errors occurs, the server is sending a JSON response with the following content:\n\n\n\n\nstatus\n: the status of the request; \nOK\n if everthing is okay, \nNO_REULTS\n if not results were found, \nBAD_REQUEST\n if a invalid request was send to the server or \nINTERNAL_SERVER_ERROR\n if an internal server error occoured.\n\n\nstatus_code\n: the HTTP status code returned.\n\n\nmessages\n: an array of human readable error messages.\n\n\n\n\nExample\n\u00b6\n\n\nExample Request:\n\nhttp://localhost:8080/heatstressrouting/api/v1/optimaltime?start=Schloss,%20Karlsruhe&time=2015-08-31T10:00:00&place_type=supermarket\n\n\n\nExample Response:\n\n{\n\n  \n\"status\"\n:\n \n\"BAD_REQUEST\"\n,\n\n  \n\"status_code\"\n:\n \n400\n,\n\n  \n\"messages\"\n:\n \n[\n\n    \n\"start (Schloss, Karlsruhe) could not be parsed: failed to parse coordinate; 'start' must be a pair of latitude and longitude seperated by a comma (','), e.g. '49.0118083,8.4251357')\"\n\n  \n]\n\n\n}\n\n\n\n\nReferences\n\u00b6\n\n\n\n\nReference\n\n\nSteadman, R. G. \nThe Assessment of Sultriness. Part I: A Temperature-Humidity Index Based on Human Physiology and Clothing.\n\nScience Journal of Applied Meteorology, 1979, 18, 861-873, DOI: 10.1175/1520-0450(1979)018<0861:TAOSPI>2.0.CO;2",
            "title": "Heatstress Routing App"
        },
        {
            "location": "/demos/heatstress/#heatstress-routing-app",
            "text": "Note  Related repository is  https://github.com/biggis-project/path-optimizer   Related Scenarios:  Smart City  The back-end exposes a simple REST-API, that can be used for routing or to find\nthe optimal point in time. The API can be accessed on  http://localhost:8080/heatstressrouting/api/v1 .\nJSON is supported as the only output format.  The following sections describe the API in detail.",
            "title": "Heatstress Routing App"
        },
        {
            "location": "/demos/heatstress/#server-information",
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/info  Returns some information about the running service, e.g. the supported area and time range",
            "title": "Server information"
        },
        {
            "location": "/demos/heatstress/#parameters",
            "text": "The  /info  site takes no parameters.",
            "title": "Parameters"
        },
        {
            "location": "/demos/heatstress/#returns",
            "text": "Returns some information about the running service (see sample response below):   bbox : the bounding box of the area supported by the service as an array of  [min_lat, min_lng, max_lat, max_lng] .  time_range : the time range supported by the service, given as time stamps of the form  2014-08-23T00:00 .  place_types : a list of place types supported by the optimal time api",
            "title": "Returns"
        },
        {
            "location": "/demos/heatstress/#example",
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/info  Sample Response: { \n   \"service\" :   \"heat stress routing\" , \n   \"version\" :   \"0.0.1-SNAPSHOT\" , \n   \"build_time\" :   \"2016-09-27T07:50:42Z\" , \n   \"bbox\" :   [ \n     48.99 , \n     8.385 , \n     49.025 , \n     8.435 \n   ], \n   \"time_range\" :   { \n     \"from\" :   \"2014-08-23T00:00\" , \n     \"to\" :   \"2016-02-23T23:00\" \n   }, \n   \"place_types\" :   [ \n     \"bakery\" , \n     \"taxi\" , \n     \"post_office\" , \n     \"ice_cream\" , \n     \"dentist\" , \n     \"post_box\" , \n     \"supermarket\" , \n     \"toilets\" , \n     \"bank\" , \n     \"cafe\" , \n     \"police\" , \n     \"doctors\" , \n     \"pharmacy\" , \n     \"drinking_water\" , \n     \"atm\" , \n     \"clinic\" , \n     \"kiosk\" , \n     \"hospital\" , \n     \"chemist\" , \n     \"fast_food\" \n   ]  }",
            "title": "Example"
        },
        {
            "location": "/demos/heatstress/#routing",
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/routing  Computes the optimal route (regarding heat stress) between a start and a destination at a given time.",
            "title": "Routing"
        },
        {
            "location": "/demos/heatstress/#parameters_1",
            "text": "The  /routing  api supports the following parameter (some are optional):   start : the start point as pair of a latitude value and longitude value (in that order)\n  seperated by a comma, e.g.  start=49.0118083,8.4251357 .  destination : the destination as pair of a latitude value and longitude value (in that order)\n  separated by a comma, e.g.  destination=49.0126868,8.4065707 .   time : the date and time the optimal route should be searched for; a time stamp of the form\n   YYYY-MM-DDTHH:MM:SS , e.g.  time=2015-08-31T10:00:00 . The value must be in the time range\n  returned by  /info  (see  above ).  weighting  (optional): the weightings to be used; a comma seperated list of the supported\n  weightings ( shortest ,  heatindex  and  temperature ), e.g.  weighting=shortest,heatindex,temperature ;\n  the default is  weighting=shortest,heatindex ; the results for the  shortest  weighting are always\n  returned, even if the value is omited in the weighings list.",
            "title": "Parameters"
        },
        {
            "location": "/demos/heatstress/#returns_1",
            "text": "The path and some other information for each of the weightings:   status : the status of the request;  OK  is everthing is okay,  BAD_REQUEST  if a invalid request was send or  INTERNAL_SERVER_ERROR  if an internal error occoured.  status_code : the HTTP status code returned.  results : the results for each weighting:  weighting : the weighting used for that result (see parameter  weighting  above).  start : the coordinates of the start point as array of  [lat, lng] .  destination : the coordinates of the destination as array of  [lat, lng] .  distance : the length of the route in meter.  duration : the walking time in milli seconds.  route_weights : the route weights of the selected weightings for the route.  path : the geometry of the path found; an array of points, were each point is an array of [lat, lng]`.",
            "title": "Returns"
        },
        {
            "location": "/demos/heatstress/#example_1",
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/routing?start=49.0118083,8.4251357&destination=49.0126868,8.4065707&time=2015-08-31T10:00:00&weighting=shortest,heatindex,temperature  Sample Response: { \n   \"status\" :   \"OK\" , \n   \"status_code\" :   200 , \n   \"results\" :   { \n     \"shortest\" :   { \n       \"weighting\" :   \"shortest\" , \n       \"start\" :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       \"destination\" :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       \"distance\" :   1698.2989202985977 , \n       \"duration\" :   1222740 , \n       \"route_weights\" :   { \n         \"temperature\" :   50903.955833052285 , \n         \"heatindex\" :   50892.20496302502 , \n         \"shortest\" :   1698.2989202985977 \n       }, \n       \"path\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     }, \n     \"heatindex\" :   { \n       \"weighting\" :   \"heatindex\" , \n       \"start\" :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       \"destination\" :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       \"distance\" :   1901.8839202985973 , \n       \"duration\" :   1369323 , \n       \"route_weights\" :   { \n         \"temperature\" :   51868.74807902536 , \n         \"heatindex\" :   51098.277424417196 , \n         \"shortest\" :   1901.8839202985978 \n       }, \n       \"path\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     }, \n     \"temperature\" :   { \n       \"weighting\" :   \"temperature\" , \n       \"start\" :   [ \n         49.0118083 , \n         8.4251357 \n       ], \n       \"destination\" :   [ \n         49.0126868 , \n         8.4065707 \n       ], \n       \"distance\" :   1901.8839202985973 , \n       \"duration\" :   1369323 , \n       \"route_weights\" :   { \n         \"temperature\" :   51868.74807902536 , \n         \"heatindex\" :   51098.277424417196 , \n         \"shortest\" :   1901.8839202985978 \n       }, \n       \"path\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.01225359765262 , \n           8.425994591995952 \n         ], \n         //   points   omitted   ... \n         [ \n           49.01272775130067 , \n           8.406514897340614 \n         ] \n       ] \n     } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/demos/heatstress/#optimal-time",
            "text": "URL:   http://localhost:8080/heatstressrouting/api/v1/optimaltime  Performce a nearby search for a given start point and computes for every place that fulfills\na specified criterion an optimal point in time, i.e. the time with the minimal heat stress.",
            "title": "Optimal time"
        },
        {
            "location": "/demos/heatstress/#parameters_2",
            "text": "The  /optimaltime  api supports the following parameter (some are optional):   start : the start point as pair of a latitude value and longitude value (in that order) seperated by a comma, e.g.  start=49.0118083,8.4251357 .   time : the date and time the optimal route should be searched for; a time stamp of the form  YYYY-MM-DDTHH:MM:SS , e.g.  time=2015-08-31T10:00:00 . The value must be in the time range returned by  /info  (see  above ).  place_type : the place type to search for; a comma seperated list of supported place types, e.g.  place_type=supermarket,chemist ; a complete list of supported place list can be queried using the  info  api (see  above ). Currently the following place tyes are supported:  bakery ,  taxi ,  post_office ,  ice_cream ,  dentist ,  post_box ,  supermarket ,  toilets ,  bank ,  cafe ,  police ,  doctors ,  pharmacy ,  drinking_water ,  atm ,  clinic ,  kiosk ,  hospital ,  chemist ,  fast_food . The place types are mapped to the corresponding  shop  respectively  amenity  tags.  max_results  (optional): the maximum number of results to consider for the nearby search (an positive integer), e.g.  max_results=10 ; the default value is 5.  max_distance  (optional): the maximum direct distance (as the crow flies) between the start point and the place in meter, e.g.  max_distance=500.0 ; the default value is 1000.0 meter.  time_buffer  (optional): the minimum time needed at the place (in minutes), i.e. the optimal time is chossen so that the place is opened for a least  time_buffer  when the user arrives, e.g.  time_buffer=30 ; the default value is 15 miniutes.  earliest_time  (optional): the earliest desired time, either a time stamp, e.g.  earliest_time=2015-08-31T09:00  or the string  null  (case is ignored); the default value is  null . If both  earliest_time  and  latest_time  are specified,  earliest_time  must be before  latest_time .  latest_time  (optional): the latest desired time, either a time stamp, e.g.  latest_time=2015-08-31T17:00  or the string  null  (case is ignored); the default value is  null . If both  earliest_time  and  latest_time  are specified,  earliest_time  must be before  latest_time ;  latest_time  must be after  time .",
            "title": "Parameters"
        },
        {
            "location": "/demos/heatstress/#returns_2",
            "text": "The optimal point in time for each place found in the specified radius ranked by the optimal-value:   status : the status of the request;  OK  if everthing is okay,  NO_REULTS  if not results were found,  BAD_REQUEST  if a invalid request was send to the server or  INTERNAL_SERVER_ERROR  if an internal server error occoured.  status_code : the HTTP status code returned.   results : the result for each place found during the nearby search:   rank : the rank of the place according to the optimal value (were 1 is the best rank).  name : the name of the place.  osm_id : the  OpenStreetMap Node ID  of the place.  location : the coordinates of the places as an array of  [lat, lng] .  opening_hours : the opening hours of the place; the format specification can be found  here .  optimal_time : the optimal point in time found for that place, e.g.  2015-08-31T20:00  optimal_value : the optimal value found for the place; the value considering the heat stress acording to steadman's heatindex  (Steadmean, 1979)  as well as the distance between the start and the place.  distance : the length of the optimal path (see  Routing  above) from the start to the place in meter.  duration : the time needed to walk from the start to the place (in milli seconds).  path_optimal : the geometry of the optimal path (see  Routing  above).  distance_shortest : the length of the shortest path between the start and the place (in meter).  duration_shortest : the time needed to walk the shortest path between the start and the place (in milli seconds).  path_optimal : the geometry of the shortest path (see  Routing  above).",
            "title": "Returns"
        },
        {
            "location": "/demos/heatstress/#example_2",
            "text": "Sample Request: http://localhost:8080/heatstressrouting/api/v1/optimaltime?start=49.0118083,8.4251357&time=2015-08-31T10:00:00&place_type=supermarket&max_distance=1000&max_results=5&time_buffer=15&earliest_time=2015-08-31T09:00:00&latest_time=2015-08-31T20:00:00  Sample Response: { \n   \"status\" :   \"OK\" , \n   \"status_code\" :   200 , \n   \"results\" :   [ \n     { \n       \"rank\" :   1 , \n       \"name\" :   \"Rewe City\" , \n       \"osm_id\" :   897615202 , \n       \"location\" :   [ \n         49.0096613 , \n         8.4237272 \n       ], \n       \"opening_hours\" :   \"Mo-Sa 07:00-22:00; Su,PH off\" , \n       \"optimal_time\" :   \"2015-08-31T20:00\" , \n       \"optimal_value\" :   12515.36230258099 , \n       \"distance\" :   539.1839746027457 , \n       \"duration\" :   388207 , \n       \"path_optimal\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00954480942009 , \n           8.423681942364334 \n         ] \n       ], \n       \"distance_shortest\" :   468.99728441805115 , \n       \"duration_shortest\" :   337669 , \n       \"path_shortest\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00954480942009 , \n           8.423681942364334 \n         ] \n       ] \n     }, \n     { \n       \"rank\" :   2 , \n       \"name\" :   \"Oststadt Super-Bio-Markt\" , \n       \"osm_id\" :   931682116 , \n       \"location\" :   [ \n         49.009433 , \n         8.4234214 \n       ], \n       \"opening_hours\" :   \"Mo-Fr 09:00-13:00,14:00-18:30; Sa 09:00-13:00\" , \n       \"optimal_time\" :   \"2015-08-31T18:09:19.199\" , \n       \"optimal_value\" :   14318.962937267655 , \n       \"distance\" :   473.346750294328 , \n       \"duration\" :   340801 , \n       \"path_optimal\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00944708743373 , \n           8.4235711322383 \n         ] \n       ], \n       \"distance_shortest\" :   473.346750294328 , \n       \"duration_shortest\" :   340801 , \n       \"path_shortest\" :   [ \n         [ \n           49.01190564077309 , \n           8.4250437301107 \n         ], \n         [ \n           49.011967867880344 , \n           8.425196821060705 \n         ], \n         //   points   omitted   ... \n         [ \n           49.00944708743373 , \n           8.4235711322383 \n         ] \n       ] \n     } \n   ]  }",
            "title": "Example"
        },
        {
            "location": "/demos/heatstress/#error-messages",
            "text": "If an error occurs, e.g. because a bade request were send to the server or an internal server errors occurs, the server is sending a JSON response with the following content:   status : the status of the request;  OK  if everthing is okay,  NO_REULTS  if not results were found,  BAD_REQUEST  if a invalid request was send to the server or  INTERNAL_SERVER_ERROR  if an internal server error occoured.  status_code : the HTTP status code returned.  messages : an array of human readable error messages.",
            "title": "Error messages"
        },
        {
            "location": "/demos/heatstress/#example_3",
            "text": "Example Request: http://localhost:8080/heatstressrouting/api/v1/optimaltime?start=Schloss,%20Karlsruhe&time=2015-08-31T10:00:00&place_type=supermarket  Example Response: { \n   \"status\" :   \"BAD_REQUEST\" , \n   \"status_code\" :   400 , \n   \"messages\" :   [ \n     \"start (Schloss, Karlsruhe) could not be parsed: failed to parse coordinate; 'start' must be a pair of latitude and longitude seperated by a comma (','), e.g. '49.0118083,8.4251357')\" \n   ]  }",
            "title": "Example"
        },
        {
            "location": "/demos/heatstress/#references",
            "text": "Reference  Steadman, R. G.  The Assessment of Sultriness. Part I: A Temperature-Humidity Index Based on Human Physiology and Clothing. \nScience Journal of Applied Meteorology, 1979, 18, 861-873, DOI: 10.1175/1520-0450(1979)018<0861:TAOSPI>2.0.CO;2",
            "title": "References"
        },
        {
            "location": "/demos/urban-heat-islands/",
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nUrban Heat Islands\n\u00b6\n\n\n\n\nTodo\n\n\n\n\nTranslate to English\n\n\nadd links to related github repos\n\n\nadd some images (but not too many)\n\n\nadd links to related papers\n\n\ndescribe APIs especially from the end-users' point of view\n\n\n\n\n\n\n\n\nTemperaturinseln in Karlsruhe und anderen St\u00e4dten\n\n\nTemperaturdaten: Volunteered geographic data (z.B. wunderground.com)\n\n\n\n\nKorrelation zwischen Bereichen in verschiedenen St\u00e4dten mit \u00e4hnlichem Temperaturverlauf \u2192 Zugang zu Ursachen f\u00fcr Temperaturentwicklung\n\n\n\n\n\n\nVorstellung der Heat-Islands-Analyse\n\n\n\n\nWetterstationen\n\n\nTechnik: Sensebox (\nhttps://sensebox.de\n)\n\n\nBeispielstation (\nhttps://opensensemap.org/explore/58b4354fe53e0b001251119d\n)\n\n\nHotspotanalyse (SoH, Stability of Hotspots):\n\n\nAbh\u00e4ngigkeit des Auftretens von Hotspots von der Aggregationsstufe\n\n\nAusblick:\n\n\nSensorfusion in Kooperation mit SDIL (smart data innovation lab)\n\n\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nSmart City",
            "title": "Urban Heat Islands"
        },
        {
            "location": "/demos/urban-heat-islands/#urban-heat-islands",
            "text": "Todo   Translate to English  add links to related github repos  add some images (but not too many)  add links to related papers  describe APIs especially from the end-users' point of view     Temperaturinseln in Karlsruhe und anderen St\u00e4dten  Temperaturdaten: Volunteered geographic data (z.B. wunderground.com)   Korrelation zwischen Bereichen in verschiedenen St\u00e4dten mit \u00e4hnlichem Temperaturverlauf \u2192 Zugang zu Ursachen f\u00fcr Temperaturentwicklung    Vorstellung der Heat-Islands-Analyse   Wetterstationen  Technik: Sensebox ( https://sensebox.de )  Beispielstation ( https://opensensemap.org/explore/58b4354fe53e0b001251119d )  Hotspotanalyse (SoH, Stability of Hotspots):  Abh\u00e4ngigkeit des Auftretens von Hotspots von der Aggregationsstufe  Ausblick:  Sensorfusion in Kooperation mit SDIL (smart data innovation lab)",
            "title": "Urban Heat Islands"
        },
        {
            "location": "/demos/urban-heat-islands/#related-scenarios",
            "text": "Smart City",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/drone-flight-planning/",
            "text": "Optimal Flight Plan for Drones\n\u00b6\n\n\nMotivation\n\u00b6\n\n\nDrones equipped with remote sensing systems are rapidly becoming a valuable tool for emergency response units as they\ncan quickly provide reliable information after an incident. However, controlling these vehicles and interpreting the\nobtained measurements also imposes further demands on the response squad in an already stressful situation. Therefore,\nintegrated and automatic mission planning tools are needed that plan drone routes such that the maximum amount of\ninformation is provided within as short a delay as possible.\n\n\nVision: Interactive UAV Deployment\n\u00b6\n\n\nOur vision for an interactive tool supporting automated UAV deployment is depicted in Figure 1. In this scenario, we\nconsider a drone equipped with a hyperspectral camera that is used to identify hazardous substances. This drone is\nstarted immediately after the response unit arrives on scene. First, it takes an initial survey image at high altitude,\nproviding a first overview of the target region. This serves as an initial data source for planning the remainder of the\nflight. It also represents the first data source for identifying and prediction gas clouds. The drone then proceeds with\nits tour, making more measurements as it travels across the target area. During this time, the planned route is modified\nin an iterative process: The newly obtained images are prepared and preprocessed in order to be transmitted to the base\nstation. They are used as updates for the gas cloud prediction and are communicated to the user. The drone mission can\nfurthermore be adapted based on these measurements, e.g. moving to areas where hazardous gases are most likely to be\npresent. Furthermore, the user can define additional POIs such as public building\n\n\n\n\nFigure 1: Interactive drone deployment\n\n\n\n\n\n\nInformative Path Planning\n\u00b6\n\n\nThe planning problem consists of determining a path \nP\nP\n, i.e. an ordered sequence of target locations for the drone. The\ncost of this path \ncost(P)\ncost(P)\n, e.g. in terms of required flight time, needs to be less or equal to the maximum budget\n\nC_{max}\nC_{max}\n available. Specifically, the drone needs to return before its energy supply is depleted. The objective is to\nfind a path \nP\nP\n that maximises some measure of informativeness \nI(P)\nI(P)\n. Hence, the fundamental planning problem is to\nbalance resource consumption and information gain during the trip. This problem can be formulated as follows:\n\n\n\n\n\\max I(P) \\\\\ns.t. cost(P) <= C_{max}\n\n\n\\max I(P) \\\\\ns.t. cost(P) <= C_{max}\n\n\n\n\nSeveral objective functions \nI(P)\nI(P)\n exist to describe this informativeness.\nTwo of the most frequently used ones in literature are\n- Maximising the achieved reduction of the prediction error variance,\n- Maximising the mutual information between observed and unobserved locations.\n\n\nThe prediction error variance increases the further away an unobserved location is from a selected sensing location.\nSimilarly, the mutual information typically decreases the farther locations are from one another. Hence, both measures\nfavour sensing locations that are well distributed across the region of interest. However, both of these approaches\nrequire a reliable model of the observed phenomenon in form of a Gaussian Process in order to determine error variances\nand mutual information. In practice, it is unlikely that an accurate process model is available before the start of the\ndrone flight. We therefore use an approximate model of spatial correlations. While this approximation is not an accurate\ndescription of the observed phenomenon, it can be computed more efficiently, which is highly relevant for a\ntime-constraint application such as the use case considered here. Similar to the two measures discussed above, it\nconsiders paths as \u201cinformative\u201d that consist of sensing locations distributed as evenly as possible within the target\nregion.\n\n\nHeuristic Solution Approach\n\u00b6\n\n\nRather than solving an exact model, we use a heuristic solution approach for determining drone routes. An initial path\nis determined using a simple construction strategy. Then, this path is iteratively improved by destroying and rebuilding\nparts of a solution using an Adaptive Large Neighbourhood Search approach. These approaches have been shown to quickly\ndetermine high-quality solutions even for large vehicle routing problems with several hundred candidate sensing\nlocations.\n\n\nEvaluation and Results\n\u00b6\n\n\nWe evaluate the proposed model and solution approach in an extensive simulative study on artificial spatially correlated\nrandom fields. This way, we can consider scenarios larger than any that would be possible in preliminary test flights,\nand validate the robustness and effectiveness of our approach on a wide range of possible situations.\n\n\nFigure 2 gives one example of such a scenario: We consider a target region of 1200 x 1200 m, and a drone with a maximum\nflight time of 900 seconds at a maximum speed of 10 m/s. The left image represents the actual distribution of values\nwithin the target region, i.e. the simulated smoke cloud. Below, we present the results of the solution approach. The\nmiddle image indicates the remaining prediction error variance. Here we can recognize selected sensing locations for the\ndrone flight dark red. At these locations, the drone obtains accurate information about the random field, which means\nthat error variance is minimal. For the remainder of the target region, error variance increases with increasing\ndistance to the next sensing locations, since there is less information is available about these locations. The\nresulting prediction is indicated on the right hand side. Obviously, the information gained about the underlying process\nis rather accurate, as the prediction closely follows the actual distribution. Hence, even though the maximum flight\ntime would be insufficient to survey each location within the target area, it still allows to accurately detect the\nsmoke cloud and to provide detailed information about the spatial distribution of hazardous gases.\n\n\n\n\nFigure 2: Autocorrelated field, predicted values and prediction quality.",
            "title": "Optimal Flight Plan for Drones"
        },
        {
            "location": "/demos/drone-flight-planning/#optimal-flight-plan-for-drones",
            "text": "",
            "title": "Optimal Flight Plan for Drones"
        },
        {
            "location": "/demos/drone-flight-planning/#motivation",
            "text": "Drones equipped with remote sensing systems are rapidly becoming a valuable tool for emergency response units as they\ncan quickly provide reliable information after an incident. However, controlling these vehicles and interpreting the\nobtained measurements also imposes further demands on the response squad in an already stressful situation. Therefore,\nintegrated and automatic mission planning tools are needed that plan drone routes such that the maximum amount of\ninformation is provided within as short a delay as possible.",
            "title": "Motivation"
        },
        {
            "location": "/demos/drone-flight-planning/#vision-interactive-uav-deployment",
            "text": "Our vision for an interactive tool supporting automated UAV deployment is depicted in Figure 1. In this scenario, we\nconsider a drone equipped with a hyperspectral camera that is used to identify hazardous substances. This drone is\nstarted immediately after the response unit arrives on scene. First, it takes an initial survey image at high altitude,\nproviding a first overview of the target region. This serves as an initial data source for planning the remainder of the\nflight. It also represents the first data source for identifying and prediction gas clouds. The drone then proceeds with\nits tour, making more measurements as it travels across the target area. During this time, the planned route is modified\nin an iterative process: The newly obtained images are prepared and preprocessed in order to be transmitted to the base\nstation. They are used as updates for the gas cloud prediction and are communicated to the user. The drone mission can\nfurthermore be adapted based on these measurements, e.g. moving to areas where hazardous gases are most likely to be\npresent. Furthermore, the user can define additional POIs such as public building   Figure 1: Interactive drone deployment",
            "title": "Vision: Interactive UAV Deployment"
        },
        {
            "location": "/demos/drone-flight-planning/#informative-path-planning",
            "text": "The planning problem consists of determining a path  P P , i.e. an ordered sequence of target locations for the drone. The\ncost of this path  cost(P) cost(P) , e.g. in terms of required flight time, needs to be less or equal to the maximum budget C_{max} C_{max}  available. Specifically, the drone needs to return before its energy supply is depleted. The objective is to\nfind a path  P P  that maximises some measure of informativeness  I(P) I(P) . Hence, the fundamental planning problem is to\nbalance resource consumption and information gain during the trip. This problem can be formulated as follows:   \\max I(P) \\\\\ns.t. cost(P) <= C_{max}  \\max I(P) \\\\\ns.t. cost(P) <= C_{max}   Several objective functions  I(P) I(P)  exist to describe this informativeness.\nTwo of the most frequently used ones in literature are\n- Maximising the achieved reduction of the prediction error variance,\n- Maximising the mutual information between observed and unobserved locations.  The prediction error variance increases the further away an unobserved location is from a selected sensing location.\nSimilarly, the mutual information typically decreases the farther locations are from one another. Hence, both measures\nfavour sensing locations that are well distributed across the region of interest. However, both of these approaches\nrequire a reliable model of the observed phenomenon in form of a Gaussian Process in order to determine error variances\nand mutual information. In practice, it is unlikely that an accurate process model is available before the start of the\ndrone flight. We therefore use an approximate model of spatial correlations. While this approximation is not an accurate\ndescription of the observed phenomenon, it can be computed more efficiently, which is highly relevant for a\ntime-constraint application such as the use case considered here. Similar to the two measures discussed above, it\nconsiders paths as \u201cinformative\u201d that consist of sensing locations distributed as evenly as possible within the target\nregion.",
            "title": "Informative Path Planning"
        },
        {
            "location": "/demos/drone-flight-planning/#heuristic-solution-approach",
            "text": "Rather than solving an exact model, we use a heuristic solution approach for determining drone routes. An initial path\nis determined using a simple construction strategy. Then, this path is iteratively improved by destroying and rebuilding\nparts of a solution using an Adaptive Large Neighbourhood Search approach. These approaches have been shown to quickly\ndetermine high-quality solutions even for large vehicle routing problems with several hundred candidate sensing\nlocations.",
            "title": "Heuristic Solution Approach"
        },
        {
            "location": "/demos/drone-flight-planning/#evaluation-and-results",
            "text": "We evaluate the proposed model and solution approach in an extensive simulative study on artificial spatially correlated\nrandom fields. This way, we can consider scenarios larger than any that would be possible in preliminary test flights,\nand validate the robustness and effectiveness of our approach on a wide range of possible situations.  Figure 2 gives one example of such a scenario: We consider a target region of 1200 x 1200 m, and a drone with a maximum\nflight time of 900 seconds at a maximum speed of 10 m/s. The left image represents the actual distribution of values\nwithin the target region, i.e. the simulated smoke cloud. Below, we present the results of the solution approach. The\nmiddle image indicates the remaining prediction error variance. Here we can recognize selected sensing locations for the\ndrone flight dark red. At these locations, the drone obtains accurate information about the random field, which means\nthat error variance is minimal. For the remainder of the target region, error variance increases with increasing\ndistance to the next sensing locations, since there is less information is available about these locations. The\nresulting prediction is indicated on the right hand side. Obviously, the information gained about the underlying process\nis rather accurate, as the prediction closely follows the actual distribution. Hence, even though the maximum flight\ntime would be insufficient to survey each location within the target area, it still allows to accurately detect the\nsmoke cloud and to provide detailed information about the spatial distribution of hazardous gases.   Figure 2: Autocorrelated field, predicted values and prediction quality.",
            "title": "Evaluation and Results"
        },
        {
            "location": "/demos/enviro-car/",
            "text": "EnviroCar (Smart City)\n\u00b6\n\n\nThe human desire for mobility is an observable global trend. To visit a place of choice fast, cheaply and safely\nnowadays has become a basic need to mankind. This increasing demand for mobility is reflected by the worldwide\nCO\n2\n emissions where motorized individual transport is contributing an essential portion to the global\nCO\n2\n emissions such that it was the second largest sector in 2014. The health effects caused by the world's\nincreasing traffic are, additionally, not only restricted to the emitted pollutants and emission gases but also to\nfactors such as noise pollution. In a recent study, for example, about 40% of the population in EU countries is exposed\nto road traffic noise at levels exceeding 55 db(A).\n\n\nMotivation\n\u00b6\n\n\nIn this work, we by propose a holistic view to the analysis of mobility data by helping experts to develop and realize\nsustainable mobility concepts. We consider visual analysis as the natural way to interact with this kind of mobility\ndata supporting analysts to create, refine and verify hypotheses. Using data from the citizen-science platform\nenviroCar, we contribute a Visual Analytics system allowing analysts to leverage their background knowledge in the\nanalysis process.\n\n\nShort section about available data\n\u00b6\n\n\nThe used dataset contains approximately around 1.7 million data points. Each data record contains 24 attributes\nreflecting sensor values of the vehicle (e.g. speed, rpm, ...) as well as a CO\n2\n estimation. Each data point is part of a\ntrip, which is described by a trajectory. While there are 5734 trips, a set of the trips' trajectories can furthermore\nbe associated with a sensor. There are 160 registered sensors which may be directly associated wit a vehicle. Additional\ninformation about the vehicle (type, etc.) is provided as well.\n\n\n\n\nPicture 1: Distribution of trips per sensor (german).\n\n\n\n\n\n\n\n\nPicture 2: Distribution of trips per month in 2016 and 2017 (german).\n\n\n\n\n\n\nVisual Analysis of Traffic Data\n\u00b6\n\n\n\n\nPicture 3: Visualization using a clock metaphor - development of the average CO\n2\n emissions.\n\n\n\n\n\n\n\n\nPicture 4: Trip trajectories\n\n\n\nExact course of all trip trajectories in the german city of M\u00f6nchengladbach between 0 am and 6 am \n(left)\n.\nMagnification of the selected region highlighted by the red rectangle \n(right)\n.\n\n\n\n\n\n\nPicture 5: Intersection\n\n\n\n\n(a)\n Dot Map of a frequently traveled intersection.\n\n(b)\n Dense pixel display visualization of the same intersection.\n    The data points are colored and sorted based after their CO\n2\n emissions.\n\n(c)\n Data points sorted based on their speed.\n\n(d)\n Data points sorted based on their engine speed.\n\n\n\n\nVisual Interactive Logging and Provenance\n\u00b6\n\n\nPerforming interactions in various visualizations and chaining together various filtering, aggregation and navigation\nsteps can quickly become overwhelming and can lead to the analyst losing the overview of the analysis process. This can\nlead to frustration with the system and also a loss of trust in the findings. Another important aspect is the issue of\nreproducibility of results arising from the inherent complexity of the system. To cope with both of these problems, we\nneed to enable the analyst to maintain an overview of the previous analysis process.\n\n\n\n\nPicture 6: Systematic approach for the realization of interactive visual logging.\n\n\n\n\n\n\nPaper\n\u00b6\n\n\nMore info about this project can be found in our paper \"Visual Analysis of Urban Traffic Data based on High-Resolution\nand High-Dimensional Environmental Sensor Data\" (currently under submission)\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nSmart City\n\n\nEnvironment",
            "title": "EnviroCar (Smart City)"
        },
        {
            "location": "/demos/enviro-car/#envirocar-smart-city",
            "text": "The human desire for mobility is an observable global trend. To visit a place of choice fast, cheaply and safely\nnowadays has become a basic need to mankind. This increasing demand for mobility is reflected by the worldwide\nCO 2  emissions where motorized individual transport is contributing an essential portion to the global\nCO 2  emissions such that it was the second largest sector in 2014. The health effects caused by the world's\nincreasing traffic are, additionally, not only restricted to the emitted pollutants and emission gases but also to\nfactors such as noise pollution. In a recent study, for example, about 40% of the population in EU countries is exposed\nto road traffic noise at levels exceeding 55 db(A).",
            "title": "EnviroCar (Smart City)"
        },
        {
            "location": "/demos/enviro-car/#motivation",
            "text": "In this work, we by propose a holistic view to the analysis of mobility data by helping experts to develop and realize\nsustainable mobility concepts. We consider visual analysis as the natural way to interact with this kind of mobility\ndata supporting analysts to create, refine and verify hypotheses. Using data from the citizen-science platform\nenviroCar, we contribute a Visual Analytics system allowing analysts to leverage their background knowledge in the\nanalysis process.",
            "title": "Motivation"
        },
        {
            "location": "/demos/enviro-car/#short-section-about-available-data",
            "text": "The used dataset contains approximately around 1.7 million data points. Each data record contains 24 attributes\nreflecting sensor values of the vehicle (e.g. speed, rpm, ...) as well as a CO 2  estimation. Each data point is part of a\ntrip, which is described by a trajectory. While there are 5734 trips, a set of the trips' trajectories can furthermore\nbe associated with a sensor. There are 160 registered sensors which may be directly associated wit a vehicle. Additional\ninformation about the vehicle (type, etc.) is provided as well.   Picture 1: Distribution of trips per sensor (german).     Picture 2: Distribution of trips per month in 2016 and 2017 (german).",
            "title": "Short section about available data"
        },
        {
            "location": "/demos/enviro-car/#visual-analysis-of-traffic-data",
            "text": "Picture 3: Visualization using a clock metaphor - development of the average CO 2  emissions.     Picture 4: Trip trajectories  \nExact course of all trip trajectories in the german city of M\u00f6nchengladbach between 0 am and 6 am  (left) .\nMagnification of the selected region highlighted by the red rectangle  (right) .    Picture 5: Intersection   (a)  Dot Map of a frequently traveled intersection. (b)  Dense pixel display visualization of the same intersection.\n    The data points are colored and sorted based after their CO 2  emissions. (c)  Data points sorted based on their speed. (d)  Data points sorted based on their engine speed.",
            "title": "Visual Analysis of Traffic Data"
        },
        {
            "location": "/demos/enviro-car/#visual-interactive-logging-and-provenance",
            "text": "Performing interactions in various visualizations and chaining together various filtering, aggregation and navigation\nsteps can quickly become overwhelming and can lead to the analyst losing the overview of the analysis process. This can\nlead to frustration with the system and also a loss of trust in the findings. Another important aspect is the issue of\nreproducibility of results arising from the inherent complexity of the system. To cope with both of these problems, we\nneed to enable the analyst to maintain an overview of the previous analysis process.   Picture 6: Systematic approach for the realization of interactive visual logging.",
            "title": "Visual Interactive Logging and Provenance"
        },
        {
            "location": "/demos/enviro-car/#paper",
            "text": "More info about this project can be found in our paper \"Visual Analysis of Urban Traffic Data based on High-Resolution\nand High-Dimensional Environmental Sensor Data\" (currently under submission)",
            "title": "Paper"
        },
        {
            "location": "/demos/enviro-car/#related-scenarios",
            "text": "Smart City  Environment",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/hotspot-analysis/",
            "text": "Hotspot analysis (using G*)\n\u00b6\n\n\nProblem definition\n\u00b6\n\n\n\n\nWe have a map, in this case a map of land surface temperatures\n\n\nWe want to find hotspots, i.e., areas on map that are \nsignificantly different from their surrounding area\n.\n\n\nWe want to use Getis-Ord G* statistic as the tool for finding the hotspots\n\n\nsee section \nStandards Getis-ord\n\n\n\n\n\n\nWe want to parallelize the computation in our Spark cluster.\n\n\nsee section \nRasterized Getis-ord\n\n\n\n\n\n\n\n\n\n\nHotspot analysis using geotrellis\n\u00b6\n\n\nIn this section we show a simplified version of the hotspot analysis.\nWe use the \nGeotrellis\n library to achieve the parallelization.\nSome assumptions are:\n\n\n\n\nwe use 2-dimenational data (only the spatial part without the time component)\n\n\nwe store our data as a layer of tiles in geotrellis catalog (distributed raster)\n\n\nour hotspot analysis uses the standard G* with variable window\n\n\n\n\nFirst, we need to express the G* formula in terms of the map algebra operations.\n\n\nScala code\n\u00b6\n\n\nFull source code can be found in our github repository: \n\nbiggis-project/biggis-landuse\n\n\n// typical type definition used by geotrellis\n\n\ntype\n \nSpatialRDD\n \n=\n \nRDD\n[(\nSpatialKey\n, \nTile\n)]\n\n                  \nwith\n \nMetadata\n[\nTileLayerMetadata\n[\nSpatialKey\n]]\n\n\n\ndef\n \ngetisord\n(\nrdd\n:\n \nSpatialRDD\n,\n \nweightMatrix\n:\n \nKernel\n,\n\n             \nglobMean\n:\nDouble\n,\n \nglobStdev\n:\nDouble\n,\n \nnumPixels\n:\nLong\n)\n:\n \nSpatialRDD\n \n=\n \n{\n\n\n  \nval\n \nwcells\n:\n \nArray\n[\nDouble\n]\n \n=\n \nweightMatrix\n.\ntile\n.\ntoArrayDouble\n\n  \nval\n \nsumW\n:\n \nDouble\n \n=\n \nwcells\n.\nsum\n\n  \nval\n \nsumW2\n:\n \nDouble\n \n=\n \nwcells\n.\nmap\n(\nx\n \n=>\n \nx\n \n*\n \nx\n).\nsum\n\n\n  \n// variables used in the getis-ord formula\n\n  \nval\n \nA\n:\n \nDouble\n \n=\n \nglobalMean\n \n*\n \nsumW\n\n  \nval\n \nB\n:\n \nDouble\n \n=\n \nglobalStdev\n \n*\n \nMath\n.\nsqrt\n((\nnumPixels\n \n*\n \nsumW2\n \n-\n \nsumW\n \n*\n \nsumW\n)\n \n/\n \n(\nnumPixels\n \n-\n \n1\n))\n\n\n  \nrdd\n.\nwithContext\n \n{\n\n    \n_\n.\nbufferTiles\n(\nweightMatrix\n.\nextent\n)\n\n      \n.\nmapValues\n \n{\n \ntileWithCtx\n \n=>\n\n        \ntileWithCtx\n.\ntile\n\n          \n.\nfocalSum\n(\nweightMatrix\n,\n \nSome\n(\ntileWithCtx\n.\ntargetArea\n))\n \n// focal op.\n\n          \n.\nmapDouble\n \n{\n \nx\n \n=>\n \n(\nx\n \n-\n \nA\n)\n \n/\n \nB\n \n}\n \n// local op.\n\n      \n}\n\n  \n}\n\n\n}\n\n\n\n\n\nLet's assume, we already have the following variables:\n\n\n\n\nlayerReader\n: helper class to query tiles from geotrellis catalog/layer,\n\n\nlayerId\n: ID of the raster layer used as input raster,\n\n\nkernelRadius\n: size of the weight matrix (how many pixels)\n\n\n\n\n// RDD (distributed dataset from Apache Spark) representing all tiles in the layer \n\n\nval\n \nqueryResult\n:\n \nSpatialRDD\n \n=\n\n  \nlayerReader\n.\nread\n[\nSpatialKey\n, \nTile\n, \nTileLayerMetadata\n[\nSpatialKey\n]](\nlayerId\n)\n\n\n\n// here, we use a circular kernel as a weight matrix\n\n\nval\n \nweightMatrix\n \n=\n \nKernel\n.\ncircle\n(\nkernelRadius\n,\n\n                                 \nqueryResult\n.\nmetadata\n.\ncellwidth\n,\n\n                                 \nkernelRadius\n)\n\n\n\n// use precomputed histogram metadata (stored in zoom level 0 inside the layer)\n\n\nval\n \nstats\n \n=\n \nqueryResult\n.\nhistogram\n.\nstatistics\n\n\nrequire\n(\nstats\n.\nnonEmpty\n)\n\n\n\nval\n \nStatistics\n(\n_\n,\n \nglobMean\n,\n \n_\n,\n \n_\n,\n \nglobStdev\n,\n \n_\n,\n \n_\n)\n \n=\n \nstats\n.\nget\n\n\nval\n \nnumPixels\n \n=\n \nqueryResult\n.\nhistogram\n.\ntotalCount\n\n\n\n// apply the parallelized getis ord\n\n\nval\n \noutRdd\n \n=\n \ngetisord\n(\nqueryResult\n,\n \nweightMatrix\n,\n \nglobMean\n,\n \nglobStdev\n,\n \nnumPixels\n)\n\n\n\n\n\nThe result \noutRdd\n is an RDD (distributed dataset from Apache Spark) that can be further processed\nor stored as a new layer in geotrellis catalog.",
            "title": "Hotspot analysis (using G*)"
        },
        {
            "location": "/demos/hotspot-analysis/#hotspot-analysis-using-g",
            "text": "",
            "title": "Hotspot analysis (using G*)"
        },
        {
            "location": "/demos/hotspot-analysis/#problem-definition",
            "text": "We have a map, in this case a map of land surface temperatures  We want to find hotspots, i.e., areas on map that are  significantly different from their surrounding area .  We want to use Getis-Ord G* statistic as the tool for finding the hotspots  see section  Standards Getis-ord    We want to parallelize the computation in our Spark cluster.  see section  Rasterized Getis-ord",
            "title": "Problem definition"
        },
        {
            "location": "/demos/hotspot-analysis/#hotspot-analysis-using-geotrellis",
            "text": "In this section we show a simplified version of the hotspot analysis.\nWe use the  Geotrellis  library to achieve the parallelization.\nSome assumptions are:   we use 2-dimenational data (only the spatial part without the time component)  we store our data as a layer of tiles in geotrellis catalog (distributed raster)  our hotspot analysis uses the standard G* with variable window   First, we need to express the G* formula in terms of the map algebra operations.",
            "title": "Hotspot analysis using geotrellis"
        },
        {
            "location": "/demos/hotspot-analysis/#scala-code",
            "text": "Full source code can be found in our github repository:  biggis-project/biggis-landuse  // typical type definition used by geotrellis  type   SpatialRDD   =   RDD [( SpatialKey ,  Tile )] \n                   with   Metadata [ TileLayerMetadata [ SpatialKey ]]  def   getisord ( rdd :   SpatialRDD ,   weightMatrix :   Kernel , \n              globMean : Double ,   globStdev : Double ,   numPixels : Long ) :   SpatialRDD   =   { \n\n   val   wcells :   Array [ Double ]   =   weightMatrix . tile . toArrayDouble \n   val   sumW :   Double   =   wcells . sum \n   val   sumW2 :   Double   =   wcells . map ( x   =>   x   *   x ). sum \n\n   // variables used in the getis-ord formula \n   val   A :   Double   =   globalMean   *   sumW \n   val   B :   Double   =   globalStdev   *   Math . sqrt (( numPixels   *   sumW2   -   sumW   *   sumW )   /   ( numPixels   -   1 )) \n\n   rdd . withContext   { \n     _ . bufferTiles ( weightMatrix . extent ) \n       . mapValues   {   tileWithCtx   => \n         tileWithCtx . tile \n           . focalSum ( weightMatrix ,   Some ( tileWithCtx . targetArea ))   // focal op. \n           . mapDouble   {   x   =>   ( x   -   A )   /   B   }   // local op. \n       } \n   }  }   Let's assume, we already have the following variables:   layerReader : helper class to query tiles from geotrellis catalog/layer,  layerId : ID of the raster layer used as input raster,  kernelRadius : size of the weight matrix (how many pixels)   // RDD (distributed dataset from Apache Spark) representing all tiles in the layer   val   queryResult :   SpatialRDD   = \n   layerReader . read [ SpatialKey ,  Tile ,  TileLayerMetadata [ SpatialKey ]]( layerId )  // here, we use a circular kernel as a weight matrix  val   weightMatrix   =   Kernel . circle ( kernelRadius , \n                                  queryResult . metadata . cellwidth , \n                                  kernelRadius )  // use precomputed histogram metadata (stored in zoom level 0 inside the layer)  val   stats   =   queryResult . histogram . statistics  require ( stats . nonEmpty )  val   Statistics ( _ ,   globMean ,   _ ,   _ ,   globStdev ,   _ ,   _ )   =   stats . get  val   numPixels   =   queryResult . histogram . totalCount  // apply the parallelized getis ord  val   outRdd   =   getisord ( queryResult ,   weightMatrix ,   globMean ,   globStdev ,   numPixels )   The result  outRdd  is an RDD (distributed dataset from Apache Spark) that can be further processed\nor stored as a new layer in geotrellis catalog.",
            "title": "Scala code"
        },
        {
            "location": "/demos/invasive-species/",
            "text": "Responsible person for this section\n\n\n\n\nHannes M\u00fcller (LUBW)\n\n\nJohannes Kutterer (Disy)\n\n\nDaniel Seebacher (Uni Konstanz)\n\n\n\n\n\n\nInvasive species\n\u00b6\n\n\nMotiviation\n\u00b6\n\n\nInvasive species are a major cause of ecological damage and commercial losses. A current problem spreading in North\nAmerica and Europe is the vinegar fly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and healthy\nfruits and is therefore of concern to fruit growers, such as vintners.  Consequently, large amounts of data about the\noccurrence of D. suzukii have been collected in recent years. However, there is a lack of interactive methods to\ninvestigate this data.\n\n\nUsed Data Sources\n\u00b6\n\n\n\n\nATKIS/ALKIS landuse data\n\n\nCounts of trapped Drosophila suzukii published on \nVitiMeteo\n\n\nASTER Elevation Map\n\n\n\n\nData Description\n\u00b6\n\n\n\n\nFigure 1: Data provided by \nVitiMeteo\n: Distribution of vineyards and traps in Baden-Wuerttemberg\n\n\n\n\n\n\nIn the data provided by \nVitiMeteo\n are, among other things, observations of the spread of D. suzukii. This data\nconsists of trap findings of D. suzukii as well as percentage information about how many berries were infested in a\nsample taken at the station. Additionally, there is percentage information about how many eggs were found in a sample.\nThis percentage can be over 100%, if there are more egg findings than berries in a sample. These observations are\ncollected from 867 stations non-uniformly spread over Baden-Wuerttemberg. Some of them only report observations for one\nday, others report multiple observations over a time period of up to 1641 days. The observations are rather sparse and\nirregularly sampled, which makes the use of standard time series analysis techniques challenging, if not impossible.\nConsequently, an interactive visual analysis should enable researchers to interactively analyze this complex data\nsource.\n\n\nHypothesis\n\u00b6\n\n\nPopulations of Drosophila suzukii depend on food and shelter in the winter months. The animals also need certain levels\nof shadow and humidty during the summer months to prevent dehydration. Forests and bushes close to the vineyards may\ntherefore support the survival of Drosophila suzukii by balancing extreme temperatures and providing diverse sources of\nfood during winter time.\n\n\nPrediction of Infested Areas\n\u00b6\n\n\nWe enriched the data provided by \nVitiMeteo\n, by adding information about the environmental surroundings of each\nstation. First, we added the height information, which we extracted from ASTER. Second, we added the surrounding land\nusage information. Since a local spread is possible by D. suzukii itself, we extracted the land usage information in a\n5~km radius around each station. Finally, we have an 85 dimensional feature vector for each instance, consisting of the\nmonth of the year, the station height, and the surrounding land usage.\n\n\nWe end up with a rather imbalanced data set with four times as many negative examples as positive ones. This can cause\nproblems since many machine learning algorithms depend on the assumption that the given data set is balanced. Although\nmachine learning techniques exist which can deal with imbalanced data sets, such as the Robust Decision Trees, we want\nto employ ensemble-based classification, which is a combination of different classifiers. This allows us to improve the\nclassification performance and also to model the uncertainty of our classification, which aids people in making more\ninformed decisions. This requires the creation of a balanced data set, which we can achieve by either using\nundersampling of the majority class or oversampling of the minority class. Undersampling can be achieved by stratified\nsampling using the occurrence class as strata. However, this would remove instances from our already small data set. To\navoid this, we employ oversampling of the minority class using the Synthetic Minority Over-sampling Technique (SMOTE).\nSMOTE picks pairs of nearest neighbors in the minority class and creates artificial instances by randomly placing a\npoint on the line between the nearest neighbors until the data is balanced. Thus, allowing us to employ default machine\nlearning algorithms.\n\n\nDevelopment of the Vector Data Pipeline in BigGIS\n\u00b6\n\n\nIf you are working with geo data you are faced with two different kinds of data types: vector and raster data. Both have\ndifferent requirements for collection, processing and storing of the data. The data provided by \nVitiMeteo\n are vector\ndata. On the webpage you can find information about egg findings of D. suzukii in berries, flies catch in traps and\nobservations of the species.  For each dataset you have a geographical position, e.g., a point.\n\n\nThe vector data pipeline performs the following steps:\n\n\n\n\nCollect the data from the source\n\n\nProcess the data\n\n\nVisualize the data\n\n\n\n\n\n\nSchematic visualisation of the Vector Data Pipeline for the Drosophila suzukii data from \nVitiMeteo\n\n\n\n\n\n\n1. Collection of KEF data\n\u00b6\n\n\nFor the demo the gathering is split in two separate steps. First the data is downloaded from the website and is saved\nlocally. All further steps are run on this data local data to avoid too much traffic on the \nVitiMeteo\n web site. The\nclient for downloading the data is based on \nSpring Batch\n and\n\nSpring Boot\n. Downloaded data is saved in GeoJSON-files.\nYou can find this client here: \nhttps://github.com/DisyInformationssysteme/biggis-download-kef-data\n\n\nThis GeoJSON file is handed over to a Kafka producer which is the first step of the stream processing prototype. The\ncode can be found on \nhttps://github.com/DisyInformationssysteme/biggis-import-kef-data-to-kafka\n\n\n2. Processing of the KEF data\n\u00b6\n\n\nThis code in implemented using Apache Flink. The implemented jobs feature the import of GeoJSON coded sensor locations\nand corresponding time series. Sources are Kafka queues. Destination is a PostGIS/Postgres database. (see\n\nhttps://github.com/DisyInformationssysteme/biggis-streaming-vector-data\n)\n\n\n3. Visualization\n\u00b6\n\n\nJust providing users with the raw results of our prediction is not sufficient as we generate over 20.000 predictions for\nall months and vineyards in Baden-Wuerttemberg. Furthermore, the raw results do not provide spatial context. Thus, it is\nnot interpretable which makes it hard for experts to integrate their domain knowledge into the analysis process. Hence,\nwe need visualization to help experts to identify spatial and temporal patterns easily. To achieve this, we follow the\nvisual information seeking mantra of Ben Shneiderman: \"Overview first, zoom and filter, details on demand\"\n\n\n\n\nFigure 2: Glyph-visualization of temporal-spatial event predictions as proposed by Seebacher et al.\n\n\n\n\n\n\nWe build a geographic information system, using a map as the basis for interaction and spatio-temporal analysis. We\nconsider the familiarity of domain experts with this kind of visualization as an additional benefit. We visualize our\npredictions on the corresponding position on the map so that users are immediately aware of the geographic context.\nAdditionally, combining our geographic visualization with a visualization of the temporal predictions into a single\nvisualization is more effective, since this requires less cognitive effort for the users. Existing related systems such\nas BirdVis offer heat map overview visualizations. However, as we want to investigate the distribution of a species over\ntime, we designed a map overlay consisting of several glyphs. This partially preserves the geographic context while\nglyph can be used to encode additional contextual information. The goal of our glyph is to visualize whether a certain\nregion is endangered or not. Consequently, we visually encode the classification results of a specific month represented\nby its time segment. The basic design of a time segment is depicted in Figure 2. Therefore, we make use of the interior\nof the respective time segment to represent the classification results of the ensemble-classifiers. For each month we\nhave a distribution of safe and endangered vineyards, according to the classification. Since the number of vineyards\nstays the same over all months for each glyph, we fill the area of the time segments according to the ratio of the\nbinary outcome (endangered, not endangered). This technique results in a radial glyph similar to a stacked bar chart\nshowing fractions of the whole. We use the colors red (endangered) and blue (not endangered), as derived from the\nwarm-cold color scale to distinguish the outcome. We provide additional functionality, such as semantic zoom, dynamic\naggregation and various details-on-demand data visualizations.\n\n\n\n\nFigure 3: Overview of the Drosophigator application\n\n\n\n\nDrosophigator enables experts to perform a visual analysis of spatio-temporal event predictions.\n\n\n\n\nUse Cases\n\u00b6\n\n\nWe want to highlight how visualization can help domain experts to gain insights about the spread dynamics of D. suzukii.\nWe show the usefulness of our system by demonstrating how domain experts can investigate hypotheses using Drosophigator.\nTherefore, we investigated two recently proposed assumptions about the time of infestation by the JKI and the influences\nof environmental factors by Pelton et al.\n\n\n\n\nFigure 4: Overview glyph-visualization of all vineyards in Baden-Wuerttemberg.\n\n\n\n\nThe development over the time-segments shows that the severity of infestation and the certainty of our prediction\nincreases in late summer and stays high until the end of the year. This corroborates the hypothesis of the JKI.\n\n\n\n\n\n\nFigure 5: Comparison of the vineyards contained in two neighboring cells.\n\n\n\n\nThe upper cell (purple) exhibits an earlier infestation by D. suzukii that the lower cell (brown). The parallel\ncoordinates plot shows, that the vineyards in the upper cell have around 10% more surrounding woodland (\nWald\n) than\nthose in the lower cell. This finding strongly supports the hypothesis of Pelton et al.\n\n\n\n\nEvaluation\n\u00b6\n\n\nWe presented our system at the 6\nth\n workshop of the working group \"D. Suzukii\" on the 5\nth\n and 6\nth\n of December in Bad\nKreuznach, Germany. The goal of this workshop is the mutual exchange of knowledge between researchers and practitioners.\nOver 80 biologists, researchers, agri- and horticulturists from various countries participated in the workshop. The\nfocus of our talk was our application \nDrosophigator\n, especially the design and interpretation of the glyph as well as\nthe interaction possibilities with the system.  After the presentation of the system, a questionnaire was handed out to\nthe workshop participants where they could rate the different aspects of our application and could provide us with\nadditional information about their background. We use the results of this questionnaire to evaluate our system and\ndesign decisions.\n\n\n\n\nFigure 7: Evaluation of system feedback of all participants (n=37)\n\n\n\n\nShown are the responses of the participants on questions regarding the visualization design (\nV1, V2, V3\n), the\ninteraction design (\nI1, I2\n) and the analysis capabilities (\nA1, A2, A3, A4\n) of our system Drosophigator.\n\n\n\n\nThe results of our evaluation make it clear that there is a strong need for intuitive and interactive systems, which\nsupport the experts in their daily analysis tasks. The experts are, for the most part, very positive about\n\nDrosphigator\n. Our glyph design was comprehensible, helped them to understand the temporal occurrence of D. suzukii and\nintegrating it in a map helped them to interpret the results. Additionally, allowing for a seamless clustering of\nvineyards into larger regions is deemed important, as it allows the analysis of micro- and macroecological factors.\nHowever, experts are still divided in their opinion, whether the application can support them in their work. This is\nreflected in their opinion about the possibility to infer causes for the occurrence of D. suzukii from our application.\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nEnvironment",
            "title": "Invasive species"
        },
        {
            "location": "/demos/invasive-species/#invasive-species",
            "text": "",
            "title": "Invasive species"
        },
        {
            "location": "/demos/invasive-species/#motiviation",
            "text": "Invasive species are a major cause of ecological damage and commercial losses. A current problem spreading in North\nAmerica and Europe is the vinegar fly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and healthy\nfruits and is therefore of concern to fruit growers, such as vintners.  Consequently, large amounts of data about the\noccurrence of D. suzukii have been collected in recent years. However, there is a lack of interactive methods to\ninvestigate this data.",
            "title": "Motiviation"
        },
        {
            "location": "/demos/invasive-species/#used-data-sources",
            "text": "ATKIS/ALKIS landuse data  Counts of trapped Drosophila suzukii published on  VitiMeteo  ASTER Elevation Map",
            "title": "Used Data Sources"
        },
        {
            "location": "/demos/invasive-species/#data-description",
            "text": "Figure 1: Data provided by  VitiMeteo : Distribution of vineyards and traps in Baden-Wuerttemberg    In the data provided by  VitiMeteo  are, among other things, observations of the spread of D. suzukii. This data\nconsists of trap findings of D. suzukii as well as percentage information about how many berries were infested in a\nsample taken at the station. Additionally, there is percentage information about how many eggs were found in a sample.\nThis percentage can be over 100%, if there are more egg findings than berries in a sample. These observations are\ncollected from 867 stations non-uniformly spread over Baden-Wuerttemberg. Some of them only report observations for one\nday, others report multiple observations over a time period of up to 1641 days. The observations are rather sparse and\nirregularly sampled, which makes the use of standard time series analysis techniques challenging, if not impossible.\nConsequently, an interactive visual analysis should enable researchers to interactively analyze this complex data\nsource.",
            "title": "Data Description"
        },
        {
            "location": "/demos/invasive-species/#hypothesis",
            "text": "Populations of Drosophila suzukii depend on food and shelter in the winter months. The animals also need certain levels\nof shadow and humidty during the summer months to prevent dehydration. Forests and bushes close to the vineyards may\ntherefore support the survival of Drosophila suzukii by balancing extreme temperatures and providing diverse sources of\nfood during winter time.",
            "title": "Hypothesis"
        },
        {
            "location": "/demos/invasive-species/#prediction-of-infested-areas",
            "text": "We enriched the data provided by  VitiMeteo , by adding information about the environmental surroundings of each\nstation. First, we added the height information, which we extracted from ASTER. Second, we added the surrounding land\nusage information. Since a local spread is possible by D. suzukii itself, we extracted the land usage information in a\n5~km radius around each station. Finally, we have an 85 dimensional feature vector for each instance, consisting of the\nmonth of the year, the station height, and the surrounding land usage.  We end up with a rather imbalanced data set with four times as many negative examples as positive ones. This can cause\nproblems since many machine learning algorithms depend on the assumption that the given data set is balanced. Although\nmachine learning techniques exist which can deal with imbalanced data sets, such as the Robust Decision Trees, we want\nto employ ensemble-based classification, which is a combination of different classifiers. This allows us to improve the\nclassification performance and also to model the uncertainty of our classification, which aids people in making more\ninformed decisions. This requires the creation of a balanced data set, which we can achieve by either using\nundersampling of the majority class or oversampling of the minority class. Undersampling can be achieved by stratified\nsampling using the occurrence class as strata. However, this would remove instances from our already small data set. To\navoid this, we employ oversampling of the minority class using the Synthetic Minority Over-sampling Technique (SMOTE).\nSMOTE picks pairs of nearest neighbors in the minority class and creates artificial instances by randomly placing a\npoint on the line between the nearest neighbors until the data is balanced. Thus, allowing us to employ default machine\nlearning algorithms.",
            "title": "Prediction of Infested Areas"
        },
        {
            "location": "/demos/invasive-species/#development-of-the-vector-data-pipeline-in-biggis",
            "text": "If you are working with geo data you are faced with two different kinds of data types: vector and raster data. Both have\ndifferent requirements for collection, processing and storing of the data. The data provided by  VitiMeteo  are vector\ndata. On the webpage you can find information about egg findings of D. suzukii in berries, flies catch in traps and\nobservations of the species.  For each dataset you have a geographical position, e.g., a point.  The vector data pipeline performs the following steps:   Collect the data from the source  Process the data  Visualize the data    Schematic visualisation of the Vector Data Pipeline for the Drosophila suzukii data from  VitiMeteo",
            "title": "Development of the Vector Data Pipeline in BigGIS"
        },
        {
            "location": "/demos/invasive-species/#1-collection-of-kef-data",
            "text": "For the demo the gathering is split in two separate steps. First the data is downloaded from the website and is saved\nlocally. All further steps are run on this data local data to avoid too much traffic on the  VitiMeteo  web site. The\nclient for downloading the data is based on  Spring Batch  and Spring Boot . Downloaded data is saved in GeoJSON-files.\nYou can find this client here:  https://github.com/DisyInformationssysteme/biggis-download-kef-data  This GeoJSON file is handed over to a Kafka producer which is the first step of the stream processing prototype. The\ncode can be found on  https://github.com/DisyInformationssysteme/biggis-import-kef-data-to-kafka",
            "title": "1. Collection of KEF data"
        },
        {
            "location": "/demos/invasive-species/#2-processing-of-the-kef-data",
            "text": "This code in implemented using Apache Flink. The implemented jobs feature the import of GeoJSON coded sensor locations\nand corresponding time series. Sources are Kafka queues. Destination is a PostGIS/Postgres database. (see https://github.com/DisyInformationssysteme/biggis-streaming-vector-data )",
            "title": "2. Processing of the KEF data"
        },
        {
            "location": "/demos/invasive-species/#3-visualization",
            "text": "Just providing users with the raw results of our prediction is not sufficient as we generate over 20.000 predictions for\nall months and vineyards in Baden-Wuerttemberg. Furthermore, the raw results do not provide spatial context. Thus, it is\nnot interpretable which makes it hard for experts to integrate their domain knowledge into the analysis process. Hence,\nwe need visualization to help experts to identify spatial and temporal patterns easily. To achieve this, we follow the\nvisual information seeking mantra of Ben Shneiderman: \"Overview first, zoom and filter, details on demand\"   Figure 2: Glyph-visualization of temporal-spatial event predictions as proposed by Seebacher et al.    We build a geographic information system, using a map as the basis for interaction and spatio-temporal analysis. We\nconsider the familiarity of domain experts with this kind of visualization as an additional benefit. We visualize our\npredictions on the corresponding position on the map so that users are immediately aware of the geographic context.\nAdditionally, combining our geographic visualization with a visualization of the temporal predictions into a single\nvisualization is more effective, since this requires less cognitive effort for the users. Existing related systems such\nas BirdVis offer heat map overview visualizations. However, as we want to investigate the distribution of a species over\ntime, we designed a map overlay consisting of several glyphs. This partially preserves the geographic context while\nglyph can be used to encode additional contextual information. The goal of our glyph is to visualize whether a certain\nregion is endangered or not. Consequently, we visually encode the classification results of a specific month represented\nby its time segment. The basic design of a time segment is depicted in Figure 2. Therefore, we make use of the interior\nof the respective time segment to represent the classification results of the ensemble-classifiers. For each month we\nhave a distribution of safe and endangered vineyards, according to the classification. Since the number of vineyards\nstays the same over all months for each glyph, we fill the area of the time segments according to the ratio of the\nbinary outcome (endangered, not endangered). This technique results in a radial glyph similar to a stacked bar chart\nshowing fractions of the whole. We use the colors red (endangered) and blue (not endangered), as derived from the\nwarm-cold color scale to distinguish the outcome. We provide additional functionality, such as semantic zoom, dynamic\naggregation and various details-on-demand data visualizations.   Figure 3: Overview of the Drosophigator application   Drosophigator enables experts to perform a visual analysis of spatio-temporal event predictions.",
            "title": "3. Visualization"
        },
        {
            "location": "/demos/invasive-species/#use-cases",
            "text": "We want to highlight how visualization can help domain experts to gain insights about the spread dynamics of D. suzukii.\nWe show the usefulness of our system by demonstrating how domain experts can investigate hypotheses using Drosophigator.\nTherefore, we investigated two recently proposed assumptions about the time of infestation by the JKI and the influences\nof environmental factors by Pelton et al.   Figure 4: Overview glyph-visualization of all vineyards in Baden-Wuerttemberg.   The development over the time-segments shows that the severity of infestation and the certainty of our prediction\nincreases in late summer and stays high until the end of the year. This corroborates the hypothesis of the JKI.    Figure 5: Comparison of the vineyards contained in two neighboring cells.   The upper cell (purple) exhibits an earlier infestation by D. suzukii that the lower cell (brown). The parallel\ncoordinates plot shows, that the vineyards in the upper cell have around 10% more surrounding woodland ( Wald ) than\nthose in the lower cell. This finding strongly supports the hypothesis of Pelton et al.",
            "title": "Use Cases"
        },
        {
            "location": "/demos/invasive-species/#evaluation",
            "text": "We presented our system at the 6 th  workshop of the working group \"D. Suzukii\" on the 5 th  and 6 th  of December in Bad\nKreuznach, Germany. The goal of this workshop is the mutual exchange of knowledge between researchers and practitioners.\nOver 80 biologists, researchers, agri- and horticulturists from various countries participated in the workshop. The\nfocus of our talk was our application  Drosophigator , especially the design and interpretation of the glyph as well as\nthe interaction possibilities with the system.  After the presentation of the system, a questionnaire was handed out to\nthe workshop participants where they could rate the different aspects of our application and could provide us with\nadditional information about their background. We use the results of this questionnaire to evaluate our system and\ndesign decisions.   Figure 7: Evaluation of system feedback of all participants (n=37)   Shown are the responses of the participants on questions regarding the visualization design ( V1, V2, V3 ), the\ninteraction design ( I1, I2 ) and the analysis capabilities ( A1, A2, A3, A4 ) of our system Drosophigator.   The results of our evaluation make it clear that there is a strong need for intuitive and interactive systems, which\nsupport the experts in their daily analysis tasks. The experts are, for the most part, very positive about Drosphigator . Our glyph design was comprehensible, helped them to understand the temporal occurrence of D. suzukii and\nintegrating it in a map helped them to interpret the results. Additionally, allowing for a seamless clustering of\nvineyards into larger regions is deemed important, as it allows the analysis of micro- and macroecological factors.\nHowever, experts are still divided in their opinion, whether the application can support them in their work. This is\nreflected in their opinion about the possibility to infer causes for the occurrence of D. suzukii from our application.",
            "title": "Evaluation"
        },
        {
            "location": "/demos/invasive-species/#related-scenarios",
            "text": "Environment",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/landuse/",
            "text": "Landuse classification\n\u00b6\n\n\n\n\nNote\n\n\nRelated repository is \nhttps://github.com/biggis-project/biggis-landuse\n\n\n\n\nProblem definition\n\u00b6\n\n\n\n\nWe have the following datasets:\n\n\nAn existing Landuse vector dataset\n\n\nOrthorectified Aerial images (Digital Ortho Photos = DOP) \n\n\nSatellite images (SAT), e.g. \nSentinel 2\n 10m\n\n\n\n\n\n\nWe want to select / extract landcover classes from landuse classes.\n\n\nWe want to use Support Vector Machines (or other Machine Learning Classifiers) for classifying landcover classes.\n\n\nsee section \nSupport Vector Machines\n\n\n\n\n\n\nWe want to support Multiple classes using a One versus All (OvA) strategy or One vs. Rest.\n\n\nsee section \nOne vs. Rest\n\n\n\n\n\n\nWe want to parallelize the computation in our Spark cluster using \nGeotrellis\n for data loading and export.\n\n\n\n\n\n\nResponsible person for this section\n\n\nAdrian Klink (EFTAS)\n\n\n\n\n\n\nTodo\n\n\n\n\nDescribe the idea\n\n\nMaybe add some geotrellis examples\n\n\n\n\n\n\nClassification of Aerial Images according to Land Use Classes (using Land Cover Classes as intermediate)\n\u00b6\n\n\nTools\n\u00b6\n\n\n\n\nMachine Learning\n\n\nTraining: Multiclass SVM\n\n\nGeotrellis\n\n\n\n\nScala code snippets\n\u00b6\n\n\ntype\n \nSpatialMultibandRDD\n \n=\n \nRDD\n[(\nSpatialKey\n, \nMultibandTile\n)]\n \nwith\n \nMetadata\n[\nTileLayerMetadata\n[\nSpatialKey\n]]\n\n\n\n// reading from Hadoop Layer (HDFS)\n\n\nval\n \nrdd\n \n:\n \nSpatialMultibandRDD\n \n=\n \nbiggis\n.\nlanduse\n.\napi\n.\nreadRddFromLayer\n(\nLayerId\n(\nlayerName\n,\n \nzoom\n))\n\n\n\n// writing to Hadoop Layer (HDFS)\n\n\nbiggis\n.\nlanduse\n.\napi\n.\nwriteRddToLayer\n(\nrdd\n,\n \nLayerId\n(\nlayerName\n,\n \nzoom\n))\n\n\n\n\n\nExample\n\u00b6\n\n\n\n\nClassification of Aerial Images May-Aug 2016\n\n\nLayerstacking: Aerial Images + Satellite images (IR, Resolution 2m)\n\n\nTraining of a Multiclass SVM with manually selected training data (classified image tiles)\n\n\n\n\nFurther Steps\n\u00b6\n\n\n\n\nAdding additional Layers, e.g.\n\n\nTerrain Height\n\n\nHomogeneity of Texture\n\n\nUsing Other Classififiers\n\n\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nEnvironment\n\n\nSmart City",
            "title": "Landuse classification"
        },
        {
            "location": "/demos/landuse/#landuse-classification",
            "text": "Note  Related repository is  https://github.com/biggis-project/biggis-landuse",
            "title": "Landuse classification"
        },
        {
            "location": "/demos/landuse/#problem-definition",
            "text": "We have the following datasets:  An existing Landuse vector dataset  Orthorectified Aerial images (Digital Ortho Photos = DOP)   Satellite images (SAT), e.g.  Sentinel 2  10m    We want to select / extract landcover classes from landuse classes.  We want to use Support Vector Machines (or other Machine Learning Classifiers) for classifying landcover classes.  see section  Support Vector Machines    We want to support Multiple classes using a One versus All (OvA) strategy or One vs. Rest.  see section  One vs. Rest    We want to parallelize the computation in our Spark cluster using  Geotrellis  for data loading and export.    Responsible person for this section  Adrian Klink (EFTAS)    Todo   Describe the idea  Maybe add some geotrellis examples",
            "title": "Problem definition"
        },
        {
            "location": "/demos/landuse/#classification-of-aerial-images-according-to-land-use-classes-using-land-cover-classes-as-intermediate",
            "text": "",
            "title": "Classification of Aerial Images according to Land Use Classes (using Land Cover Classes as intermediate)"
        },
        {
            "location": "/demos/landuse/#tools",
            "text": "Machine Learning  Training: Multiclass SVM  Geotrellis",
            "title": "Tools"
        },
        {
            "location": "/demos/landuse/#scala-code-snippets",
            "text": "type   SpatialMultibandRDD   =   RDD [( SpatialKey ,  MultibandTile )]   with   Metadata [ TileLayerMetadata [ SpatialKey ]]  // reading from Hadoop Layer (HDFS)  val   rdd   :   SpatialMultibandRDD   =   biggis . landuse . api . readRddFromLayer ( LayerId ( layerName ,   zoom ))  // writing to Hadoop Layer (HDFS)  biggis . landuse . api . writeRddToLayer ( rdd ,   LayerId ( layerName ,   zoom ))",
            "title": "Scala code snippets"
        },
        {
            "location": "/demos/landuse/#example",
            "text": "Classification of Aerial Images May-Aug 2016  Layerstacking: Aerial Images + Satellite images (IR, Resolution 2m)  Training of a Multiclass SVM with manually selected training data (classified image tiles)",
            "title": "Example"
        },
        {
            "location": "/demos/landuse/#further-steps",
            "text": "Adding additional Layers, e.g.  Terrain Height  Homogeneity of Texture  Using Other Classififiers",
            "title": "Further Steps"
        },
        {
            "location": "/demos/landuse/#related-scenarios",
            "text": "Environment  Smart City",
            "title": "Related Scenarios"
        },
        {
            "location": "/demos/optical-remote-sensing/",
            "text": "Optical Remote Sensing\n\u00b6\n\n\nThe starting point for the BOS scenario in the BigGIS frame is the satellite-based emergency management services of\nCopernicus or the charter \u201cSpace and major Disasters\u201d. The idea was to implement similar sensors on an unmanned aerial\nvehicle (UAV) platform and bring it to smaller incidents like larger fires or CBRN. Therefore, thermal (IR) and\nhyperspectral cameras was used as well as RGB cameras to do some testing in simulated situations:\n\n\n\n\nDetection and following smoke clouds in imageries\n\n\nDetection of \u201cnon visible\u201d gas clouds\n\n\nIdentification of \u201cchemicals\u201d\n\n\n\n\nGas Cloud Detection\n\u00b6\n\n\nTo perform the simulations several test scenarios were prepared in two campaigns in Karlsruhe and Dortmund. The smoke of\nHeptane (UN 1206/Kemmler 33) was recorded as well as a mixture from gasoline (1203/33) and diesel (1202/30). A gas\nleakage was simulated at the Dortmund Fire Brigade Education Center using Methane (\nCH_4\nCH_4\n; 1971/23). And a gas cloud\ncontaining \u201cchemicals\u201d was simulated by a fog machine which nebulized a 50% mixture of propylene glycol\n(propane-1,2-diol) and chlorophyll from the food branch.\n\n\nFirst analysis eg. for the \u201cinvisible\u201d Methane gas cloud show quite good results using the IR cameras. On UAVs offered\nby Sitebots and AI Drones two choices of cameras were used:\n\n\n\n\nOPTRIS PI\n\n\nFLIR Vue Pro R\n\n\n\n\nBoth cameras give the radiometric signatures and not just \u201ccolored pictures\u201d. The Images were spatially referenced by\nstandard procedures. It was found that building differences just show intereferences in the pictures (shown in the first\nrow of Picture 1). Good results were given by a Halcon referencing based on sub pixel accuracy (row two in Picture 1).\nUsing difference analysis on about 25 pictures show a clear signature of the exhaling methane (row three in Picture 1).\n\n\n\n\nPicture 1: Gas cloud detection using thermal imageries\n\n\n\n\n\n\n\n\nSpectral Analysis of Gas Clouds\n\u00b6\n\n\nThe idea of remotely detecting and identifying chemicals using a hyperspectral sensor is not new.\nThe so called Analytical Task Force (ATF\n1\n) is using the Van-based RAMAN spectroscope SIGIS 2 to\ndetect and identify chemicals in CBRN incidents.\n\n\nThe BigGIS project intended to be more flexible than a SIGIS 2 mounted in a car. Therefore, as a proof-of-concept the\nimplementation of a system allowing for spectral analysis of gas and aerosol clouds mounted on a UAV was subject of\nstudy on the level of a proof-of-concept. Due to the fact that multispectral IR-sensors as they are utilized in the\nSIGIS 2 system require relatively heavy-weight cooling units disqualify these systems for the usage with a UAV.\nTherefore, within the project a multispectral sensor sensitive in the spectral range of visible light and near IR (450 -\n950 nm) was utilized for spectral analyzation. The sensor Cubert 185 UHD Firefly\n2\n has a weight of about 500 g\nand could easily be mounted on a UAV.\n\n\nUsing this sensor \u201csmoke clouds\u201d from a mixture of \u201cDisco fog\u201d and chlorophyll (see below) were recorded. Each pixel in\nthe image contains the spectral information of the reflected light spread over 125 bands ranging from a minimum\nwavelength of 450nm to a maximum of 950nm.\n\n\nTriangular Chlorophyll Index\n\u00b6\n\n\nIn a first Analysis the propagating chlorophyll cloud was identified in the recorded image via the calculation of the\n\nTriangular Chlorophyll Index (TCI)\n for each pixel in the spatially referenced\nimage.\n\n\nThe result is shown in the picture below:\nRow one in Picture 2 is showing the chlorophyll cloud propagating from west\nto east through spatially referenced pseudo color pictures. Interesting\nis the underground partly paved and partly consisted of a grass strip.\nIn the right picture the cloud covers a small\ntree.\n\n\nThe TCI algorithm was applied on the three pictures in the middle row.\nUsing a reference aerial photograph and building the difference to such\nan image (third row) one can find chlorophyll only detected on the\nasphalt, not on the green strip or the tree due to the fact that the\nmethod cannot distinguish chlorophyll from the plants from\nchlorophyll of the cloud.\n\n\n\n\nTODO: Chemical cloud detection\n\n\n\n\nWie ist die obige Erkl\u00e4rung zu verstehen? Geht es hier darum die Bereiche mit zeitlich konstant hohem TCI-Wert (=\nnicht die Chlorophyll-Wolke) auszuschlie\u00dfen?\n\n\n\n\nPicture 3 now shows the composition of the referenced and analyzed\npictures with all three stages of the moving chlorophyll cloud. One now can\nsee the grass strip and the tree in addition to the gas cloud over the\nasphalt.\n\n\n\n\nPicture 3: TODO: Eingef\u00e4rbte Elemente detailierter erkl\u00e4ren.\n\n\n\n\n\n\nLogistic Regression Classification of Cloud Reflectance\n\u00b6\n\n\nA second experiment set was dedicated to a more general evaluation of cloud constituents. Here, the identification of\nconstituents via logistic regression classification bases on the whole spectral range of the Cubert 185 UHD Firefly, in\ncontrary to the previous example where the identification based on only three wavelengths.\n\n\nAs a proof of concept, multispectral images of clouds produced by a fog generator fueled with two different fog fluids\nand solutions of each fog fluid mixed with defined proportions of chlorophyll (TODO: fog fluids genauer spezifizieren\nund chlorophyll-l\u00f6sung) were recorded.\n\n\nIn the experimental setup the Cubert 185 UHD Firefly was positioned facing a white wall in 1.2 m distance as constant\nbackground. The fog generator was placed between camera and wall in such a way, that the ejected cloud passed the camera\nin roughly 0.6 m distance while filling the whole recorded image plane. Immediately before each measurement set the\nincident intensity (white-balance intensity) in front of the camera was recorded by capturing the reflectance of a\nspectralon coated sheet at a distance of 0.6 m. By dividing the recorded reflected cloud intensities by the\nwhite-balance intensity the cloud reflectance was determined.\n\n\n\n\n\nThe reflectance spectra of evaporated mixtures of fog fluid with chlorophyll not in all cases show the typical\nchlorophyll absorption minimum. Consequently, chlorophyll is not uniformly evaporated with the given setup but ejected\nin irregular chlorophyll bursts instead. Therefore, a TCI pre-evaluation was carried through on each pixel of all\nrecordings of clouds of evaporated chlorophyll mixtures, in order to label positive chlorophyll spectra as training and\ntest data for the logistic regression. After spectra inspection of several samples a threshold of \nTCI = 0.05\nTCI = 0.05\n was\nchosen above which the spectra was labeled chlorophyll-containing. Spectra of evaporated chlorophyll mixtures with\n\nTCI < 0.05\nTCI < 0.05\n were not regarded in the further analysis. The spectra of clouds of evaporated pure fog fluids provided the\nnegative chlorophyll data. The table below shows the count of spectra for the different cloud categories that was used\nfor training for the logistic regression classifiers in the next subsections.\n\n\n\n\n\n\n\n\n\n\nFog Fluid 1\n\n\nFog Fluid 2\n\n\n\n\n\n\n\n\n\n\nNo Chlorophyll\n\n\n5000\n\n\n7500\n\n\n\n\n\n\nChlorophyll\n\n\n3926\n\n\n68541\n\n\n\n\n\n\n\n\nThe test data set comprised the following sample sizes:\n\n\n\n\n\n\n\n\n\n\nFog Fluid 1\n\n\nFog Fluid 2\n\n\n\n\n\n\n\n\n\n\nNo Chlorophyll\n\n\n2500\n\n\n2500\n\n\n\n\n\n\nChlorophyll\n\n\n2500\n\n\n2463\n\n\n\n\n\n\n\n\nBased on this data different logistic regression classifiers were trained.\n\n\nChlorophyll vs. Non-Chlorophyll\n\u00b6\n\n\n\n\nFigure TODO: Pure Fog vs Chlorophyl\n\n\n\n\n\n\nFirst a logistic regression classifier was trained to distinguish between spectra of clouds containing chlorophyll and\npure fog fluid, irrespective of the type of the fog fluid. The figure above displays typical spectra for a cloud aof\npure fog fluid and a cloud containing chlorophyll. In the latter case, the reflectance minimum due to the absorption\nmaximum of chlorophyll can clearly be seen in the curve around channel 45. This also is the region where the variable\nimportance (i.e. value of the t\u2013statistic for each model parameter (= channel)) peaks and marks the most significant\nchannels of the classifier.\n\n\nThe training accuracy was found to be\n$$\nAcc_{train} = \\frac{true Positives + true Negatives}{Positives + Negatives} = 1.\n$$\n\n\nThe out-of-sample test also showed very reliable results:\n\n\nTest samples with fog fluid 1:\n\n\n\n\n\n\n\n\n\n\nClassified No-Chloro.\n\n\nClassified Chloro.\n\n\n\n\n\n\n\n\n\n\nSample No-Chloro.\n\n\n2500\n\n\n0\n\n\n\n\n\n\nSample Chloro.\n\n\n0\n\n\n2500\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 1\n\n\n\n\nAcc_{test} = 1\n\n\n\n\n\n\n\n\nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1\n\n\n\n\nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1\n\n\n\n\n\nTest samples with fog fluid 2:\n\n\n\n\n\n\n\n\n\n\nClassified No-Chloro.\n\n\nClassified Chloro.\n\n\n\n\n\n\n\n\n\n\nSample No-Chloro.\n\n\n2500\n\n\n0\n\n\n\n\n\n\nSample Chloro.\n\n\n0\n\n\n2463\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 1\n\n\n\n\nAcc_{test} = 1\n\n\n\n\n\nThe high accuracy bases on the very clear feature of the chlorophyll absorption dip in the spectrum.\n\n\nPure Fog Fluid 1 vs. Fog Fluid 2\n\u00b6\n\n\n\n\nWhile the spectra of chlorophyll containing clouds show a clear distinction feature against the non-chlorophyll\ncontaining spectra, the reflectance spectra of the two pure fog fluids show a similar relative pattern, while the\nabsolute reflectance level of the fog fluid 1 seems to be elevated compared to fog fluid 2 (see figure above).\nTherefore, a logistic regression classifier was trained for the distinction of clouds of the two pure fog fluids. The\ntraining accuracy for this classifier was found to be:\n$$\nAcc_{train} = 1\n$$\n\n\nOut-of-sample performance:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n2452\n\n\n48\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n1\n\n\n2499\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 0.99\n\n\n\n\nAcc_{test} = 0.99\n\n\n\n\n\n\n\n\nF1_{test} = 0.99\n\n\n\n\nF1_{test} = 0.99\n\n\n\n\n\nDespite the lack of prominent characteristic features in the reflectance spectra the accuracy and the F1 score of the\nclassifier is yet rather high. That changes when the classifier is tested on reflectance data of mixtures of the\ndifferent fog fluids with chlorophyll, as shown in the table below:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n0\n\n\n2500\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n0\n\n\n2463\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 0.49\n\n\n\n\nAcc_{test} = 0.49\n\n\n\n\n\n\n\n\nF1_{test} = 0.66\n\n\n\n\nF1_{test} = 0.66\n\n\n\n\n\nObviously this classifier is not robust against the mixture of features of another substance in the cloud.\n\n\nFog Fluid 1 vs. Fog Fluid 2 (with and without Chlorophyll)\n\u00b6\n\n\nTo overcome the weakness of the previous classifier another classifier for the distinction between different fog fluids\nwas trained including the reflectance spectra of mixtures with chlorophyll. Here, the training accuracy was found to be:\n$$\nAcc_{train} = 1\n$$\n\n\nThe out of sample performance for clouds of pure fog fluids is shown in the table below:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n2500\n\n\n0\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n0\n\n\n2500\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 1\n\n\n\n\nAcc_{test} = 1\n\n\n\n\n\n\n\n\nF1_{test} = 1\n\n\n\n\nF1_{test} = 1\n\n\n\n\n\nInterestingly, the performance on the test data of pure fog fluids is even slightly better when trained with data\nsamples including mixtures with chlorophyll. This might be explained with overfitting in the case of training only with\ndata of pure fog fluids (TODO diskutieren).\n\n\nThe out-of-sample performance for mixtures of the different fog fluids with chlorophyll is shown in the table below:\n\n\n\n\n\n\n\n\n\n\nClassified Fog Fluid 1\n\n\nClassified Fog Fluid 2\n\n\n\n\n\n\n\n\n\n\nSample Fog Fluid 1\n\n\n2014\n\n\n486\n\n\n\n\n\n\nSample Fog Fluid 2\n\n\n11\n\n\n2452\n\n\n\n\n\n\n\n\n\n\n\nAcc_{test} = 0.90\n\n\n\n\nAcc_{test} = 0.90\n\n\n\n\n\n\n\n\nF1_{test} = 0.90\n\n\n\n\nF1_{test} = 0.90\n\n\n\n\n\nThe performance is rather convincing even though the spectra of the fog fluids are overlayed with the\ncharacteristic chlorophyll spectrum.\n\n\nConclusion\n\u00b6\n\n\nIt was shown that logistic regression can be used to identify constituents of clouds on basis of the reflectance spectra\nin the range of visible light with the here presented system, especially when strong characteristic features are present\nas in the case of chlorophyll. Besides that, even the identification of constituents with weaker characteristic features\ncan be carried through as in the case for the distinction between the two fog fluids. For the latter case, the above\nfindings suggest that the classifier for the constituent of interest should be carried through with data also regarding\na variety of probable accompanying cloud constituents. Otherwise, the overlay of the different spectra might diminish\nthe classifiers performance.\n\n\n\n\n\n\n\n\n\n\nhttps://www.bbk.bund.de/DE/AufgabenundAusstattung/CBRNSchutz/TaskForce/ATF_einstieg1.html\n\u00a0\n\u21a9\n\n\n\n\n\n\nhttp://cubert-gmbh.com/uhd-185-firefly/\n\u00a0\n\u21a9",
            "title": "Optical Remote Sensing"
        },
        {
            "location": "/demos/optical-remote-sensing/#optical-remote-sensing",
            "text": "The starting point for the BOS scenario in the BigGIS frame is the satellite-based emergency management services of\nCopernicus or the charter \u201cSpace and major Disasters\u201d. The idea was to implement similar sensors on an unmanned aerial\nvehicle (UAV) platform and bring it to smaller incidents like larger fires or CBRN. Therefore, thermal (IR) and\nhyperspectral cameras was used as well as RGB cameras to do some testing in simulated situations:   Detection and following smoke clouds in imageries  Detection of \u201cnon visible\u201d gas clouds  Identification of \u201cchemicals\u201d",
            "title": "Optical Remote Sensing"
        },
        {
            "location": "/demos/optical-remote-sensing/#gas-cloud-detection",
            "text": "To perform the simulations several test scenarios were prepared in two campaigns in Karlsruhe and Dortmund. The smoke of\nHeptane (UN 1206/Kemmler 33) was recorded as well as a mixture from gasoline (1203/33) and diesel (1202/30). A gas\nleakage was simulated at the Dortmund Fire Brigade Education Center using Methane ( CH_4 CH_4 ; 1971/23). And a gas cloud\ncontaining \u201cchemicals\u201d was simulated by a fog machine which nebulized a 50% mixture of propylene glycol\n(propane-1,2-diol) and chlorophyll from the food branch.  First analysis eg. for the \u201cinvisible\u201d Methane gas cloud show quite good results using the IR cameras. On UAVs offered\nby Sitebots and AI Drones two choices of cameras were used:   OPTRIS PI  FLIR Vue Pro R   Both cameras give the radiometric signatures and not just \u201ccolored pictures\u201d. The Images were spatially referenced by\nstandard procedures. It was found that building differences just show intereferences in the pictures (shown in the first\nrow of Picture 1). Good results were given by a Halcon referencing based on sub pixel accuracy (row two in Picture 1).\nUsing difference analysis on about 25 pictures show a clear signature of the exhaling methane (row three in Picture 1).   Picture 1: Gas cloud detection using thermal imageries",
            "title": "Gas Cloud Detection"
        },
        {
            "location": "/demos/optical-remote-sensing/#spectral-analysis-of-gas-clouds",
            "text": "The idea of remotely detecting and identifying chemicals using a hyperspectral sensor is not new.\nThe so called Analytical Task Force (ATF 1 ) is using the Van-based RAMAN spectroscope SIGIS 2 to\ndetect and identify chemicals in CBRN incidents.  The BigGIS project intended to be more flexible than a SIGIS 2 mounted in a car. Therefore, as a proof-of-concept the\nimplementation of a system allowing for spectral analysis of gas and aerosol clouds mounted on a UAV was subject of\nstudy on the level of a proof-of-concept. Due to the fact that multispectral IR-sensors as they are utilized in the\nSIGIS 2 system require relatively heavy-weight cooling units disqualify these systems for the usage with a UAV.\nTherefore, within the project a multispectral sensor sensitive in the spectral range of visible light and near IR (450 -\n950 nm) was utilized for spectral analyzation. The sensor Cubert 185 UHD Firefly 2  has a weight of about 500 g\nand could easily be mounted on a UAV.  Using this sensor \u201csmoke clouds\u201d from a mixture of \u201cDisco fog\u201d and chlorophyll (see below) were recorded. Each pixel in\nthe image contains the spectral information of the reflected light spread over 125 bands ranging from a minimum\nwavelength of 450nm to a maximum of 950nm.",
            "title": "Spectral Analysis of Gas Clouds"
        },
        {
            "location": "/demos/optical-remote-sensing/#triangular-chlorophyll-index",
            "text": "In a first Analysis the propagating chlorophyll cloud was identified in the recorded image via the calculation of the Triangular Chlorophyll Index (TCI)  for each pixel in the spatially referenced\nimage.  The result is shown in the picture below:\nRow one in Picture 2 is showing the chlorophyll cloud propagating from west\nto east through spatially referenced pseudo color pictures. Interesting\nis the underground partly paved and partly consisted of a grass strip.\nIn the right picture the cloud covers a small\ntree.  The TCI algorithm was applied on the three pictures in the middle row.\nUsing a reference aerial photograph and building the difference to such\nan image (third row) one can find chlorophyll only detected on the\nasphalt, not on the green strip or the tree due to the fact that the\nmethod cannot distinguish chlorophyll from the plants from\nchlorophyll of the cloud.   TODO: Chemical cloud detection   Wie ist die obige Erkl\u00e4rung zu verstehen? Geht es hier darum die Bereiche mit zeitlich konstant hohem TCI-Wert (=\nnicht die Chlorophyll-Wolke) auszuschlie\u00dfen?   Picture 3 now shows the composition of the referenced and analyzed\npictures with all three stages of the moving chlorophyll cloud. One now can\nsee the grass strip and the tree in addition to the gas cloud over the\nasphalt.   Picture 3: TODO: Eingef\u00e4rbte Elemente detailierter erkl\u00e4ren.",
            "title": "Triangular Chlorophyll Index"
        },
        {
            "location": "/demos/optical-remote-sensing/#logistic-regression-classification-of-cloud-reflectance",
            "text": "A second experiment set was dedicated to a more general evaluation of cloud constituents. Here, the identification of\nconstituents via logistic regression classification bases on the whole spectral range of the Cubert 185 UHD Firefly, in\ncontrary to the previous example where the identification based on only three wavelengths.  As a proof of concept, multispectral images of clouds produced by a fog generator fueled with two different fog fluids\nand solutions of each fog fluid mixed with defined proportions of chlorophyll (TODO: fog fluids genauer spezifizieren\nund chlorophyll-l\u00f6sung) were recorded.  In the experimental setup the Cubert 185 UHD Firefly was positioned facing a white wall in 1.2 m distance as constant\nbackground. The fog generator was placed between camera and wall in such a way, that the ejected cloud passed the camera\nin roughly 0.6 m distance while filling the whole recorded image plane. Immediately before each measurement set the\nincident intensity (white-balance intensity) in front of the camera was recorded by capturing the reflectance of a\nspectralon coated sheet at a distance of 0.6 m. By dividing the recorded reflected cloud intensities by the\nwhite-balance intensity the cloud reflectance was determined.   The reflectance spectra of evaporated mixtures of fog fluid with chlorophyll not in all cases show the typical\nchlorophyll absorption minimum. Consequently, chlorophyll is not uniformly evaporated with the given setup but ejected\nin irregular chlorophyll bursts instead. Therefore, a TCI pre-evaluation was carried through on each pixel of all\nrecordings of clouds of evaporated chlorophyll mixtures, in order to label positive chlorophyll spectra as training and\ntest data for the logistic regression. After spectra inspection of several samples a threshold of  TCI = 0.05 TCI = 0.05  was\nchosen above which the spectra was labeled chlorophyll-containing. Spectra of evaporated chlorophyll mixtures with TCI < 0.05 TCI < 0.05  were not regarded in the further analysis. The spectra of clouds of evaporated pure fog fluids provided the\nnegative chlorophyll data. The table below shows the count of spectra for the different cloud categories that was used\nfor training for the logistic regression classifiers in the next subsections.      Fog Fluid 1  Fog Fluid 2      No Chlorophyll  5000  7500    Chlorophyll  3926  68541     The test data set comprised the following sample sizes:      Fog Fluid 1  Fog Fluid 2      No Chlorophyll  2500  2500    Chlorophyll  2500  2463     Based on this data different logistic regression classifiers were trained.",
            "title": "Logistic Regression Classification of Cloud Reflectance"
        },
        {
            "location": "/demos/optical-remote-sensing/#chlorophyll-vs-non-chlorophyll",
            "text": "Figure TODO: Pure Fog vs Chlorophyl    First a logistic regression classifier was trained to distinguish between spectra of clouds containing chlorophyll and\npure fog fluid, irrespective of the type of the fog fluid. The figure above displays typical spectra for a cloud aof\npure fog fluid and a cloud containing chlorophyll. In the latter case, the reflectance minimum due to the absorption\nmaximum of chlorophyll can clearly be seen in the curve around channel 45. This also is the region where the variable\nimportance (i.e. value of the t\u2013statistic for each model parameter (= channel)) peaks and marks the most significant\nchannels of the classifier.  The training accuracy was found to be\n$$\nAcc_{train} = \\frac{true Positives + true Negatives}{Positives + Negatives} = 1.\n$$  The out-of-sample test also showed very reliable results:  Test samples with fog fluid 1:      Classified No-Chloro.  Classified Chloro.      Sample No-Chloro.  2500  0    Sample Chloro.  0  2500      \nAcc_{test} = 1  \nAcc_{test} = 1    \nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1  \nPrecision_{test} = 1\nRecall_{test} = 1\nF1_{test} = 1   Test samples with fog fluid 2:      Classified No-Chloro.  Classified Chloro.      Sample No-Chloro.  2500  0    Sample Chloro.  0  2463      \nAcc_{test} = 1  \nAcc_{test} = 1   The high accuracy bases on the very clear feature of the chlorophyll absorption dip in the spectrum.",
            "title": "Chlorophyll vs. Non-Chlorophyll"
        },
        {
            "location": "/demos/optical-remote-sensing/#pure-fog-fluid-1-vs-fog-fluid-2",
            "text": "While the spectra of chlorophyll containing clouds show a clear distinction feature against the non-chlorophyll\ncontaining spectra, the reflectance spectra of the two pure fog fluids show a similar relative pattern, while the\nabsolute reflectance level of the fog fluid 1 seems to be elevated compared to fog fluid 2 (see figure above).\nTherefore, a logistic regression classifier was trained for the distinction of clouds of the two pure fog fluids. The\ntraining accuracy for this classifier was found to be:\n$$\nAcc_{train} = 1\n$$  Out-of-sample performance:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  2452  48    Sample Fog Fluid 2  1  2499      \nAcc_{test} = 0.99  \nAcc_{test} = 0.99    \nF1_{test} = 0.99  \nF1_{test} = 0.99   Despite the lack of prominent characteristic features in the reflectance spectra the accuracy and the F1 score of the\nclassifier is yet rather high. That changes when the classifier is tested on reflectance data of mixtures of the\ndifferent fog fluids with chlorophyll, as shown in the table below:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  0  2500    Sample Fog Fluid 2  0  2463      \nAcc_{test} = 0.49  \nAcc_{test} = 0.49    \nF1_{test} = 0.66  \nF1_{test} = 0.66   Obviously this classifier is not robust against the mixture of features of another substance in the cloud.",
            "title": "Pure Fog Fluid 1 vs. Fog Fluid 2"
        },
        {
            "location": "/demos/optical-remote-sensing/#fog-fluid-1-vs-fog-fluid-2-with-and-without-chlorophyll",
            "text": "To overcome the weakness of the previous classifier another classifier for the distinction between different fog fluids\nwas trained including the reflectance spectra of mixtures with chlorophyll. Here, the training accuracy was found to be:\n$$\nAcc_{train} = 1\n$$  The out of sample performance for clouds of pure fog fluids is shown in the table below:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  2500  0    Sample Fog Fluid 2  0  2500      \nAcc_{test} = 1  \nAcc_{test} = 1    \nF1_{test} = 1  \nF1_{test} = 1   Interestingly, the performance on the test data of pure fog fluids is even slightly better when trained with data\nsamples including mixtures with chlorophyll. This might be explained with overfitting in the case of training only with\ndata of pure fog fluids (TODO diskutieren).  The out-of-sample performance for mixtures of the different fog fluids with chlorophyll is shown in the table below:      Classified Fog Fluid 1  Classified Fog Fluid 2      Sample Fog Fluid 1  2014  486    Sample Fog Fluid 2  11  2452      \nAcc_{test} = 0.90  \nAcc_{test} = 0.90    \nF1_{test} = 0.90  \nF1_{test} = 0.90   The performance is rather convincing even though the spectra of the fog fluids are overlayed with the\ncharacteristic chlorophyll spectrum.",
            "title": "Fog Fluid 1 vs. Fog Fluid 2 (with and without Chlorophyll)"
        },
        {
            "location": "/demos/optical-remote-sensing/#conclusion",
            "text": "It was shown that logistic regression can be used to identify constituents of clouds on basis of the reflectance spectra\nin the range of visible light with the here presented system, especially when strong characteristic features are present\nas in the case of chlorophyll. Besides that, even the identification of constituents with weaker characteristic features\ncan be carried through as in the case for the distinction between the two fog fluids. For the latter case, the above\nfindings suggest that the classifier for the constituent of interest should be carried through with data also regarding\na variety of probable accompanying cloud constituents. Otherwise, the overlay of the different spectra might diminish\nthe classifiers performance.      https://www.bbk.bund.de/DE/AufgabenundAusstattung/CBRNSchutz/TaskForce/ATF_einstieg1.html \u00a0 \u21a9    http://cubert-gmbh.com/uhd-185-firefly/ \u00a0 \u21a9",
            "title": "Conclusion"
        },
        {
            "location": "/demos/sensor-placement/",
            "text": "Optimal Sensor Placement\n\u00b6\n\n\nSemistationary sensors such as the SenseBox (\nhttps://sensebox.de/en/\n) offer an excellent possibility for gaining\ndetailed insights into the urban climate. Connected to WiFI or LAN, they continuously provide measurements, e.g.\nconcerning the temperature, humidity, noise or brightness. However, these measurements are local in nature, meaning that\nin order to gain detailed and reliable information about an entire city, several sensors have to be distributed within\nthe region of interest. In this case, the placement of sensors becomes essential.  We therefore propose an approach for\nproposing optimal sensing locations, taking into account their relative distance, covered land use classes and\ntemperatures.\n\n\n\n\nFigure 1: Land use classes in Karlsruhe\n\n\n\n\n\n\nFeatures\n\u00b6\n\n\nOur model incorporates several features:\n\n\n\n\nSensors are placed such that they are within a maximum range to a number of base stations of a LoRaWAN\n    (Long Range Wide Area Network), allowing to continuously transmit their measurements.\n\n\nThe set of sensors should cover all land use classes within the city.\n\n\nThe selected sensing locations should cover a wide range of temperatures.\n\n\nThe locations of any other available sensor should be taken into account in order to avoid redundant measurements.\n\n\n\n\n\n\nFigure 2: Temperatures aggregated for landuse class polygons\n\n\n\n\n\n\nSolution Approach and Results\n\u00b6\n\n\nIn a preprocessing step, the centers of the polygons describing individual land use classes are selected as candidate\nsensing locations, making it possible to formulate a discrete planning problem. The range of the LoRaWAN restricts\npossible locations, thus ensuring connectivity. We can then model the planning problem as a p-center problem. Generally\nspeaking, this formulation seeks to minimize the maximum distance between locations of interest as described above and\nthe next sensing locations.  The approach yields both optimal sensing locations as well as alternative candidates in\ncase the initially chosen locations is unavailable. As shown in Figure 3, locations are distributed well across the\ncity, covering as large an area as possible.\n\n\n\n\nFigure 3: Proposed sensing locations and alternative positions",
            "title": "Optimal Sensor Placement"
        },
        {
            "location": "/demos/sensor-placement/#optimal-sensor-placement",
            "text": "Semistationary sensors such as the SenseBox ( https://sensebox.de/en/ ) offer an excellent possibility for gaining\ndetailed insights into the urban climate. Connected to WiFI or LAN, they continuously provide measurements, e.g.\nconcerning the temperature, humidity, noise or brightness. However, these measurements are local in nature, meaning that\nin order to gain detailed and reliable information about an entire city, several sensors have to be distributed within\nthe region of interest. In this case, the placement of sensors becomes essential.  We therefore propose an approach for\nproposing optimal sensing locations, taking into account their relative distance, covered land use classes and\ntemperatures.   Figure 1: Land use classes in Karlsruhe",
            "title": "Optimal Sensor Placement"
        },
        {
            "location": "/demos/sensor-placement/#features",
            "text": "Our model incorporates several features:   Sensors are placed such that they are within a maximum range to a number of base stations of a LoRaWAN\n    (Long Range Wide Area Network), allowing to continuously transmit their measurements.  The set of sensors should cover all land use classes within the city.  The selected sensing locations should cover a wide range of temperatures.  The locations of any other available sensor should be taken into account in order to avoid redundant measurements.    Figure 2: Temperatures aggregated for landuse class polygons",
            "title": "Features"
        },
        {
            "location": "/demos/sensor-placement/#solution-approach-and-results",
            "text": "In a preprocessing step, the centers of the polygons describing individual land use classes are selected as candidate\nsensing locations, making it possible to formulate a discrete planning problem. The range of the LoRaWAN restricts\npossible locations, thus ensuring connectivity. We can then model the planning problem as a p-center problem. Generally\nspeaking, this formulation seeks to minimize the maximum distance between locations of interest as described above and\nthe next sensing locations.  The approach yields both optimal sensing locations as well as alternative candidates in\ncase the initially chosen locations is unavailable. As shown in Figure 3, locations are distributed well across the\ncity, covering as large an area as possible.   Figure 3: Proposed sensing locations and alternative positions",
            "title": "Solution Approach and Results"
        },
        {
            "location": "/demos/soh-eval/",
            "text": "Stability of hotspots\n\u00b6\n\n\n\n\nResponsible person for this section\n\n\nMarc Gassenschmidt\n\n\n\n\ncomparison of different metrics (soh, jaccard, ...)\n\n\ncomparison of different methods (gstar, focalgstar, ...)\n\n\n\n\n\n\nSetup\n\u00b6\n\n\nIn \nparameters.Setting\n the folders have to be specified first.\n\n\ninputDirectoryCSV\n:\nFolder for the CSV data. If data does not exist change to local folders.\nTo download the data in demo. Execute the main program up to the line 16. \nThis will automatically download the New York Taxi Trip Data for January/February/March 2011-2016.\n\n\n\n\nWarning\n\n\nThis dataset is approx. 30 GB\n\n\n\n\nResults are stored in directories \nouptDirectory\n and \nstatDirectory\n.\n\n\nCalculation of hotspots for geo data\n\u00b6\n\n\nExecute \ndemo.Main'\n for the calculation of hotspots and variation of the focal matrix/weight matrix and aggregation\nlevel.\n\n\nScenario(variation of parameters between parent and child): Focal/Weight/Aggregationsstufe\n\n\nResults can be found at \"outputDirectory\"/\"Scenaria\"/focal_/\"(true|false)\"d3.csv\nFor example: \"outputDirectory\"/Aggregation/focal_falsed3.csv\n\n\nF,W,Z,Down,Up\n0,1,2,0.0,0.06053811659192825\n0,1,3,0.0,0.10396039603960396\n\n\n\n\nAs well as the TIFs in folder \n2016\n.\nCan then be displayed in the following way using QGis:\n\n\n\n\nFocal G* example - single band\n\n\n\n\n\n\nCalculation of hotspots for geotemporal data\n\u00b6\n\n\nscripts. MetricValidation\n can be executed to perform the calculation for the geo-temporal space. Here the metrics\n\ngetisOrd. SoH#getMetrikResults\n will be called. An \nArray\n with different test settings can be createdUnder\n\nscripts.MetricValidation#getScenarioSettings\n.\n\n\nThe following will be computed:\n\n\n\n\nG* for 2016\n\n\nVarious Metrics\n\n\n\n\nG* for 2011-2015 including the percentage match\n\n\n\n\n\n\nFocal G* for 2016\n\n\n\n\nVarious Metrics\n\n\nFocal G* f\u00fcr 2011-2015 including the percentage match\n\n\n\n\nThe path of the metrics results is defined in the method \nimportExport.PathFormatter#getResultDirectoryAndName\n.\n\n\nFor example \n{ouptDirectory}/GIS_Daten/2016/1/Metrik/focal/a4_w5_wT1_f20_fT2result.txt\n\n\n\n\n\n\n\n\nMetric\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nSoH_Down\n\n\n0.7549668874172185\n\n\n\n\n\n\nSoH_Up\n\n\n0.04694835680751175\n\n\n\n\n\n\nneighbours\n\n\n(1,0,1,0,1,0)\n\n\n\n\n\n\njaccard\n\n\n0.5352112676056338\n\n\n\n\n\n\npercentual\n\n\n-1.0\n\n\n\n\n\n\ntime_Down\n\n\n0.45329470205147443\n\n\n\n\n\n\ntime_Up\n\n\n0.13020833333333334\n\n\n\n\n\n\nKL\n\n\n0.9963685899429499\n\n\n\n\n\n\n...\n\n\n...\n\n\n\n\n\n\n\n\nThe paths to the TIFs are defined within the method \nimportExport.PathFormatter#getDirectoryAndName\n.\n\n\n\n\nFocal G* example  (multi-band)\n\n\n{ouptDirectory}/GIS_Daten/Mulitbandtrue/2016/3/GStar/focal/a3_w6_wT2_f16_fT3_z1",
            "title": "Stability of hotspots"
        },
        {
            "location": "/demos/soh-eval/#stability-of-hotspots",
            "text": "Responsible person for this section  Marc Gassenschmidt   comparison of different metrics (soh, jaccard, ...)  comparison of different methods (gstar, focalgstar, ...)",
            "title": "Stability of hotspots"
        },
        {
            "location": "/demos/soh-eval/#setup",
            "text": "In  parameters.Setting  the folders have to be specified first.  inputDirectoryCSV :\nFolder for the CSV data. If data does not exist change to local folders.\nTo download the data in demo. Execute the main program up to the line 16. \nThis will automatically download the New York Taxi Trip Data for January/February/March 2011-2016.   Warning  This dataset is approx. 30 GB   Results are stored in directories  ouptDirectory  and  statDirectory .",
            "title": "Setup"
        },
        {
            "location": "/demos/soh-eval/#calculation-of-hotspots-for-geo-data",
            "text": "Execute  demo.Main'  for the calculation of hotspots and variation of the focal matrix/weight matrix and aggregation\nlevel.  Scenario(variation of parameters between parent and child): Focal/Weight/Aggregationsstufe  Results can be found at \"outputDirectory\"/\"Scenaria\"/focal_/\"(true|false)\"d3.csv\nFor example: \"outputDirectory\"/Aggregation/focal_falsed3.csv  F,W,Z,Down,Up\n0,1,2,0.0,0.06053811659192825\n0,1,3,0.0,0.10396039603960396  As well as the TIFs in folder  2016 .\nCan then be displayed in the following way using QGis:   Focal G* example - single band",
            "title": "Calculation of hotspots for geo data"
        },
        {
            "location": "/demos/soh-eval/#calculation-of-hotspots-for-geotemporal-data",
            "text": "scripts. MetricValidation  can be executed to perform the calculation for the geo-temporal space. Here the metrics getisOrd. SoH#getMetrikResults  will be called. An  Array  with different test settings can be createdUnder scripts.MetricValidation#getScenarioSettings .  The following will be computed:   G* for 2016  Various Metrics   G* for 2011-2015 including the percentage match    Focal G* for 2016   Various Metrics  Focal G* f\u00fcr 2011-2015 including the percentage match   The path of the metrics results is defined in the method  importExport.PathFormatter#getResultDirectoryAndName .  For example  {ouptDirectory}/GIS_Daten/2016/1/Metrik/focal/a4_w5_wT1_f20_fT2result.txt     Metric  Value      SoH_Down  0.7549668874172185    SoH_Up  0.04694835680751175    neighbours  (1,0,1,0,1,0)    jaccard  0.5352112676056338    percentual  -1.0    time_Down  0.45329470205147443    time_Up  0.13020833333333334    KL  0.9963685899429499    ...  ...     The paths to the TIFs are defined within the method  importExport.PathFormatter#getDirectoryAndName .   Focal G* example  (multi-band)  {ouptDirectory}/GIS_Daten/Mulitbandtrue/2016/3/GStar/focal/a3_w6_wT2_f16_fT3_z1",
            "title": "Calculation of hotspots for geotemporal data"
        },
        {
            "location": "/demos/traffic-incidents/",
            "text": "Traffic Incidents\n\u00b6\n\n\nMotivation\n\u00b6\n\n\nThe visual analysis of traffic incidents is of high interest in the BOS-scenario. The real time gathering of traffic\ndata and effective ways of visualizing aim to support emergency services in reacting to incidents or when searching for\nhot spots. Furthermore, the analysis of traffic and mobility data is of high interest for urban planning as it, for\nexample, allows the planning of future infrastructure.\n\n\nVisual Analysis of Traffic Incidents\n\u00b6\n\n\nOur visual analysis allows to gain overview as well as detailed representations of the underlaying data. At first\nglance, a calendar visualization of traffic incidents (Figure 1) allows to get an impression about the temporal\ndistribution of the incidents. Figure 1, for example, allows the user to easily detect a repeating pattern in July 2016\nwhere each monday for three weeks in a row was  strongly noticeable.\n\n\n\n\nFigure 1: Calendar visualization of traffic incidents.\n\n\n\n\n\n\nAdditionally, we enable domain experts to inspect the categorical distribution of gathered traffic incidents. Domain\nexperts can interactively decide for timespans of interest (e.g., only taking incidents into account which occur at\nnight (Figure 2) or at afternoon (Figure 3)).\n\n\n\n\nFigure 2: Categorical distribution of gathered traffic incidents occuring at night.\n\n\n\n\n\n\n\n\nFigure 3: Categorical distribution of gathered traffic incidents occuring at afternoon.\n\n\n\n\n\n\nUltimately, it is of high importance to provide adequate spatial visaulizations in order to allow domain experts to\nexplore the data. We provide a more detailed animated heat map visualization on the map (see the video) as well as more\nabstract graph representation. The graph has been developed in order to detect patterns at larger scale. The graph is\nbuilt hierarchically. On the top level, a spatial clustering is used to detect areas of interest. Afterwards, we split\nthe clusters in the graph visualization based on their incident type by using color. On the next level, we indicate the\nseverity by using saturation.\n\n\n\n\nFigure 4: Several kinds of visual overviews to effectively aggregate the available data.\n\n\n\n\n\n\nVideo\n\u00b6\n\n\nA video demonstrating some of the capabilities of the analysis of traffic incidents can be found by clicking below or\n\nhere\n.",
            "title": "Traffic Incidents"
        },
        {
            "location": "/demos/traffic-incidents/#traffic-incidents",
            "text": "",
            "title": "Traffic Incidents"
        },
        {
            "location": "/demos/traffic-incidents/#motivation",
            "text": "The visual analysis of traffic incidents is of high interest in the BOS-scenario. The real time gathering of traffic\ndata and effective ways of visualizing aim to support emergency services in reacting to incidents or when searching for\nhot spots. Furthermore, the analysis of traffic and mobility data is of high interest for urban planning as it, for\nexample, allows the planning of future infrastructure.",
            "title": "Motivation"
        },
        {
            "location": "/demos/traffic-incidents/#visual-analysis-of-traffic-incidents",
            "text": "Our visual analysis allows to gain overview as well as detailed representations of the underlaying data. At first\nglance, a calendar visualization of traffic incidents (Figure 1) allows to get an impression about the temporal\ndistribution of the incidents. Figure 1, for example, allows the user to easily detect a repeating pattern in July 2016\nwhere each monday for three weeks in a row was  strongly noticeable.   Figure 1: Calendar visualization of traffic incidents.    Additionally, we enable domain experts to inspect the categorical distribution of gathered traffic incidents. Domain\nexperts can interactively decide for timespans of interest (e.g., only taking incidents into account which occur at\nnight (Figure 2) or at afternoon (Figure 3)).   Figure 2: Categorical distribution of gathered traffic incidents occuring at night.     Figure 3: Categorical distribution of gathered traffic incidents occuring at afternoon.    Ultimately, it is of high importance to provide adequate spatial visaulizations in order to allow domain experts to\nexplore the data. We provide a more detailed animated heat map visualization on the map (see the video) as well as more\nabstract graph representation. The graph has been developed in order to detect patterns at larger scale. The graph is\nbuilt hierarchically. On the top level, a spatial clustering is used to detect areas of interest. Afterwards, we split\nthe clusters in the graph visualization based on their incident type by using color. On the next level, we indicate the\nseverity by using saturation.   Figure 4: Several kinds of visual overviews to effectively aggregate the available data.",
            "title": "Visual Analysis of Traffic Incidents"
        },
        {
            "location": "/demos/traffic-incidents/#video",
            "text": "A video demonstrating some of the capabilities of the analysis of traffic incidents can be found by clicking below or here .",
            "title": "Video"
        },
        {
            "location": "/demos/uhi-visual-analysis/",
            "text": "Visual Analysis of Spatio-Temporal Event Predictions\n\u00b6\n\n\nThe well-known phenomenon of the Urban Heat Island effect in city areas is an important issue as a result from the\nongoing urbanization and industrialization process. Analyzing the causes for increased temperatures in urban areas is a\ncomplex task that depends on several variables like energy management, surface characteristics, weather, vegetation\nindex, population, industrial territory, transportation, air pollution and others. City and country planners are in need\nfor new technology that supports them during decision-making processes by meaningful visualization and data analysis\ntechniques to improve urban climate and the efficiency of energy management. We propose a visual analytics approach to\nenable domain experts to visually explore current temperature conditions of city areas and interactive steering\ntechniques on characteristic features to discover the influence of available variables on the outcome. The presented\nprediction models are intended to provide insights about future conditions to elicit the effect of various variables on\nthe intensity of urban heat islands.\n\n\nUsed Data Sources\n\u00b6\n\n\nA variety of data source is used for our visual analysis of urban heat islands such as\n\n\n\n\nWunderground\n\n\nATKIS/ALKIS\n\n\nDWD\n\n\n...\n\n\n\n\nWe incorporoated data sets for Germany, however, focused on the analysis of regions around Karlsruhe. \n\n\n\n\nPicture 1: WU Station distribution in and around Karlsruhe\n\n\n\n\n\n\nVisual Analysis of Urban Heat Islands\n\u00b6\n\n\nOur system allows the prediction of Urban Heat Islands with Machine Learning. Furthermore, we provide an interactive\nsystem for the visual analysis of the occurences of urban heat islands. Interactive parameter steering and similarity\nsearch allows to investigate impacts of different variables. \n\n\n\n\nPicture 2: Similarity search for urban heat islands, starting from Karlsruhe.\n\n\n\n\n\n\n\n\nPicture 3: Identified urban heat islands with high similarity for a station in Karlsruhe.\n\n\n\nThe identified similar weather station is positioned in Duisburg. The user is enabled to further explore the dataset\nby brushing and linking.\n\n\n\n\nRelated Scenarios\n\u00b6\n\n\n\n\nSmart City",
            "title": "Visual Analysis of Spatio-Temporal Event Predictions"
        },
        {
            "location": "/demos/uhi-visual-analysis/#visual-analysis-of-spatio-temporal-event-predictions",
            "text": "The well-known phenomenon of the Urban Heat Island effect in city areas is an important issue as a result from the\nongoing urbanization and industrialization process. Analyzing the causes for increased temperatures in urban areas is a\ncomplex task that depends on several variables like energy management, surface characteristics, weather, vegetation\nindex, population, industrial territory, transportation, air pollution and others. City and country planners are in need\nfor new technology that supports them during decision-making processes by meaningful visualization and data analysis\ntechniques to improve urban climate and the efficiency of energy management. We propose a visual analytics approach to\nenable domain experts to visually explore current temperature conditions of city areas and interactive steering\ntechniques on characteristic features to discover the influence of available variables on the outcome. The presented\nprediction models are intended to provide insights about future conditions to elicit the effect of various variables on\nthe intensity of urban heat islands.",
            "title": "Visual Analysis of Spatio-Temporal Event Predictions"
        },
        {
            "location": "/demos/uhi-visual-analysis/#used-data-sources",
            "text": "A variety of data source is used for our visual analysis of urban heat islands such as   Wunderground  ATKIS/ALKIS  DWD  ...   We incorporoated data sets for Germany, however, focused on the analysis of regions around Karlsruhe.    Picture 1: WU Station distribution in and around Karlsruhe",
            "title": "Used Data Sources"
        },
        {
            "location": "/demos/uhi-visual-analysis/#visual-analysis-of-urban-heat-islands",
            "text": "Our system allows the prediction of Urban Heat Islands with Machine Learning. Furthermore, we provide an interactive\nsystem for the visual analysis of the occurences of urban heat islands. Interactive parameter steering and similarity\nsearch allows to investigate impacts of different variables.    Picture 2: Similarity search for urban heat islands, starting from Karlsruhe.     Picture 3: Identified urban heat islands with high similarity for a station in Karlsruhe.  \nThe identified similar weather station is positioned in Duisburg. The user is enabled to further explore the dataset\nby brushing and linking.",
            "title": "Visual Analysis of Urban Heat Islands"
        },
        {
            "location": "/demos/uhi-visual-analysis/#related-scenarios",
            "text": "Smart City",
            "title": "Related Scenarios"
        },
        {
            "location": "/methods/",
            "text": "About Method\n\u00b6\n\n\nThis section contains a \nlist of methods\n that serve as a \ntheoretical background\n\nfor the other sections inside the documentation, especially for the \nDemos\n.\n\n\nSome methods depend on each other which is reflected in their ordering.",
            "title": "About Method"
        },
        {
            "location": "/methods/#about-method",
            "text": "This section contains a  list of methods  that serve as a  theoretical background \nfor the other sections inside the documentation, especially for the  Demos .  Some methods depend on each other which is reflected in their ordering.",
            "title": "About Method"
        },
        {
            "location": "/methods/exasol-docker/",
            "text": "Exasol Docker Support\n\u00b6\n\n\nExasol has developed support for running the Exasol RDBMS inside a dockerized environment.\n\n\nCurrently supported features:\n\n\n\n\ncreate / start / stop a database in a virtual cluster\n\n\nuse the UDF framework\n\n\nexpose ports from containers on the local host\n\n\nupdate the virtual cluster\n\n\ncreate backups on archive volumes\n\n\n\n\nHow to use this image\n\u00b6\n\n\n\n\nPull the image to your Docker host:\n  \n$\n docker pull exasol/docker-db:latest\n\n\n\nInstall \nexadt\n:\n  \n$\n git clone https://github.com/EXASOL/docker-db.git\n\n$\n \ncd\n docker-db\n\n\n\nInstall the \nexadt\n dependencies:\n  \n$\n pip install --upgrade -r exadt_requirements.txt\n\n\n\nCreate and configure your virtual EXASOL cluster by using the commands described in the \nexadt\n documentation below.\n\n\n\n\nEXASOL Docker Tool \u2014 \nexadt\n\u00b6\n\n\nThe \nexadt\n command-line tool is used to create, initialize, start, stop, update and delete a Docker based EXASOL\ncluster.\n\n\n\n\nNote\n\n\nexadt\n currently only supports single-host-clusters. See below for how to create a multi-host-cluster\n(with one container per host).\n\n\n\n\n1. Creating a cluster\n\u00b6\n\n\nSelect a root directory for your EXASOl cluster. It will be used to store the data, metadata and buckets of all local\ncontainers and should therefore be located on a filesystem with sufficient free space (min. 10 GiB are recommended).\n\n\n\n\nNote\n\n\nthis example creates only one node. You can easily create mutliple (virtual) nodes by using the --num-nodes option.\n\n\n\n\n$\n ./exadt create-cluster --root ~/MyCluster/ --create-root MyCluster\n\nSuccessfully created cluster 'MyCluster' with root directory '/home/user/MyCluster/'.\n\n\n\n\n\nexadt\n stores information about all clusters within \n$HOME/.exadt.conf\n and \n/etc/exadt.conf\n (if the current user has\nwrite permission in \n/etc\n). Both files are searched when executing a command that needs the cluster name as an\nargument.\n\n\nIn order to list all existing clusters you can use \nexadt list-clusters\n:\n\n\n$\n ./exadt list-clusters\n\n CLUSTER                     ROOT                                       IMAGE                    \n\n\n MyCluster                   /home/user/MyCluster                       <uninitialized>\n\n\n\n\n\n2. Initializing a cluster\n\u00b6\n\n\nAfter creating a cluster it has to be initialized. Mandatory parameters are:\n\n\n\n\nthe EXASOL Docker image \n\n\nthe license file\n\n\nthe type of EXAStorage devices (currently only 'file' is supported)\n\n\n\n\n$\n ./exadt init-cluster --image exasol/docker-db:latest --license ./license/license.xml --auto-storage MyCluster\n\nSuccessfully initialized configuration in '/home/user/MyCluster/EXAConf'.\n\n\nSuccessfully initialized root directory '/home/user/MyCluster/'.\n\n\n\n\n\nThis command creates subdirectories for each virtual node in the root directory. These are mounted as Docker volumes\nwithin each container (at '/exa') and contain all data, metadata and buckets.\n\n\nIt also creates the file \nEXAConf\n in the root directory, which contains the configuration for the whole cluster and\ncurrently has to be edited manually if a non-default setup is used.\n\n\nAutomatically creating and assigning file devices\n\u00b6\n\n\nThe example above uses the \n--auto-storage\n option which tells \nexadt\n to automatically create file-devices for all\nvirtual nodes (within the root directory). These devices are assigned to the EXAStorage volumes, that are also\nautomatically created. The devices need at least 10GiB of free space and use up to 100GiB of it (all devices combined).\n\n\nIf \n--auto-storage\n is used, you can skip the next step entirely (and \ncontinue with section 4\n).\n\n\n3. Adding EXAStorage devices\n\u00b6\n\n\n\n\nNote\n\n\nThis step can be skipped if \n--auto-storage\n has been used during initialization.\n\n\n\n\nNext, devices for EXAStorage need to be added. This can be done by executing:\n\n\n$\n ./exadt create-file-devices --size 80GiB MyCluster\n\nSuccessfully created the following file devices:\n\n\nNode 11 : ['/home/user/MyCluster/n11/data/storage/dev.1']\n\n\n\n\n\nAs you can see, the file devices are created within the \ndata/storage\n subdirectory of each node's Docker root. They are\ncreated as \nsparse files\n, i. e. their size is stated as the given size but they actually have size 0 and grow as new\ndata is being written.\n\n\nAll devices must be assigned to a 'disk'. A disk is a group of devices that can be assigned to an EXAStorage volume. The\ndisk name can be specified with the \n--disk\n parameter. If omitted, the newly created devices will be assigned to the\ndisk named 'default'.\n\n\nAssigning devices to volumes\n\u00b6\n\n\nAfter creating the devices, they have to be assigned to the corresponding volumes. If you did not use \n--auto-storage\n\n(see above), you have to edit \nEXAConf\n manually. Open it and locate the following section:\n\n\n[EXAVolume : DataVolume1]\n    Type = data\n    Nodes = 11\n    Disk =\n    Size =\n    Redundancy = 1\n\n\n\n\nNow add the name of the disk ('default', if you did not specify a name when executing \ncreate-file-devices\n) and the\nvolume size, e. g:\n\n\n    Disk = default\n    Size = 100GiB\n\n\n\n\nThen do the same for the section \n[EXAVolume : ArchiveVolume1]\n.\n\n\nMake sure not to make the volume too big! The specified size is the size that is available for the database, i. e. if\nthe redundancy is 2, the volume will actually use twice the amount of space! Also make sure to leave some free space for\nthe temporary volume, that is created by the database during startup.\n\n\n4. Starting a cluster\n\u00b6\n\n\nThe cluster is started using the \nexadt start-cluster\n command. Before the containers are actually created, \nexadt\n\nchecks if there is enough free space for the sparse files (if they grow to their max. size). If not, the startup will\nfail:\n\n\n$\n ./exadt start-cluster MyCluster\n\nFree space on '/' is only 22.2 GiB, but accumulated size of (sparse) file-devices is 80.0 GiB!\n\n\n'ERROR::DockerHandler: Check for space usage failed! Aborting startup.'\n\n\n\n\n\nIf that's the case, you can replace the existing devices with smaller ones and (optionally) place them on an external\npartition:\n\n\n$\n ./exadt create-file-devices --size 10GiB MyCluster --replace --path /mnt/data/\n\nDo you really want to replace all file-devices of cluster 'MyCluster'? (y/n): y\n\n\nThe following file devices have been removed:\n\n\nNode 11 : ['/home/user/MyCluster/n11/data/storage/dev.1']\n\n\nSuccessfully created the following file devices:\n\n\nNode 11 : ['/mnt/data/n11/dev.1']\n\n\n\n\n\nThe devices that are located outside of the root directory are mapped into the file system of the container (within\n\n/exa/data/storage/\n). They are often referenced as 'mapped devices'.\n\n\nNow the cluster can be started:\n\n\n$\n ./exadt start-cluster MyCluster\n\nCopying EXAConf to all node volumes.\n\n\nCreating private network 10.10.10.0/24 ('MyCluster_priv')... successful\n\n\nNo public network specified.\n\n\nCreating container 'MyCluster_11'... successful\n\n\nStarting container 'MyCluster_11'... successful\n\n\n\n\n\nThis command creates and starts all containers and networks. Each cluster uses one or two networks to connect the\ncontainers. These networks are not connected to other clusters.\n\n\nThe containers are (re)created each time the cluster is started and they are destroyed when it is deleted! All\npersistent data is stored within the root directory (and the mapped devices, if any).\n\n\n5. Inspecting a cluster\n\u00b6\n\n\nAll containers of an existing cluster can be listed by executing:\n\n\n$\n ./exadt ps MyCluster\n\n NODE ID      STATUS          IMAGE                       NAME   CONTAINER ID   CONTAINER NAME    EXPOSED PORTS       \n\n\n 11           Up 5 seconds    exasol/docker-db:6.0.0-d1   n11    e9347c3e41ca   MyCluster_11      8899->8888,6594->6583\n\n\n\n\n\nThe \nEXPOSED PORTS\n column shows all container ports that are reachable from outside the local host\n('host'->'container'), usually one for the database and one for BucketFS.\n\n\n6. Stopping a cluster\n\u00b6\n\n\nA cluster can be stopped by executing:\n\n\n$\n ./exadt stop-cluster MyCluster\n\nStopping container 'MyCluster_11'... successful\n\n\nRemoving container 'MyCluster_11'... successful\n\n\nRemoving network 'MyCluster_priv'... successful\n\n\n\n\n\nAs stated above, the containers are deleted when a cluster is stopped, but the root directory is preserved (as well as\nall mapped devices). Also the automatically created networks are removed.\n\n\n7. Updating a cluster\n\u00b6\n\n\nA cluster can be updated by exchanging the EXASOL Docker image (but it has to be stopped first):\n\n\n$\n git pull\n\n$\n pip install --upgrade -r exadt_requirements.txt\n\n$\n docker pull exasol/docker-db:latest\n\n$\n ./exadt update-cluster --image exasol/docker-db:latest MyCluster\n\nCluster 'MyCluster' has been successfully updated!\n\n\n- Image :  exasol/docker-db:6.0.0-d1 --> exasol/docker-db:6.0.0-d2\n\n\n- DB    :  6.0.0                     --> 6.0.1\n\n\n- OS    :  6.0.0                     --> 6.0.0\n\n\nRestart the cluster in order to apply the changes.\n\n\n\n\n\nThe cluster has to be restarted in order to recreate the containers from the new image (and trigger the internal update\nmechanism).\n\n\n8. Deleting a cluster\n\u00b6\n\n\nA cluster can be completely deleted by executing:\n\n\n$\n ./exadt delete-cluster MyCluster\n\nDo you really want to delete cluster 'MyCluster' (and all file-devices)?  (y/n): y\n\n\nDeleting directory '/mnt/data/n11'.\n\n\nDeleting directory '/mnt/data/n11'.\n\n\nDeleting root directory '/home/user/MyCluster/'.\n\n\nSuccessfully removed cluster 'MyCluster'.\n\n\n\n\n\nNote that all file devices (even the mapped ones) and the root directory are deleted. You can use \n--keep-root\n and\n\n--keep-mapped-devices\n in order to prevent this.\n\n\nA cluster has to be stopped before it can be deleted (even if all containers are down)!\n\n\nCreating a stand-alone EXASOL container\n\u00b6\n\n\nStarting with version 6.0.2-d1, there is no more separate \"self-contained\" image version. You can simply create an\nEXASOL container from the EXASOL docker image using the following command:\n\n\n$\n docker run --name exasoldb -p \n127\n.0.0.1:8899:8888 --detach --privileged \n\\\n\n    --stop-timeout \n120\n  exasol/docker-db:latest\n\n\n\n\nIn this example port 8888 (within the container) is exposed on the local port 8899. Use this port to connect to the DB.\n\n\nAll data is stored within the container and lost when the container is removed. In order to make it persistent, you'd\nhave to mount a volume into the container at \n/exa\n, for example:\n\n\n$\n docker run --name exasoldb  -p \n127\n.0.0.1:8899:8888 --detach --privileged \n\\\n\n    --stop-timeout \n120\n -v exa_volume:/exa exasol/docker-db:latest\n\n\n\n\nSee \nthe Docker volumes documentation\n for more examples on how\nto create and manage persistent volumes.\n\n\n\n\nNote\n\n\nMake sure the database has been shut down correctly before stopping the container!\n\n\n\n\nA high stop-timeout (see example above) increases the chance that the DB can be shut down gracefully before the\ncontainer is stopped, but it's not guaranteed. However, it can be stopped manually by executing the following command\nwithin the container (after attaching to it):\n\n\n$\n dwad_client stop-wait DB1\n\n\n\n\nOr from outside the container:\n\n\n$\n docker \nexec\n -ti exasoldb dwad_client stop-wait DB1\n\n\n\n\nUpdating the persistent volume of a stand-alone EXASOL container\n\u00b6\n\n\nStarting with version 6.0.3-d1, an existing persistent volume can be updated (for use with a later version of an EXASOL\nimage) by calling the following command with the \nnew\n image:\n\n\n$\n docker run -v exa_volume:/exa exasol/docker-db:6.0.3-d1 update-sc\n\n\n\n\nIf everything works correctly, you should see output similar to this:\n\n\nUpdating EXAConf '/exa/etc/EXAConf' from version '6.0.2' to '6.0.3'\n\n\nContainer has been successfully updated!\n\n\n- Image ver. :  6.0.2-d1 --> 6.0.3-d1\n\n\n- DB ver.    :  6.0.2 --> 6.0.3\n\n\n- OS ver.    :  6.0.2 --> 6.0.3\n\n\n\n\n\nAfter that, a new container can be created (from the new image) using the old / updated volume.\n\n\nCreating a multi-host EXASOL cluster\n\u00b6\n\n\nStarting with version 6.0.7-d1, it's possible to create multiple containers on different hosts and connect them to a\ncluster (one container per host).\n\n\n1. Create the configuration\n\u00b6\n\n\nFirst you have to create the configuration for the cluster. There are two possible ways to do so:\n\n\na. Create an /exa/ directory template (RECOMMENDED):\n\u00b6\n\n\nExecute the following command (\n--num-nodes\n is the number of containers in the cluster):\n\n\n$\n docker run -v \n$HOME\n/exa_template:/exa --rm -i exasol/docker-db:latest \n\\\n\n    init-sc --template --num-nodes \n3\n\n\n\n\n\nAfter the command has finished, the directory \n$HOME/exa_template\n contains all subdirectories as well as an EXAConf\ntemplate (in \n/etc\n). The EXAConf is also printed to stdout.\n\n\nb. Create an EXAConf template\n\u00b6\n\n\nYou can create a template file and redirect it to wherever you want by executing: \n\n\n$\n docker run --rm -i exasol/docker-db:latest init-sc --template \n\\\n\n    --num-nodes \n3\n > ~/MyExaConf\n\n\n\n\n\n\nNote\n\n\nWe recommend to create an /exa/ template directory and the following steps assume that you did so.\nIf you choose to only create the EXAConf file, you have to build a new Docker image with it and create\nthe EXAStorage devices files within that image.\n\n\n\n\n2. Complete the configuration\n\u00b6\n\n\nThe EXAConf template has to be completed before the cluster can be started. You have to provide:\n\n\nThe private network of all nodes:\n\u00b6\n\n\n[Node : 11]\n\n\n    PrivateNet = 10.10.10.11/24 # <-- replace with the real network\n\n\n\n\n\nThe EXAStorage devices on all nodes:\n\u00b6\n\n\n[[Disk : default]]\n\n\n        Devices = dev.1    #'dev.1.data' and 'dev.1.meta' files must be located in '/exa/data/storage'\n\n\n\n\n\n\n\nNote\n\n\nYou can leave this entry as it is if you create the devices as described below.\n\n\n\n\nThe EXAVolume sizes:\n\u00b6\n\n\n[EXAVolume : DataVolume1]\n\n\n    Type = data\n\n\n    Nodes = 11, 12, 13\n\n\n    Disk = default\n\n\n    #\n Volume size \n(\ne. g. \n'1 TiB'\n)\n\n\n    Size =  # <-- enter volume size here\n\n\n\n\n\nThe network port numbers (optional)\n\u00b6\n\n\nIf you are using the host network mode (see \"Start the cluster\" below), you may have to adjust the port numbers used by\nthe EXASOL services. The one that's most likely to collide is the SSH daemon, which is using the well-known port 22. You\ncan change it in EXAConf:\n\n\n[Global]\n\n\n    SSHPort = 22  # <-- replace with any unused port number\n\n\n\n\n\nThe other EXASOL services (e. g. Cored, BucketFS and the DB itself) are using port numbers above 1024. However, you can\nchange them all by editing EXAConf.\n\n\nThe nameservers (optional):\n\u00b6\n\n\n[Global]\n\n\n    ...\n\n\n    #\n Comma-separated list of nameservers \nfor\n this cluster.\n\n    NameServers =\n\n\n\n\n\n3. Copy the configuration to all nodes\n\u00b6\n\n\nCopy the \n$HOME/exa_template/\n directory to all cluster nodes (the exact path is not relevant, but should be identical\non all nodes).\n\n\n4. Create the EXAStorage device files\n\u00b6\n\n\nYou can create the EXAStorage device files by executing (on each node):\n\n\n$\n dd \nif\n=\n/dev/zero \nof\n=\n$HOME\n/exa_template/data/storage/dev.1.data \nbs\n=\n1M \ncount\n=\n1\n \nseek\n=\n999\n\n\n$\n touch \n$HOME\n/exa_template/data/storage/dev.1.meta\n\n\n\n\nThis will create a sparse file of 1GB (1000 blocks of 1 MB) that holds the data and also a file that holds the metadata\nfor that device. Adjust the size to your needs.\n\n\n\n\nNote\n\n\nAlternatively you can partition a block-device (the meta partition needs only 2 MB) and place symlinks (or new\ndevice files) named \ndev.1.data\n and \ndev.1.meta\n in the same directory.\n\n\n\n\n5. Start the cluster\n\u00b6\n\n\nThe cluster is started by creating all containers individually and passing each of them its ID from the EXAConf. For\n\nn11\n the command would be:\n\n\n$\n docker run --detach --network\n=\nhost --privileged \n\\\n\n    -v \n$HOME\n/exa_template:/exa exasol/docker-db:latest \n\\\n\n    init-sc --node-id \n11\n\n\n\n\n\n\n\nNote\n\n\nThis example uses the host network stack, i. e. the containers are directly accessing a host interface to connect\nto each other. There is no need to expose ports in this mode: they are all accessible on the host.\n\n\n\n\nInstalling custom JDBC drivers\n\u00b6\n\n\nStarting with version 6.0.7-d1, custom JDBC drivers can be added by uploading them into a bucket. The bucket and path\nfor the drivers can be configured in each database section of EXAConf. The default configuration is:\n\n\n[DB : DB1]\n\n\n    ...\n\n\n    #\n OPTIONAL: JDBC driver configuration\n\n    [[JDBC]]\n\n\n        BucketFS = bfsdefault\n\n\n        Bucket = default\n\n\n        #\n Directory within the bucket that contains the drivers\n\n        Dir = drivers/jdbc\n\n\n\n\n\nIn order for the database to find the driver, you need to upload it into a subdirectory of \ndrivers/jdbc\n of the default\nbucket (which is automatically created if you don't modify EXAConf). See the section \nInstalling Oracle drivers\n for\nhelp on how to upload files to BucketFS.\n\n\nIn addition to the driver file(s), you also have to create and upload a file called \nsettings.cfg\n , that looks like\nthis:\n\n\nDRIVERNAME=MY_JDBC_DRIVER\n\n\nJAR=my_jdbc_driver.jar\n\n\nDRIVERMAIN=com.mydriver.jdbc.Driver\n\n\nPREFIX=jdbc:mydriver:\n\n\nFETCHSIZE=100000\n\n\nINSERTSIZE=-1\n\n\n\n\n\nChange the variables DRIVERNAME, JAR, DRIVERMAIN and PREFIX according to your driver and upload the file (into the\n\nsame directory\n as the driver itself).\n\n\n\n\nWarning\n\n\nDo not modify the last two lines!\n\n\n\n\nIf you use the default bucket and the default path, you can add multiple JDBC drivers during runtime. The DB will find\nthem without having to restart it (as long as they're located in a subfolder of the default path). Otherwise, a\ncontainer restart is required.\n\n\nInstalling Oracle drivers\n\u00b6\n\n\nStarting with version 6.0.7-d1, Oracle drivers can be added by uploading them into a bucket. The bucket and path for the\ndrivers can be configured in each database section of EXAConf. The default configuration is:\n\n\n[DB : DB1]\n\n\n    ...\n\n\n    #\n OPTIONAL: Oracle driver configuration\n\n    [[ORACLE]]\n\n\n        BucketFS = bfsdefault\n\n\n        Bucket = default\n\n\n        #\n Directory within the bucket that contains the drivers\n\n        Dir = drivers/oracle\n\n\n\n\n\nIn order for the database to find the driver, you have to upload it to \ndrivers/oracle\n of the default bucket (which is\nautomatically created if you don't modify EXAConf).\n\n\nYou can use \ncurl\n for uploading, e. g.:\n\n\n$\n curl -v -X PUT -T instantclient-basic-linux.x64-12.1.0.2.0.zip \n\\\n\n    http://w:PASSWORD@10.10.10.11:6583/default/drivers/oracle/instantclient-basic-linux.x64-12.2.0.1.0.zip\n\n\n\n\nReplace \nPASSWORD\n with the \nWritePasswd\n for the bucket. You can find it in the EXAConf. It's base64 encoded and can be\ndecoded like this:\n\n\n$\n awk \n'/WritePasswd/{ print $3; }'\n EXAConf \n|\n base64 -d\n\n\n\n\n\n\nNote\n\n\nThe only currently supported driver version is 12.1.0.20. Please download the package\n\ninstantclient-basic-linux.x64-12.1.0.2.0.zip\n from \nhttp://www.oracle.com\n and upload it as described above.",
            "title": "Exasol Docker Support"
        },
        {
            "location": "/methods/exasol-docker/#exasol-docker-support",
            "text": "Exasol has developed support for running the Exasol RDBMS inside a dockerized environment.  Currently supported features:   create / start / stop a database in a virtual cluster  use the UDF framework  expose ports from containers on the local host  update the virtual cluster  create backups on archive volumes",
            "title": "Exasol Docker Support"
        },
        {
            "location": "/methods/exasol-docker/#how-to-use-this-image",
            "text": "Pull the image to your Docker host:\n   $  docker pull exasol/docker-db:latest  Install  exadt :\n   $  git clone https://github.com/EXASOL/docker-db.git $   cd  docker-db  Install the  exadt  dependencies:\n   $  pip install --upgrade -r exadt_requirements.txt  Create and configure your virtual EXASOL cluster by using the commands described in the  exadt  documentation below.",
            "title": "How to use this image"
        },
        {
            "location": "/methods/exasol-docker/#exasol-docker-tool-exadt",
            "text": "The  exadt  command-line tool is used to create, initialize, start, stop, update and delete a Docker based EXASOL\ncluster.   Note  exadt  currently only supports single-host-clusters. See below for how to create a multi-host-cluster\n(with one container per host).",
            "title": "EXASOL Docker Tool \u2014 exadt"
        },
        {
            "location": "/methods/exasol-docker/#1-creating-a-cluster",
            "text": "Select a root directory for your EXASOl cluster. It will be used to store the data, metadata and buckets of all local\ncontainers and should therefore be located on a filesystem with sufficient free space (min. 10 GiB are recommended).   Note  this example creates only one node. You can easily create mutliple (virtual) nodes by using the --num-nodes option.   $  ./exadt create-cluster --root ~/MyCluster/ --create-root MyCluster Successfully created cluster 'MyCluster' with root directory '/home/user/MyCluster/'.   exadt  stores information about all clusters within  $HOME/.exadt.conf  and  /etc/exadt.conf  (if the current user has\nwrite permission in  /etc ). Both files are searched when executing a command that needs the cluster name as an\nargument.  In order to list all existing clusters you can use  exadt list-clusters :  $  ./exadt list-clusters  CLUSTER                     ROOT                                       IMAGE                       MyCluster                   /home/user/MyCluster                       <uninitialized>",
            "title": "1. Creating a cluster"
        },
        {
            "location": "/methods/exasol-docker/#2-initializing-a-cluster",
            "text": "After creating a cluster it has to be initialized. Mandatory parameters are:   the EXASOL Docker image   the license file  the type of EXAStorage devices (currently only 'file' is supported)   $  ./exadt init-cluster --image exasol/docker-db:latest --license ./license/license.xml --auto-storage MyCluster Successfully initialized configuration in '/home/user/MyCluster/EXAConf'.  Successfully initialized root directory '/home/user/MyCluster/'.   This command creates subdirectories for each virtual node in the root directory. These are mounted as Docker volumes\nwithin each container (at '/exa') and contain all data, metadata and buckets.  It also creates the file  EXAConf  in the root directory, which contains the configuration for the whole cluster and\ncurrently has to be edited manually if a non-default setup is used.",
            "title": "2. Initializing a cluster"
        },
        {
            "location": "/methods/exasol-docker/#automatically-creating-and-assigning-file-devices",
            "text": "The example above uses the  --auto-storage  option which tells  exadt  to automatically create file-devices for all\nvirtual nodes (within the root directory). These devices are assigned to the EXAStorage volumes, that are also\nautomatically created. The devices need at least 10GiB of free space and use up to 100GiB of it (all devices combined).  If  --auto-storage  is used, you can skip the next step entirely (and  continue with section 4 ).",
            "title": "Automatically creating and assigning file devices"
        },
        {
            "location": "/methods/exasol-docker/#3-adding-exastorage-devices",
            "text": "Note  This step can be skipped if  --auto-storage  has been used during initialization.   Next, devices for EXAStorage need to be added. This can be done by executing:  $  ./exadt create-file-devices --size 80GiB MyCluster Successfully created the following file devices:  Node 11 : ['/home/user/MyCluster/n11/data/storage/dev.1']   As you can see, the file devices are created within the  data/storage  subdirectory of each node's Docker root. They are\ncreated as  sparse files , i. e. their size is stated as the given size but they actually have size 0 and grow as new\ndata is being written.  All devices must be assigned to a 'disk'. A disk is a group of devices that can be assigned to an EXAStorage volume. The\ndisk name can be specified with the  --disk  parameter. If omitted, the newly created devices will be assigned to the\ndisk named 'default'.",
            "title": "3. Adding EXAStorage devices"
        },
        {
            "location": "/methods/exasol-docker/#assigning-devices-to-volumes",
            "text": "After creating the devices, they have to be assigned to the corresponding volumes. If you did not use  --auto-storage \n(see above), you have to edit  EXAConf  manually. Open it and locate the following section:  [EXAVolume : DataVolume1]\n    Type = data\n    Nodes = 11\n    Disk =\n    Size =\n    Redundancy = 1  Now add the name of the disk ('default', if you did not specify a name when executing  create-file-devices ) and the\nvolume size, e. g:      Disk = default\n    Size = 100GiB  Then do the same for the section  [EXAVolume : ArchiveVolume1] .  Make sure not to make the volume too big! The specified size is the size that is available for the database, i. e. if\nthe redundancy is 2, the volume will actually use twice the amount of space! Also make sure to leave some free space for\nthe temporary volume, that is created by the database during startup.",
            "title": "Assigning devices to volumes"
        },
        {
            "location": "/methods/exasol-docker/#4-starting-a-cluster",
            "text": "The cluster is started using the  exadt start-cluster  command. Before the containers are actually created,  exadt \nchecks if there is enough free space for the sparse files (if they grow to their max. size). If not, the startup will\nfail:  $  ./exadt start-cluster MyCluster Free space on '/' is only 22.2 GiB, but accumulated size of (sparse) file-devices is 80.0 GiB!  'ERROR::DockerHandler: Check for space usage failed! Aborting startup.'   If that's the case, you can replace the existing devices with smaller ones and (optionally) place them on an external\npartition:  $  ./exadt create-file-devices --size 10GiB MyCluster --replace --path /mnt/data/ Do you really want to replace all file-devices of cluster 'MyCluster'? (y/n): y  The following file devices have been removed:  Node 11 : ['/home/user/MyCluster/n11/data/storage/dev.1']  Successfully created the following file devices:  Node 11 : ['/mnt/data/n11/dev.1']   The devices that are located outside of the root directory are mapped into the file system of the container (within /exa/data/storage/ ). They are often referenced as 'mapped devices'.  Now the cluster can be started:  $  ./exadt start-cluster MyCluster Copying EXAConf to all node volumes.  Creating private network 10.10.10.0/24 ('MyCluster_priv')... successful  No public network specified.  Creating container 'MyCluster_11'... successful  Starting container 'MyCluster_11'... successful   This command creates and starts all containers and networks. Each cluster uses one or two networks to connect the\ncontainers. These networks are not connected to other clusters.  The containers are (re)created each time the cluster is started and they are destroyed when it is deleted! All\npersistent data is stored within the root directory (and the mapped devices, if any).",
            "title": "4. Starting a cluster"
        },
        {
            "location": "/methods/exasol-docker/#5-inspecting-a-cluster",
            "text": "All containers of an existing cluster can be listed by executing:  $  ./exadt ps MyCluster  NODE ID      STATUS          IMAGE                       NAME   CONTAINER ID   CONTAINER NAME    EXPOSED PORTS          11           Up 5 seconds    exasol/docker-db:6.0.0-d1   n11    e9347c3e41ca   MyCluster_11      8899->8888,6594->6583   The  EXPOSED PORTS  column shows all container ports that are reachable from outside the local host\n('host'->'container'), usually one for the database and one for BucketFS.",
            "title": "5. Inspecting a cluster"
        },
        {
            "location": "/methods/exasol-docker/#6-stopping-a-cluster",
            "text": "A cluster can be stopped by executing:  $  ./exadt stop-cluster MyCluster Stopping container 'MyCluster_11'... successful  Removing container 'MyCluster_11'... successful  Removing network 'MyCluster_priv'... successful   As stated above, the containers are deleted when a cluster is stopped, but the root directory is preserved (as well as\nall mapped devices). Also the automatically created networks are removed.",
            "title": "6. Stopping a cluster"
        },
        {
            "location": "/methods/exasol-docker/#7-updating-a-cluster",
            "text": "A cluster can be updated by exchanging the EXASOL Docker image (but it has to be stopped first):  $  git pull $  pip install --upgrade -r exadt_requirements.txt $  docker pull exasol/docker-db:latest $  ./exadt update-cluster --image exasol/docker-db:latest MyCluster Cluster 'MyCluster' has been successfully updated!  - Image :  exasol/docker-db:6.0.0-d1 --> exasol/docker-db:6.0.0-d2  - DB    :  6.0.0                     --> 6.0.1  - OS    :  6.0.0                     --> 6.0.0  Restart the cluster in order to apply the changes.   The cluster has to be restarted in order to recreate the containers from the new image (and trigger the internal update\nmechanism).",
            "title": "7. Updating a cluster"
        },
        {
            "location": "/methods/exasol-docker/#8-deleting-a-cluster",
            "text": "A cluster can be completely deleted by executing:  $  ./exadt delete-cluster MyCluster Do you really want to delete cluster 'MyCluster' (and all file-devices)?  (y/n): y  Deleting directory '/mnt/data/n11'.  Deleting directory '/mnt/data/n11'.  Deleting root directory '/home/user/MyCluster/'.  Successfully removed cluster 'MyCluster'.   Note that all file devices (even the mapped ones) and the root directory are deleted. You can use  --keep-root  and --keep-mapped-devices  in order to prevent this.  A cluster has to be stopped before it can be deleted (even if all containers are down)!",
            "title": "8. Deleting a cluster"
        },
        {
            "location": "/methods/exasol-docker/#creating-a-stand-alone-exasol-container",
            "text": "Starting with version 6.0.2-d1, there is no more separate \"self-contained\" image version. You can simply create an\nEXASOL container from the EXASOL docker image using the following command:  $  docker run --name exasoldb -p  127 .0.0.1:8899:8888 --detach --privileged  \\ \n    --stop-timeout  120   exasol/docker-db:latest  In this example port 8888 (within the container) is exposed on the local port 8899. Use this port to connect to the DB.  All data is stored within the container and lost when the container is removed. In order to make it persistent, you'd\nhave to mount a volume into the container at  /exa , for example:  $  docker run --name exasoldb  -p  127 .0.0.1:8899:8888 --detach --privileged  \\ \n    --stop-timeout  120  -v exa_volume:/exa exasol/docker-db:latest  See  the Docker volumes documentation  for more examples on how\nto create and manage persistent volumes.   Note  Make sure the database has been shut down correctly before stopping the container!   A high stop-timeout (see example above) increases the chance that the DB can be shut down gracefully before the\ncontainer is stopped, but it's not guaranteed. However, it can be stopped manually by executing the following command\nwithin the container (after attaching to it):  $  dwad_client stop-wait DB1  Or from outside the container:  $  docker  exec  -ti exasoldb dwad_client stop-wait DB1",
            "title": "Creating a stand-alone EXASOL container"
        },
        {
            "location": "/methods/exasol-docker/#updating-the-persistent-volume-of-a-stand-alone-exasol-container",
            "text": "Starting with version 6.0.3-d1, an existing persistent volume can be updated (for use with a later version of an EXASOL\nimage) by calling the following command with the  new  image:  $  docker run -v exa_volume:/exa exasol/docker-db:6.0.3-d1 update-sc  If everything works correctly, you should see output similar to this:  Updating EXAConf '/exa/etc/EXAConf' from version '6.0.2' to '6.0.3'  Container has been successfully updated!  - Image ver. :  6.0.2-d1 --> 6.0.3-d1  - DB ver.    :  6.0.2 --> 6.0.3  - OS ver.    :  6.0.2 --> 6.0.3   After that, a new container can be created (from the new image) using the old / updated volume.",
            "title": "Updating the persistent volume of a stand-alone EXASOL container"
        },
        {
            "location": "/methods/exasol-docker/#creating-a-multi-host-exasol-cluster",
            "text": "Starting with version 6.0.7-d1, it's possible to create multiple containers on different hosts and connect them to a\ncluster (one container per host).",
            "title": "Creating a multi-host EXASOL cluster"
        },
        {
            "location": "/methods/exasol-docker/#1-create-the-configuration",
            "text": "First you have to create the configuration for the cluster. There are two possible ways to do so:",
            "title": "1. Create the configuration"
        },
        {
            "location": "/methods/exasol-docker/#a-create-an-exa-directory-template-recommended",
            "text": "Execute the following command ( --num-nodes  is the number of containers in the cluster):  $  docker run -v  $HOME /exa_template:/exa --rm -i exasol/docker-db:latest  \\ \n    init-sc --template --num-nodes  3   After the command has finished, the directory  $HOME/exa_template  contains all subdirectories as well as an EXAConf\ntemplate (in  /etc ). The EXAConf is also printed to stdout.",
            "title": "a. Create an /exa/ directory template (RECOMMENDED):"
        },
        {
            "location": "/methods/exasol-docker/#b-create-an-exaconf-template",
            "text": "You can create a template file and redirect it to wherever you want by executing:   $  docker run --rm -i exasol/docker-db:latest init-sc --template  \\ \n    --num-nodes  3  > ~/MyExaConf   Note  We recommend to create an /exa/ template directory and the following steps assume that you did so.\nIf you choose to only create the EXAConf file, you have to build a new Docker image with it and create\nthe EXAStorage devices files within that image.",
            "title": "b. Create an EXAConf template"
        },
        {
            "location": "/methods/exasol-docker/#2-complete-the-configuration",
            "text": "The EXAConf template has to be completed before the cluster can be started. You have to provide:",
            "title": "2. Complete the configuration"
        },
        {
            "location": "/methods/exasol-docker/#the-private-network-of-all-nodes",
            "text": "[Node : 11]      PrivateNet = 10.10.10.11/24 # <-- replace with the real network",
            "title": "The private network of all nodes:"
        },
        {
            "location": "/methods/exasol-docker/#the-exastorage-devices-on-all-nodes",
            "text": "[[Disk : default]]          Devices = dev.1    #'dev.1.data' and 'dev.1.meta' files must be located in '/exa/data/storage'    Note  You can leave this entry as it is if you create the devices as described below.",
            "title": "The EXAStorage devices on all nodes:"
        },
        {
            "location": "/methods/exasol-docker/#the-exavolume-sizes",
            "text": "[EXAVolume : DataVolume1]      Type = data      Nodes = 11, 12, 13      Disk = default      #  Volume size  ( e. g.  '1 TiB' )      Size =  # <-- enter volume size here",
            "title": "The EXAVolume sizes:"
        },
        {
            "location": "/methods/exasol-docker/#the-network-port-numbers-optional",
            "text": "If you are using the host network mode (see \"Start the cluster\" below), you may have to adjust the port numbers used by\nthe EXASOL services. The one that's most likely to collide is the SSH daemon, which is using the well-known port 22. You\ncan change it in EXAConf:  [Global]      SSHPort = 22  # <-- replace with any unused port number   The other EXASOL services (e. g. Cored, BucketFS and the DB itself) are using port numbers above 1024. However, you can\nchange them all by editing EXAConf.",
            "title": "The network port numbers (optional)"
        },
        {
            "location": "/methods/exasol-docker/#the-nameservers-optional",
            "text": "[Global]      ...      #  Comma-separated list of nameservers  for  this cluster.     NameServers =",
            "title": "The nameservers (optional):"
        },
        {
            "location": "/methods/exasol-docker/#3-copy-the-configuration-to-all-nodes",
            "text": "Copy the  $HOME/exa_template/  directory to all cluster nodes (the exact path is not relevant, but should be identical\non all nodes).",
            "title": "3. Copy the configuration to all nodes"
        },
        {
            "location": "/methods/exasol-docker/#4-create-the-exastorage-device-files",
            "text": "You can create the EXAStorage device files by executing (on each node):  $  dd  if = /dev/zero  of = $HOME /exa_template/data/storage/dev.1.data  bs = 1M  count = 1   seek = 999  $  touch  $HOME /exa_template/data/storage/dev.1.meta  This will create a sparse file of 1GB (1000 blocks of 1 MB) that holds the data and also a file that holds the metadata\nfor that device. Adjust the size to your needs.   Note  Alternatively you can partition a block-device (the meta partition needs only 2 MB) and place symlinks (or new\ndevice files) named  dev.1.data  and  dev.1.meta  in the same directory.",
            "title": "4. Create the EXAStorage device files"
        },
        {
            "location": "/methods/exasol-docker/#5-start-the-cluster",
            "text": "The cluster is started by creating all containers individually and passing each of them its ID from the EXAConf. For n11  the command would be:  $  docker run --detach --network = host --privileged  \\ \n    -v  $HOME /exa_template:/exa exasol/docker-db:latest  \\ \n    init-sc --node-id  11    Note  This example uses the host network stack, i. e. the containers are directly accessing a host interface to connect\nto each other. There is no need to expose ports in this mode: they are all accessible on the host.",
            "title": "5. Start the cluster"
        },
        {
            "location": "/methods/exasol-docker/#installing-custom-jdbc-drivers",
            "text": "Starting with version 6.0.7-d1, custom JDBC drivers can be added by uploading them into a bucket. The bucket and path\nfor the drivers can be configured in each database section of EXAConf. The default configuration is:  [DB : DB1]      ...      #  OPTIONAL: JDBC driver configuration     [[JDBC]]          BucketFS = bfsdefault          Bucket = default          #  Directory within the bucket that contains the drivers         Dir = drivers/jdbc   In order for the database to find the driver, you need to upload it into a subdirectory of  drivers/jdbc  of the default\nbucket (which is automatically created if you don't modify EXAConf). See the section  Installing Oracle drivers  for\nhelp on how to upload files to BucketFS.  In addition to the driver file(s), you also have to create and upload a file called  settings.cfg  , that looks like\nthis:  DRIVERNAME=MY_JDBC_DRIVER  JAR=my_jdbc_driver.jar  DRIVERMAIN=com.mydriver.jdbc.Driver  PREFIX=jdbc:mydriver:  FETCHSIZE=100000  INSERTSIZE=-1   Change the variables DRIVERNAME, JAR, DRIVERMAIN and PREFIX according to your driver and upload the file (into the same directory  as the driver itself).   Warning  Do not modify the last two lines!   If you use the default bucket and the default path, you can add multiple JDBC drivers during runtime. The DB will find\nthem without having to restart it (as long as they're located in a subfolder of the default path). Otherwise, a\ncontainer restart is required.",
            "title": "Installing custom JDBC drivers"
        },
        {
            "location": "/methods/exasol-docker/#installing-oracle-drivers",
            "text": "Starting with version 6.0.7-d1, Oracle drivers can be added by uploading them into a bucket. The bucket and path for the\ndrivers can be configured in each database section of EXAConf. The default configuration is:  [DB : DB1]      ...      #  OPTIONAL: Oracle driver configuration     [[ORACLE]]          BucketFS = bfsdefault          Bucket = default          #  Directory within the bucket that contains the drivers         Dir = drivers/oracle   In order for the database to find the driver, you have to upload it to  drivers/oracle  of the default bucket (which is\nautomatically created if you don't modify EXAConf).  You can use  curl  for uploading, e. g.:  $  curl -v -X PUT -T instantclient-basic-linux.x64-12.1.0.2.0.zip  \\ \n    http://w:PASSWORD@10.10.10.11:6583/default/drivers/oracle/instantclient-basic-linux.x64-12.2.0.1.0.zip  Replace  PASSWORD  with the  WritePasswd  for the bucket. You can find it in the EXAConf. It's base64 encoded and can be\ndecoded like this:  $  awk  '/WritePasswd/{ print $3; }'  EXAConf  |  base64 -d   Note  The only currently supported driver version is 12.1.0.20. Please download the package instantclient-basic-linux.x64-12.1.0.2.0.zip  from  http://www.oracle.com  and upload it as described above.",
            "title": "Installing Oracle drivers"
        },
        {
            "location": "/methods/exasol-geo/",
            "text": "Exasol GEO data and spatial indexes\n\u00b6\n\n\nTBD",
            "title": "Exasol GEO data and spatial indexes"
        },
        {
            "location": "/methods/exasol-geo/#exasol-geo-data-and-spatial-indexes",
            "text": "TBD",
            "title": "Exasol GEO data and spatial indexes"
        },
        {
            "location": "/methods/exasol-udf/",
            "text": "Exasol UDF framework\n\u00b6\n\n\nUDF scripts provides you the ability to program your own analyses,\nprocessing or generation functions and execute them in parallel inside\nExasol's high performance cluster (In Database Analytics). Through\nthis principle many problems can be solved very efficiently which were\nnot possible with SQL statements. Via UDF scripts you therefore get a\nhighly flexible interface for implementing nearly every\nrequirement. Hence you become a HPC developer without the need for\ncertain previous knowledge.\n\n\nWith UDF scripts you can implement the following extensions:\n\n\n\n\nScalar functions\n\n\nAggregate functions\n\n\nAnalytical functions\n\n\nMapReduce algorithms\n\n\nUser-defined ETL processes\n\n\n\n\nTo take advantage of the variety of UDF scripts, you only need to\ncreate a script and use this script afterwards within a SELECT\nstatement. By this close embedding within SQL you can achieve ideal\nperformance and scalability.\n\n\nExasol supports multiple programming languages (Java, Lua, Python, R)\nto simplify your start. Furthermore the different languages provide\nyou different advantages due to their respective focus\n(e.g. statistical functions in R) and the different delivered\nlibraries (XML parser, etc.). Thus, please note the next chapters in\nwhich the specific characteristics of each language is described.\n\n\nThe actual versions of the scripting languages can be listed with\ncorresponding metadata functions.\n\n\nWithin the \nCREATE SCRIPT STATEMENT\n command, you have to define the\ntype of input and output values. You can e.g. create a script which\ngenerates multiple result rows out of a single input row\n(\nSCALAR ... EMITS\n).\n\n\n\n\n\n\nInput values:\n\n\n\n\n\n\nSCALAR\n\n\nThe keyword SCALAR specifies that the script processes single\n input rows. It's code is therefore called once per input row.\n\n\n\n\n\n\nSET\n\n\nIf you define the option SET, then the processing refers to a\n set of input values. Within the code, you can iterate through\n those values.\n\n\n\n\n\n\n\n\n\n\nOutput values:\n\n\n\n\n\n\nRETURNS\n\n\nIn this case the script returns a single value.\n\n\n\n\n\n\nEMITS\n\n\nIf the keyword EMITS was defined, the script can create\n (emit) multiple result rows (tuples). In case of input type\n SET, the EMITS result can only be used alone, thus not be\n combined with other expressions. However you can of course\n nest it through a subselect to do further processing of those\n intermediate results.\n\n\n\n\n\n\n\n\n\n\nThe data types of input and output parameters can be defined to\nspecify the conversion between internal data types and the database\nSQL data types. If you don't specify them, the script has to handle\nthat dynamically (see details and examples below).\n\n\nPlease note that input parameters of scripts are always treated\ncase-sensitive, similar to the script code itself. This is different\nto SQL identifiers which are only treated case-sensitive when being\ndelimited.\n\n\nScripts must contain the main function \nrun()\n. This function is\ncalled with a parameter providing access to the input data of\nExasol. If your script processes multiple input tuples (thus a SET\nscript), you can iterate through the single tuples by the use of this\nparameter.\n\n\nPlease note the information in the following table:\n\n\n\n\n\n\nInternal processing\n\n\nDuring the processing of an SELECT statement, multiple virtual\nmachines are started for each script and node, which process the\ndata independently.\n\n\nFor scalar functions, the input rows are distributed across those\nvirtual machines to achieve a maximal parallelism.\n\n\nIn case of SET input tuples, the virtual machines are used per\ngroup (if you specify a GROUP BY clause). If no GROUP BY clause is\ndefined, then only one group exists and therefore only one node\nand virtual machine can process the data.\n\n\n\n\n\n\nORDER BY\n\n\nEither when creating a script or when calling it you can specify\nan ORDER BY clause which leads to a sorted processing of the\ngroups of SET input data. For some algorithms, this can be\nreasonable. But if it is necessary for the algorithm, then you\nshould already specify this clause during the creation to avoid\nwrong results due to misuse.\n\n\n\n\n\n\nPerformance\n\n\nThe performance of the different languages can hardly be compared,\nsince the specific elements of the languages can have different\ncapacities. Thus a string processing can be faster in one\nlanguage, while the XML parsing is faster in the other one.\n\n\nHowever, we generally recommend to use Lua if performance is the\nmost important criteria. Due to technical reasons, Lua is\nintegrated in Exasol in the most native way and therefore has the\nsmallest process overhead.\n\n\n\n\n\n\nIntroducing examples\n\u00b6\n\n\nIn this chapter we provide some introducing Lua examples to give you a\ngeneral idea about the functionality of user defined scripts. Examples\nfor the other programming languages can be found in the later chapters\nexplaining the details of each language.\n\n\nScalar functions\n\u00b6\n\n\nUser defined scalar functions (keyword \nSCALAR\n) are the simplest case\nof user defined scripts, returning one scalar result value (keyword\n\nRETURNS\n) or several result tuples (keyword \nSET\n) for each input\nvalue (or tuple).\n\n\nPlease note that scripts have to implement a function \nrun()\n in which\nthe processing is done. This function is called during the execution\nand gets a kind of context as parameter (has name \ndata\n in the\nexamples) which is the actual interface between the script and the\ndatabase.\n\n\nIn the following example, a script is defined which returns the\nmaximum of two values. This is equivalent to the \nCASE\n expression \nCASE WHEN x>=y THEN x WHEN\nx<y THEN y ELSE NULL\n. The ending slash (\n/\n) is only required\nwhen using EXAplus.\n\n\nCREATE\n \nLUA\n \nSCALAR\n \nSCRIPT\n \nmy_maximum\n \n(\na\n \nDOUBLE\n,\n \nb\n \nDOUBLE\n)\n \n           \nRETURNS\n \nDOUBLE\n \nAS\n\n\nfunction\n \nrun\n(\nctx\n)\n\n    \nif\n \nctx\n.\na\n \n==\n \nnull\n \nor\n \nctx\n.\nb\n \n==\n \nnull\n\n        \nthen\n \nreturn\n \nnull\n\n    \nend\n\n    \nif\n \nctx\n.\na\n \n>\n \nctx\n.\nb\n \n        \nthen\n \nreturn\n \nctx\n.\na\n\n        \nelse\n \nreturn\n \nctx\n.\nb\n      \n    \nend\n\n\nend\n \n\n/\n\n\n\nSELECT\n \nx\n,\ny\n,\nmy_maximum\n(\nx\n,\ny\n)\n \nFROM\n \nt\n;\n\n\n\nX\n                 \nY\n                 \nMY_MAXIMUM\n(\nT\n.\nX\n,\nT\n.\nY\n)\n \n\n----------------- ----------------- -------------------\n\n                \n1\n                 \n2\n                   \n2\n\n                \n2\n                 \n2\n                   \n2\n\n                \n3\n                 \n2\n                   \n3\n\n\n\n\n\nAggregate and analytical functions\n\u00b6\n\n\nUDF scripts get essentially more interesting if the script processes\nmultiple data at once. Hereby you can create any kind of aggregate or\nanalytical functions. By defining the keyword \nSET\n you specify that\nmultiple input tuples are processed.  Within the \nrun()\n method, you\ncan iterate through this data (by the method\nnext()\n).\n\n\nFurthermore, scripts can either return a single scalar value (keyword\n\nRETURNS\n) or multiple result tuples (keyword \nEMITS\n).\n\n\nThe following example defines two scripts: the aggregate function\n\nmy_average\n (simulates \nAVG\n) the\nanalytical function \nmy_sum\n which creates three values per input row\n(one sequential number, the current value and the sum of the previous\nvalues). The latter one processes the input data in sorted order due\nto the \nORDER BY\n clause.\n\n\nCREATE\n \nLUA\n \nSET\n \nSCRIPT\n \nmy_average\n \n(\na\n \nDOUBLE\n)\n \n           \nRETURNS\n \nDOUBLE\n \nAS\n\n\nfunction\n \nrun\n(\nctx\n)\n\n    \nif\n \nctx\n.\nsize\n()\n \n==\n \n0\n\n        \nthen\n \nreturn\n \nnull\n\n    \nelse\n\n        \nlocal\n \nsum\n \n=\n \n0\n\n        \nrepeat\n\n            \nif\n \nctx\n.\na\n \n~=\n \nnull\n \nthen\n \n                \nsum\n \n=\n \nsum\n \n+\n \nctx\n.\na\n\n            \nend\n\n        \nuntil\n \nnot\n \nctx\n.\nnext\n()\n\n        \nreturn\n \nsum\n/\nctx\n.\nsize\n()\n\n    \nend\n\n\nend\n \n\n/\n\n\n\nSELECT\n \nmy_average\n(\nx\n)\n \nFROM\n \nt\n;\n\n\n\nMY_AVERAGE\n(\nT\n.\nX\n)\n     \n\n-----------------\n\n             \n7\n.\n75\n\n\n\nCREATE\n \nLUA\n \nSET\n \nSCRIPT\n \nmy_sum\n \n(\na\n \nDOUBLE\n)\n \n           \nEMITS\n \n(\ncount\n \nDOUBLE\n,\n \nval\n \nDOUBLE\n,\n \nsum\n \nDOUBLE\n)\n \nAS\n\n\nfunction\n \nrun\n(\nctx\n)\n\n    \nlocal\n \nsum\n   \n=\n \n0\n\n    \nlocal\n \ncount\n \n=\n \n0\n\n    \nrepeat\n\n        \nif\n \nctx\n.\na\n \n~=\n \nnull\n \nthen\n \n            \nsum\n \n=\n \nsum\n \n+\n \nctx\n.\na\n\n            \ncount\n \n=\n \ncount\n \n+\n \n1\n\n            \nctx\n.\nemit\n(\ncount\n,\nctx\n.\na\n,\nsum\n)\n\n        \nend\n\n    \nuntil\n \nnot\n \nctx\n.\nnext\n()\n\n\nend\n \n\n/\n\n\n\nSELECT\n \nmy_sum\n(\nx\n \nORDER\n \nBY\n \nx\n)\n \nFROM\n \nt\n;\n\n\n\nCOUNT\n             \nVAL\n               \nSUM\n              \n\n----------------- ----------------- -----------------\n\n                \n1\n                 \n4\n                 \n4\n\n                \n2\n                 \n7\n                \n11\n\n                \n3\n                 \n9\n                \n20\n\n                \n4\n                \n11\n                \n31\n\n\n\n\n\nDynamic input and output parameters\n\u00b6\n\n\nInstead of statically defining input and output parameters, you can\nuse the syntax \n(...)\n within \nCREATE SCRIPT\n to create extremely\nflexible scripts with dynamic parameters. The same script can then be\nused for any input data type (e.g. a maximum function independent of\nthe data type) and for a varying number of input columns.  Similarly,\nif you have an EMITS script, the number of output parameters and their\ntype can also be made dynamic.\n\n\nIn order to access and evaluate dynamic input parameters in UDF\n      scripts, extract the number of input parameters and their types from the\n      metadata and then access each parameter value by its index. For\n      instance, in Python the number of input parameters is stored in the\n      variable \nexa.meta.input_column_count\n.\n\n\nIf the UDF script is defined with dynamic output parameters, the\n      actual output parameters and their types are determined dynamically\n      whenever the UDF is called. There are three possibilities:\n\n\n\n\n\n\nYou can specify the output parameters directly in the query\n          after the UDF call using the \nEMITS\n keyword followed by\n          the names and types the UDF shall output in this specific\n          call.\n\n\n\n\n\n\nIf the UDF is used in the top level \nSELECT\n of an\n          \nINSERT INTO SELECT\n statement, the columns of the target\n          table are used as output parameters.\n\n\n\n\n\n\nIf neither \nEMITS\n is specified, nor \nINSERT\n          INTO SELECT\n is used, the database tries to call the function\n          \ndefault_output_columns()\n (the name varies, here for\n          Python) which returns the output parameters dynamically, e.g. based\n          on the input parameters. This method can be implemented by the user.\n\n\n\n\n\n\nIn the example below you can see all three possibilities.\n\n\n-- Define a pretty simple sampling script where the last parameter defines \n\n\n-- the percentage of samples to be emitted. \n\n\nCREATE\n \nPYTHON\n \nSCALAR\n \nSCRIPT\n \nsample_simple\n \n(...)\n \nEMITS\n \n(...)\n \nAS\n\n\nfrom\n \nrandom\n \nimport\n \nrandint\n,\n \nseed\n\n\nseed\n(\n1001\n)\n\n\ndef\n \nrun\n(\nctx\n):\n\n  \npercent\n \n=\n \nctx\n[\nexa\n.\nmeta\n.\ninput_column_count\n-\n1\n]\n\n  \nif\n \nrandint\n(\n0\n,\n100\n)\n \n<=\n \npercent\n:\n\n    \ncurrentRow\n \n=\n \n[\nctx\n[\ni\n]\n \nfor\n \ni\n \nin\n \nrange\n(\n0\n,\n \nexa\n.\nmeta\n.\ninput_column_count\n-\n1\n)]\n\n    \nctx\n.\nemit\n(\n*\ncurrentRow\n)\n\n\n/\n\n\n\n-- This is the same UDF, but output arguments are generated automatically \n\n\n-- to avoid explicit EMITS definition in SELECT.\n\n\n-- In default_output_columns(), a prefix 'c' is added to the column names \n\n\n-- because the input columns are autogenerated numbers\n\n\nCREATE\n \nPYTHON\n \nSCALAR\n \nSCRIPT\n \nsample\n \n(...)\n \nEMITS\n \n(...)\n \nAS\n\n\nfrom\n \nrandom\n \nimport\n \nrandint\n,\n \nseed\n\n\nseed\n(\n1001\n)\n\n\ndef\n \nrun\n(\nctx\n):\n\n  \npercent\n \n=\n \nctx\n[\nexa\n.\nmeta\n.\ninput_column_count\n-\n1\n]\n\n  \nif\n \nrandint\n(\n0\n,\n100\n)\n \n<=\n \npercent\n:\n\n    \ncurrentRow\n \n=\n \n[\nctx\n[\ni\n]\n \nfor\n \ni\n \nin\n \nrange\n(\n0\n,\n \nexa\n.\nmeta\n.\ninput_column_count\n-\n1\n)]\n\n    \nctx\n.\nemit\n(\n*\ncurrentRow\n)\n\n\ndef\n \ndefault_output_columns\n():\n\n  \noutput_args\n \n=\n \nlist\n()\n\n  \nfor\n \ni\n \nin\n \nrange\n(\n0\n,\n \nexa\n.\nmeta\n.\ninput_column_count\n-\n1\n):\n\n    \nname\n \n=\n \nexa\n.\nmeta\n.\ninput_columns\n[\ni\n].\nname\n\n    \ntype\n \n=\n \nexa\n.\nmeta\n.\ninput_columns\n[\ni\n].\nsql_type\n\n    \noutput_args\n.\nappend\n(\n\"c\"\n \n+\n \nname\n \n+\n \n\" \"\n \n+\n \ntype\n)\n\n  \nreturn\n \nstr\n(\n\", \"\n.\njoin\n(\noutput_args\n))\n\n\n/\n\n\n\n-- Example table\n\n\nID\n       \nUSER_NAME\n \nPAGE_VISITS\n\n\n-------- --------- -----------\n\n       \n1\n \nAlice\n              \n12\n\n       \n2\n \nBob\n                 \n4\n\n       \n3\n \nPete\n                \n0\n\n       \n4\n \nHans\n              \n101\n\n       \n5\n \nJohn\n               \n32\n\n       \n6\n \nPeter\n              \n65\n\n       \n7\n \nGraham\n             \n21\n\n       \n8\n \nSteve\n               \n4\n\n       \n9\n \nBill\n               \n64\n\n      \n10\n \nClaudia\n           \n201\n \n\n\n\n-- The first UDF requires to specify the output columns via EMITS.\n\n\n-- Here, 20% of rows should be extracted randomly.\n\n\n\nSELECT\n \nsample_simple\n(\nid\n,\n \nuser_name\n,\n \npage_visits\n,\n \n20\n)\n\n \nEMITS\n \n(\nid\n \nINT\n,\n \nuser_name\n \nVARCHAR\n(\n100\n),\n \nPAGE_VISITS\n \nint\n)\n\n \nFROM\n \npeople\n;\n\n\n\nID\n       \nUSER_NAME\n \nPAGE_VISITS\n\n\n-------- --------- -----------\n\n       \n2\n \nBob\n                 \n4\n\n       \n5\n \nJohn\n               \n32\n\n\n\n-- The second UDF computes the output columns dynamically\n\n\nSELECT\n \nSAMPLE\n(\nid\n,\n \nuser_name\n,\n \npage_visits\n,\n \n20\n)\n\n  \nFROM\n \npeople\n;\n\n\n\nC0\n       \nC1\n        \nC2\n\n\n-------- --------- -----------\n\n       \n2\n \nBob\n                 \n4\n\n       \n5\n \nJohn\n               \n32\n\n\n\n-- In case of INSERT INTO, the UDF uses the target types automatically\n\n\nCREATE\n \nTABLE\n \npeople_sample\n \nLIKE\n \npeople\n;\n\n\nINSERT\n \nINTO\n \npeople_sample\n\n  \nSELECT\n \nsample_simple\n(\nid\n,\n \nuser_name\n,\n \npage_visits\n,\n \n20\n)\n \nFROM\n \npeople\n;\n\n\n\n\n\nMapReduce programs\n\u00b6\n\n\nDue to its flexibility, the UDF scripts framework is able to\n      implement any kind of analyses you can imagine. To show you it's power,\n      we list an example of a MapReduce program which calculates the frequency\n      of single words within a text - a problem which cannot be solved with\n      standard SQL.\n\n\nIn the example, the script \nmap_words\n extracts single\n      words out of a text and emits them. This script is integrated within a\n      SQL query without having the need for an additional aggregation script\n      (the typical Reduce step of MapReduce), because we can use the built-in\n      SQL function \nCOUNT\n. This reduces the\n      implementation efforts since a whole bunch of built-in SQL functions are\n      already available in Exasol. Additionally, the performance can be\n      increased by that since the SQL execution within the built-in functions\n      is more native.\n\n\nCREATE\n \nLUA\n \nSCALAR\n \nSCRIPT\n \nmap_words\n(\nw\n \nvarchar\n(\n10000\n))\n\n\nEMITS\n \n(\nwords\n \nvarchar\n(\n100\n))\n \nAS\n\n\nfunction\n \nrun\n(\nctx\n)\n\n    \nlocal\n \nword\n \n=\n \nctx\n.\nw\n\n    \nif\n \n(\nword\n \n~=\n \nnull\n)\n\n    \nthen\n\n        \nfor\n \ni\n \nin\n \nunicode\n.\nutf8\n.\ngmatch\n(\nword\n,\n'([%w%p]+)'\n)\n\n        \ndo\n\n            \nctx\n.\nemit\n(\ni\n)\n\n        \nend\n\n    \nend\n\n\nend\n\n\n/\n\n\n\nSELECT\n \nwords\n,\n \nCOUNT\n(\n*\n)\n \nFROM\n \n    \n(\nSELECT\n \nmap_words\n(\nl_comment\n)\n \nFROM\n \ntpc\n.\nlineitem\n)\n \n\nGROUP\n \nBY\n \nwords\n \nORDER\n \nBY\n \n2\n \ndesc\n \nLIMIT\n \n10\n;\n\n\n\nWORDS\n                       \nCOUNT\n(\n*\n)\n           \n\n--------------------------- -------------------\n\n\nthe\n                                     \n1376964\n\n\nslyly\n                                    \n649761\n\n\nregular\n                                  \n619211\n\n\nfinal\n                                    \n542206\n\n\ncarefully\n                                \n535430\n\n\nfuriously\n                                \n534054\n\n\nironic\n                                   \n519784\n\n\nblithely\n                                 \n450438\n\n\neven\n                                     \n425013\n\n\nquickly\n                                  \n354712\n\n\n\n\n\nAccess to external services\n\u00b6\n\n\nWithin scripts you can exchange data with external services which\n      increases your flexibility significantly.\n\n\nIn the following example, a list of URLs (stored in a table) is\n      processed, the corresponding documents are read from the webserver and\n      finally the length of the documents is calculated. Please note that\n      every script language provides different libraries to connect to the\n      internet.\n\n\nCREATE\n \nLUA\n \nSCALAR\n \nSCRIPT\n \nlength_of_doc\n \n(\nurl\n \nVARCHAR\n(\n50\n))\n \n           \nEMITS\n \n(\nurl\n \nVARCHAR\n(\n50\n),\n \ndoc_length\n \nDOUBLE\n)\n \nAS\n\n\nhttp\n \n=\n \nrequire\n(\n\"socket.http\"\n)\n\n\nfunction\n \nrun\n(\nctx\n)\n \n    \nfile\n \n=\n \nhttp\n.\nrequest\n(\nctx\n.\nurl\n)\n\n    \nif\n \nfile\n \n==\n \nnil\n \nthen\n \nerror\n(\n'Cannot open URL '\n \n..\n \nctx\n.\nurl\n)\n \nend\n\n    \nctx\n.\nemit\n(\nctx\n.\nurl\n,\n \nunicode\n.\nutf8\n.\nlen\n(\nfile\n))\n\n\nend\n\n\n/\n\n\n\nSELECT\n \nlength_of_doc\n(\nurl\n)\n \nFROM\n \nt\n;\n\n\n\nURL\n                                                \nDOC_LENGTH\n           \n\n-------------------------------------------------- -----------------\n\n\nhttp\n:\n//\nen\n.\nwikipedia\n.\norg\n/\nwiki\n/\nMain_Page\n.\nhtm\n                     \n59960\n\n\nhttp\n:\n//\nen\n.\nwikipedia\n.\norg\n/\nwiki\n/\nExasol\n                            \n30007\n\n\n\n\n\nUser-defined ETL using UDFs\n\u00b6\n\n\nUDF scripts can also be used to implement very flexible ETL\n      processes by defining how to extract and convert data from external data\n      sources.\n\n\nBucketFS\n\u00b6\n\n\nWhen scripts are executed in parallel on the Exasol cluster, there\n    exist some use cases where all instances have to access the same external\n    data. Your algorithms could for example use a statistical model or weather\n    data. For such requirements, it's obviously possible to use an external\n    service (e.g. a file server). But in terms of performance, it would be\n    quite handy to have such data available locally on the cluster\n    nodes.\n\n\nThe Exasol BucketFS file system has been developed for such use\n    cases, where data should be stored synchronously and replicated across the\n    cluster. But we will also explain in the following sections how this\n    concept can be used to extend script languages and even to install\n    completely new script languages on the Exasol cluster.\n\n\nWhat is BucketFS?\n\u00b6\n\n\nThe BucketFS file system is a synchronous file system which is\n      available in the Exasol cluster. This means that each cluster node can\n      connect to this service (e.g. through the http interface) and will see\n      exactly the same content.\n\n\nThe data is replicated locally on every server and automatically\n        synchronized. Hence, you shouldn't store large amounts of data\n        there.\n\n\nThe data in BucketFS is not part of the database backups and has\n        to be backed up manually if necessary.\n\n\nOne BucketFS service contains any number of so-called buckets, and\n      every bucket stores any number of files. Every bucket can have different\n      access privileges as we will explain later on. Folders are not supported\n      directly, but if you specify an upload path including folders, these\n      will be created transparently if they do not exist yet. If all files\n      from a folder are deleted, the folder will be dropped\n      automatically.\n\n\nWriting data is an atomic operation. There don't exist any locks\n      on files, so the latest write operation will finally overwrite the file.\n      In contrast to the database itself, BucketFS is a pure file-based system\n      and has no transactional semantic.\n\n\nSetting up BucketFS and creating buckets\n\u00b6\n\n\nOn the left part of the EXAoperation administration interface,\n      you'll find the link to the BucketFS configuration. You will find a\n      pre-installed default BucketFS service for the configured data disk. If\n      you want to create additional file system services, you need to specify\n      only the data disk and specify ports for http(s).\n\n\nIf you follow the link of an BucketFS Id, you can create and\n      configure any number of buckets within this BucketFS. Beside the bucket\n      name, you have to specify read/write passwords and define whether the\n      bucket should be public readable, i.e. accessible for everyone.\n\n\nA default bucket already exists in the default BucketFS which\n        contains the pre-installed script languages (Java, Python, R).\n        However, for storing larger user data we highly recommend to create a\n        separate BucketFS instance on a separate partition.\n\n\nAccess and access control\n\u00b6\n\n\nFrom outside the cluster, it is possible to access the buckets and\n      the contained files through http(s) clients such as\n      \ncurl. You only have to use one of the\n      database servers' IP address, the specified port and the bucket name,\n      and potentially adjust your internal firewall configuration.\n\n\nFor accessing a bucket through http(s) the users \nr\n\n        and \nw\n are always configured and are associated with the\n        configured read and write passwords.\n\n\nIn the following example the http client \ncurl\n is used\n      to list the existing buckets, upload the files \nfile1\n and\n      \ntar1.tgz\n into the bucket \nbucket\n1 and finally\n      display the list of contained files in this bucket. The relevant\n      parameters for our example are the port of the BucketFS\n      (\n1234\n), the name of the bucket (\nbucket\n1) and\n      the passwords (\nreadpw\n and \nwritepw\n).\n\n\n$\n>\n \ncurl\n \nhttp\n:\n//\n192\n.\n168\n.\n6\n.\n75\n:\n1234\n\n\ndefault\n\n\nbucket1\n\n\n$\n>\n \ncurl\n \n-\nX\n \nPUT\n \n-\nT\n \nfile1\n \nhttp\n:\n//\nw\n:\nwritepw\n@\n192\n.\n168\n.\n6\n.\n75\n:\n1234\n/\nbucket1\n/\nfile1\n\n\n$\n>\n \ncurl\n \n-\nX\n \nPUT\n \n-\nT\n \ntar1\n.\ntgz\n \n\\\n\n\nhttp\n:\n//\nw\n:\nwritepw\n@\n192\n.\n168\n.\n6\n.\n75\n:\n1234\n/\nbucket1\n/\ntar1\n.\ntgz\n\n\n$\n>\n \ncurl\n \nhttp\n:\n//\nr\n:\nreadpw\n@\n192\n.\n168\n.\n6\n.\n75\n:\n1234\n/\nbucket1\n\n\nfile1\n\n\ntar1\n.\ntgz\n\n\n\n\n\nFrom scripts running on the Exasol cluster, you can access the\n      files locally for simplification reasons. You don't need to define any\n      IP address and can be sure that the data is used from the local node.\n      The corresponding path for a bucket can be found in EXAoperation in the\n      overview of a bucket.\n\n\nThe access control is organized by using a database CONNECTION\n      object,\n      because for the database, buckets look similar to normal external data\n      sources. The connection contains the path to the bucket and the read\n      password. After granting that connection to someone using the \nGRANT\n command, the bucket becomes\n      visible/accessible for that user. If you want to allow all users to\n      access a bucket, you can define that bucket in EXAoperation as\n      \npublic\n.\n\n\nSimilar to external clients, write access from scripts is only\n        possible via http(s), but you still would have to be careful with the\n        parallelism of script processes.\n\n\nIn the following example, a connection to a bucket is defined and\n      granted. Afterwards, a script is created which lists the files from a\n      local path. You can see in the example that the equivalent local path\n      for the previously created bucket \nbucket\n1 is\n      \n/buckets/bfsdefault/bucket\n1.\n\n\nCREATE\n \nCONNECTION\n \nmy_bucket_access\n \nTO\n \n'bucketfs:bfsdefault/bucket1'\n \n  \nIDENTIFIED\n \nBY\n \n'readpw'\n;\n\n\n\nGRANT\n \nCONNECTION\n \nmy_bucket_access\n \nTO\n \nmy_user\n;\n\n\n\nCREATE\n \nPYTHON\n \nSCALAR\n \nSCRIPT\n \nls\n(\nmy_path\n \nVARCHAR\n(\n100\n))\n \n\nEMITS\n \n(\nfiles\n \nVARCHAR\n(\n100\n))\n \nAS\n\n\nimport\n \nsubprocess\n\n\n\ndef\n \nrun\n(\nc\n):\n\n    \ntry\n:\n\n        \np\n \n=\n \nsubprocess\n.\nPopen\n(\n'ls '\n+\nc\n.\nmy_path\n,\n \n                             \nstdout\n    \n=\n \nsubprocess\n.\nPIPE\n,\n \n                             \nstderr\n    \n=\n \nsubprocess\n.\nSTDOUT\n,\n \n                             \nclose_fds\n \n=\n \nTrue\n,\n \n                             \nshell\n     \n=\n \nTrue\n)\n\n        \nout\n,\n \nerr\n \n=\n \np\n.\ncommunicate\n()\n\n        \nfor\n \nline\n \nin\n \nout\n.\nstrip\n().\nsplit\n(\n'\\n'\n):\n\n            \nc\n.\nemit\n(\nline\n)\n\n    \nfinally\n:\n\n        \nif\n \np\n \nis\n \nnot\n \nNone\n:\n\n            \ntry\n:\n \np\n.\nkill\n()\n\n            \nexcept\n:\n \npass\n\n\n/\n\n\n\nSELECT\n \nls\n(\n'/buckets/bfsdefault/bucket1'\n);\n\n\n\nFILES\n                                                                                                \n\n---------------------\n\n\nfile1\n\n\ntar1\n\n\n\nSELECT\n \nls\n(\n'/buckets/bfsdefault/bucket1/tar1/'\n);\n\n\n\nFILES\n                                                                                                \n\n---------------------\n\n\na\n\n\nb\n        \n\n\nAs you might have recognized in the example, archives\n      (\n.zip\n, \n.tar\n, \n.tar.gz\n or\n      \n.tgz\n) are always extracted for the script access on the\n      local file system. From outside (e.g. via \ncurl\n) you see the\n      archive while you can locally use the extracted files from within the\n      scripts.\n\nIf you store archives (\n.zip\n, \n.tar\n,\n        \n.tar.gz\n or \n.tgz\n) in the BucketFS, both the\n        original files and the extracted files are stored and need therefore\n        storage space twice.\n\n\nIf you want to work on an archive directly, you can avoid the\n        extraction by renaming the file extension (e.g. \n.zipx\n\n        instead of \n.zip\n).\n\n\nExpanding script languages using BucketFS\n\u00b6\n\n\nIf Exasol's preconfigured set of script languages is sufficient for\n    your needs, you don't need to consider this chapter. But if you want to\n    expand the script languages (e.g. installing additional R packages) or\n    even integrate completely new languages into the script framework, you can\n    easily do that using BucketFS.\n\n\nExpanding the existing script languages\n\u00b6\n\n\nThe first option is to expand the existing languages by adding\n      further packages. The corresponding procedure differs for every language\n      and will therefore be explained in the following sections.\n\n\nThe script language Lua is not expandable, because it is\n        natively compiled into the Exasol database software.\n\n\nJava files (.jar)\n\u00b6\n\n\nFor Java, you can integrate\n.jar\n files in Exasol\n        easily. You only have to save the file in a bucket and reference the\n        corresponding path directly in your Java script.\n\n\nIf you for instance want to use Google's library to process\n        telephone numbers (\nhttp://mavensearch.io/repo/com.googlecode.libphonenumber/libphonenumber/4.2\n),\n        you could upload the file similarly to the examples above in the\n        bucket named \njavalib\n. The corresponding local bucket path\n        would look like the following:\n        \n/buckets/bfsdefault/javalib/libphonenumber-4.2.jar\n.\n\n\nIn the script below you can see how this path is specified to be\n        able to import the library.\n\n\nCREATE\n \nJAVA\n \nSCALAR\n \nSCRIPT\n \njphone\n(\nnum\n \nVARCHAR\n(\n2000\n))\n\n\nRETURNS\n \nVARCHAR\n(\n2000\n)\n \nAS\n\n\n%\njar\n \n/\nbuckets\n/\nbfsdefault\n/\njavalib\n/\nlibphonenumber\n-\n4\n.\n2\n.\njar\n;\n\n\n\nimport\n \ncom\n.\ngoogle\n.\ni18n\n.\nphonenumbers\n.\nPhoneNumberUtil\n;\n\n\nimport\n \ncom\n.\ngoogle\n.\ni18n\n.\nphonenumbers\n.\nNumberParseException\n;\n\n\nimport\n \ncom\n.\ngoogle\n.\ni18n\n.\nphonenumbers\n.\nPhonenumber\n.\nPhoneNumber\n;\n\n\n\nclass\n \nJPHONE\n \n{\n\n  \nstatic\n \nString\n \nrun\n(\nExaMetadata\n \nexa\n,\n \nExaIterator\n \nctx\n)\n \nthrows\n \nException\n \n{\n\n    \nPhoneNumberUtil\n \nphoneUtil\n \n=\n \nPhoneNumberUtil\n.\ngetInstance\n();\n\n    \ntry\n \n{\n\n      \nPhoneNumber\n \nswissNumberProto\n \n=\n \nphoneUtil\n.\nparse\n(\nctx\n.\ngetString\n(\n\"num\"\n),\n\n                                                     \n\"DE\"\n);\n\n      \nreturn\n \nswissNumberProto\n.\ntoString\n();\n\n    \n}\n \ncatch\n \n(\nNumberParseException\n \ne\n)\n \n{\n\n      \nSystem\n.\nerr\n.\nprintln\n(\n\"NumberParseException thrown: \"\n \n+\n \ne\n.\ntoString\n());\n\n    \n}\n\n    \nreturn\n \n\"failed\"\n;\n\n  \n}\n\n\n}\n\n\n/\n \n\n\n\n\nPython libraries\n\u00b6\n\n\nPython libraries are often provided in the form of\n        \n.whl\n files. You can easily integrate such libraries into\n        Exasol by uploading the file to a bucket and extend the Python search\n        path appropriately.\n\n\nIf you for instance want to use Google's library for processing\n        phone numbers (\nhttps://pypi.python.org/pypi/phonenumbers\n), you could\n        upload the file into the bucket named \npylib\n. The\n        corresponding local path would look like the following:\n        \n/buckets/bfsdefault/pylib/phonenumbers-7.7.5-py2.py3-none-any.whl\n.\n\n\nIn the script below you can see how this path is specified to be\n        able to import the library.\n\n\nCREATE\n \nPYTHON\n \nSCALAR\n \nSCRIPT\n \nphonenumber\n(\nphone_number\n \nVARCHAR\n(\n2000\n))\n \n\nRETURNS\n \nVARCHAR\n(\n2000\n)\n \nAS\n\n\nimport\n \nsys\n\n\nimport\n \nglob\n\n\n\nsys\n.\npath\n.\nextend\n(\nglob\n.\nglob\n(\n'/buckets/bfsdefault/pylib/*'\n))\n\n\nimport\n \nphonenumbers\n\n\n\ndef\n \nrun\n(\nc\n):\n\n   \nreturn\n \nstr\n(\nphonenumbers\n.\nparse\n(\nc\n.\nphone_number\n,\nNone\n))\n\n\n/\n\n\n\nSELECT\n \nphonenumber\n(\n'+1-555-2368'\n)\n \nAS\n \nghost_busters\n;\n\n\n\nGHOST_BUSTERS\n\n\n-----------------------------------------\n\n\nCountry\n \nCode\n:\n \n1\n \nNational\n \nNumber\n:\n \n55552368\n\n\n\n\n\nR packages\n\u00b6\n\n\nThe installation of R packages is a bit more complex because\n        they have to be compiled using the c compiler. To manage that, you can\n        download the existing Exasol Linux container, compile the package in\n        that container and upload the resulting package into BucketFS. Details\n        about the Linux container will be explained in the following\n        chapter.\n\n\nA simple method is using Docker as described in the following\n        example.\n\n\n$> bucketfs = http://192.168.6.75:1234\n$> cont = /default/EXAClusterOS/ScriptLanguages-2016-10-21.tar.gz\n$> docker $bucketfs$cont import my_dock\n$> mkdir r_pkg\n$> docker run -v `pwd`/r_pkg:/r_pkg --name=my_dock -it my_dock /bin/bash\n\n\n\n\nAgain we want to use an existing package for processing phone\n        numbers, this time from Steve Myles (\nhttps://cran.r-project.org/web/packages/phonenumber/index.html\n).\n\n\nWithin the Docker container, you can start R and install\n        it:\n\n\n# export R_LIBS=\"/r_pkg/\"\n# R\n> install.packages('phonenumber', repos=\"http://cran.r-project.org\")\nInstalling package into '/r_pkg'\n(as 'lib' is unspecified)\ntrying URL 'http://cran.r-project.org/src/contrib/phonenumber_0.2.2.tar.gz'\nContent type 'application/x-gzip' length 10516 bytes (10 KB)\n==================================================\ndownloaded 10 KB\n\n\n\n\nThe first line installs the package from the Linux container\n        into the subfolder \nr_pkg\n which can be accessed outside\n        the Docker container.\n\n\nAfterwards, the resulting tgz archive is uploaded into the\n        bucket named \nrlib\n:\n\n\n$\n>\n \nbucketfs\n \n=\n \nhttp\n:\n//\nw\n:\nwritepw\n@\n192\n.\n168\n.\n6\n.\n75\n:\n1234\n\n\n$\n>\n \ncurl\n \n-\nvX\n \nPUT\n \n-\nT\n \nr_pkg\n.\ntgz\n \n$\nbucketfs\n/\nrlib\n/\nr_pkg\n.\ntgz\n\n\n\nIn the following script you can see how the resulting local path\n        (\n/buckets/bfsdefault/rlib/r_pkg\n) is specified to be able\n        to use the library.\n\n\nCREATE\n \nR\n \nSCALAR\n \nSCRIPT\n \ntophone\n(\nletters\n \nVARCHAR\n(\n2000\n))\n \nRETURNS\n \nINT\n \nAS\n\n\n.\nlibPaths\n(\n \nc\n(\n \n.\nlibPaths\n(),\n \n\"/buckets/bfsdefault/rlib/r_pkg\"\n)\n \n)\n\n\nlibrary\n(\nphonenumber\n)\n\n\n\nrun\n \n<-\n \nfunction\n(\nctx\n)\n \n{\n\n   \nletterToNumber\n(\nctx$letters\n,\n \nqz\n \n=\n \n1\n)\n\n\n}\n\n\n/\n\n\n\n\n\nInstalling new script languages\n\u00b6\n\n\nDue to Exasol's open script framework it is simply possible to\n      integrate completely new script languages into Exasol, following these 3\n      steps:\n\n\n\n\nCreating a functioning language client\n\n\nUpload the resulting client into a bucket\n\n\nDefine a script language alias\n\n\n\n\nThe language has to be expanded by some small\n      APIs which implement the communication between Exasol and the language.\n      Afterwards the client is compiled for the usage in the pre-installed\n      Exasol Linux Container, so that it can be started within a secure\n      process on the Exasol cluster.\n\n\nThe last step creates a link between the language client and\n      Exasol's SQL language, actually the \nCREATE SCRIPT\n command. This facilitates many\n      options to try out new versions of a language or completely new\n      languages and finally replace them completely.\n\n\nIf you for instance plan to migrate from Python 2 to Python 3, you\n      could upload a new client and link the alias \nPYTHON\n\n      temporarily to the new version via \nALTER SESSION\n. After a thorough testing phase,\n      you can finally switch to the new version for all users via the \nALTER SYSTEM\n statement.\n\n\nOn the other side, it is also possible to use both language\n      versions in parallel by defining two aliases separately (e.g.\n      \nPYTHON2\n and \nPYTHON3\n).\n\n\nCreating a script client\n\u00b6\n\n\nThe main task of installing new languages is developing the\n        script client. If you are interested in a certain language, you should\n        first check whether the corresponding client has already been\n        published in our open source repository (see \nhttps://github.com/exasol/script-languages\n). Otherwise\n        we would be very happy if you would contribute self-developed clients\n        to our open source community.\n\n\nA script client is based on a Linux container which is stored in\n        BucketFS. The pre-installed script client for languages Python, Java\n        and R is located in the default bucket of the default BucketFS. Using\n        the Linux container technology, an encapsulated virtual machine is\n        started in a safe environment whenever an SQL query contains script\n        code. The Linux container includes a complete Linux distribution and\n        starts the corresponding script client for executing the script code.\n        In general, you could also upload your own Linux container and combine\n        it with your script client.\n\n\nThe script client has to implement a certain protocol that\n        controls the communication between the scripts and the database. For\n        that purpose, the established technologies ZeroMQ\n    and Google's Protocol Buffers are used. Because\n        of the length, the actual protocol definition is not included in this\n        user manual. For details, please have a look into our open source\n        repository where\n        you'll find the following:\n\n\nScript aliases for the integration into SQL\n\u00b6\n\n\nAfter building and uploading a script client, you have to\n        configure an alias within Exasol. The database then knows where each\n        script language has been installed.\nYou can change the script aliases via the commands \nALTER SESSION\n and \nALTER SYSTEM\n, either session-wide or\n        system-wide. This is handy especially for using several language\n        versions in parallel, or migrating from one version to another.\nThe current session and system parameters can be found in \nEXA_PARAMETERS\n. The scripts aliases are defined via the\n        parameter \nSCRIPT_LANGUAGES\n:\n\n\nSELECT\n \nsession_value\n \nFROM\n \nexa_parameters\n \n\nWHERE\n \nparameter_name\n=\n'SCRIPT_LANGUAGES'\n;\n\n\n\nPARAMETER_NAME\n\n\n---------------------------------------------------\n\n\nPYTHON\n=\nbuiltin_python\n \nR\n=\nbuiltin_r\n \nJAVA\n=\nbuiltin_java\n\n\n\nThese values are not very meaningful since they are just\n        internal macros to make that parameter compact and to dynamically use\n        the last installed version. Written out, the alias for Java would look\n        like the following:\n\nJAVA=localzmq+protobuf:///bfsdefault/default/EXAClusterOS/ScriptLanguages-2016-10-21/?lang=java#buckets/bfsdefault/default/EXASolution-6.0.0/exaudfclient\n\n\nThat alias means that for all \nCREATE JAVA SCRIPT\n\n        statements, the Exasol database will use the script client\n        \nexaudfclient\n from local path\n        \nbuckets/bfsdefault/default/EXASolution-2016-10-21/\n,\n        started within the Exasol Linux container from path\n        \nbfsdefault/default/EXAClusterOS/ScriptLanguages-2016-10-21\n.\n        The used communication protocol is \nlocalzmq+protobuf\n\n        (this is the only supported protocol so far).\n\n\nFor the three pre-installed languages (Python, R, Java), Exasol\n        uses one single script client which evaluates the parameter\n        \nlang=java\n to differentiate between these languages.\n        That's why the internal macro for the Python alias looks nearly\n        identical. Script clients implemented by users can of course define\n        and evaluate such optional parameters individually.\n\n\nIn general, a script alias contains the following\n        elements:\n\n<alias>=localzmq+protobuf:///<path_to_linux-container>/[?<client_param_list>]#<path_to_client>\n\n\n\nPlease note that a script alias may not contain\n            spaces.\n\n\nMaybe you have noticed in the Java alias that the path of the\n        Linux container begins with the BucketFS while the client path\n        contains the prefix \nbuckets/\n. The reason for that is that\n        the client is started in a secure environment within the Linux\n        container and may only access the existing (and visible) buckets.\n        Access is granted just to the embedded buckets (via\n        \nmount\n), but not to the real server's underlying file\n        system. The path \n/buckets/\n can be used read-only from\n        within scripts as described in the previous chapter.\n\n\nYou can define or rename any number of aliases. As mentioned\n        before, we recommend to test such adjustments at first in your own\n        session before making that change globally visible via \nALTER SYSTEM\n.",
            "title": "Exasol UDF framework"
        },
        {
            "location": "/methods/exasol-udf/#exasol-udf-framework",
            "text": "UDF scripts provides you the ability to program your own analyses,\nprocessing or generation functions and execute them in parallel inside\nExasol's high performance cluster (In Database Analytics). Through\nthis principle many problems can be solved very efficiently which were\nnot possible with SQL statements. Via UDF scripts you therefore get a\nhighly flexible interface for implementing nearly every\nrequirement. Hence you become a HPC developer without the need for\ncertain previous knowledge.  With UDF scripts you can implement the following extensions:   Scalar functions  Aggregate functions  Analytical functions  MapReduce algorithms  User-defined ETL processes   To take advantage of the variety of UDF scripts, you only need to\ncreate a script and use this script afterwards within a SELECT\nstatement. By this close embedding within SQL you can achieve ideal\nperformance and scalability.  Exasol supports multiple programming languages (Java, Lua, Python, R)\nto simplify your start. Furthermore the different languages provide\nyou different advantages due to their respective focus\n(e.g. statistical functions in R) and the different delivered\nlibraries (XML parser, etc.). Thus, please note the next chapters in\nwhich the specific characteristics of each language is described.  The actual versions of the scripting languages can be listed with\ncorresponding metadata functions.  Within the  CREATE SCRIPT STATEMENT  command, you have to define the\ntype of input and output values. You can e.g. create a script which\ngenerates multiple result rows out of a single input row\n( SCALAR ... EMITS ).    Input values:    SCALAR  The keyword SCALAR specifies that the script processes single\n input rows. It's code is therefore called once per input row.    SET  If you define the option SET, then the processing refers to a\n set of input values. Within the code, you can iterate through\n those values.      Output values:    RETURNS  In this case the script returns a single value.    EMITS  If the keyword EMITS was defined, the script can create\n (emit) multiple result rows (tuples). In case of input type\n SET, the EMITS result can only be used alone, thus not be\n combined with other expressions. However you can of course\n nest it through a subselect to do further processing of those\n intermediate results.      The data types of input and output parameters can be defined to\nspecify the conversion between internal data types and the database\nSQL data types. If you don't specify them, the script has to handle\nthat dynamically (see details and examples below).  Please note that input parameters of scripts are always treated\ncase-sensitive, similar to the script code itself. This is different\nto SQL identifiers which are only treated case-sensitive when being\ndelimited.  Scripts must contain the main function  run() . This function is\ncalled with a parameter providing access to the input data of\nExasol. If your script processes multiple input tuples (thus a SET\nscript), you can iterate through the single tuples by the use of this\nparameter.  Please note the information in the following table:    Internal processing  During the processing of an SELECT statement, multiple virtual\nmachines are started for each script and node, which process the\ndata independently.  For scalar functions, the input rows are distributed across those\nvirtual machines to achieve a maximal parallelism.  In case of SET input tuples, the virtual machines are used per\ngroup (if you specify a GROUP BY clause). If no GROUP BY clause is\ndefined, then only one group exists and therefore only one node\nand virtual machine can process the data.    ORDER BY  Either when creating a script or when calling it you can specify\nan ORDER BY clause which leads to a sorted processing of the\ngroups of SET input data. For some algorithms, this can be\nreasonable. But if it is necessary for the algorithm, then you\nshould already specify this clause during the creation to avoid\nwrong results due to misuse.    Performance  The performance of the different languages can hardly be compared,\nsince the specific elements of the languages can have different\ncapacities. Thus a string processing can be faster in one\nlanguage, while the XML parsing is faster in the other one.  However, we generally recommend to use Lua if performance is the\nmost important criteria. Due to technical reasons, Lua is\nintegrated in Exasol in the most native way and therefore has the\nsmallest process overhead.",
            "title": "Exasol UDF framework"
        },
        {
            "location": "/methods/exasol-udf/#introducing-examples",
            "text": "In this chapter we provide some introducing Lua examples to give you a\ngeneral idea about the functionality of user defined scripts. Examples\nfor the other programming languages can be found in the later chapters\nexplaining the details of each language.",
            "title": "Introducing examples"
        },
        {
            "location": "/methods/exasol-udf/#scalar-functions",
            "text": "User defined scalar functions (keyword  SCALAR ) are the simplest case\nof user defined scripts, returning one scalar result value (keyword RETURNS ) or several result tuples (keyword  SET ) for each input\nvalue (or tuple).  Please note that scripts have to implement a function  run()  in which\nthe processing is done. This function is called during the execution\nand gets a kind of context as parameter (has name  data  in the\nexamples) which is the actual interface between the script and the\ndatabase.  In the following example, a script is defined which returns the\nmaximum of two values. This is equivalent to the  CASE  expression  CASE WHEN x>=y THEN x WHEN\nx<y THEN y ELSE NULL . The ending slash ( / ) is only required\nwhen using EXAplus.  CREATE   LUA   SCALAR   SCRIPT   my_maximum   ( a   DOUBLE ,   b   DOUBLE )  \n            RETURNS   DOUBLE   AS  function   run ( ctx ) \n     if   ctx . a   ==   null   or   ctx . b   ==   null \n         then   return   null \n     end \n     if   ctx . a   >   ctx . b  \n         then   return   ctx . a \n         else   return   ctx . b       \n     end  end   /  SELECT   x , y , my_maximum ( x , y )   FROM   t ;  X                   Y                   MY_MAXIMUM ( T . X , T . Y )   ----------------- ----------------- ------------------- \n                 1                   2                     2 \n                 2                   2                     2 \n                 3                   2                     3",
            "title": "Scalar functions"
        },
        {
            "location": "/methods/exasol-udf/#aggregate-and-analytical-functions",
            "text": "UDF scripts get essentially more interesting if the script processes\nmultiple data at once. Hereby you can create any kind of aggregate or\nanalytical functions. By defining the keyword  SET  you specify that\nmultiple input tuples are processed.  Within the  run()  method, you\ncan iterate through this data (by the method next() ).  Furthermore, scripts can either return a single scalar value (keyword RETURNS ) or multiple result tuples (keyword  EMITS ).  The following example defines two scripts: the aggregate function my_average  (simulates  AVG ) the\nanalytical function  my_sum  which creates three values per input row\n(one sequential number, the current value and the sum of the previous\nvalues). The latter one processes the input data in sorted order due\nto the  ORDER BY  clause.  CREATE   LUA   SET   SCRIPT   my_average   ( a   DOUBLE )  \n            RETURNS   DOUBLE   AS  function   run ( ctx ) \n     if   ctx . size ()   ==   0 \n         then   return   null \n     else \n         local   sum   =   0 \n         repeat \n             if   ctx . a   ~=   null   then  \n                 sum   =   sum   +   ctx . a \n             end \n         until   not   ctx . next () \n         return   sum / ctx . size () \n     end  end   /  SELECT   my_average ( x )   FROM   t ;  MY_AVERAGE ( T . X )       ----------------- \n              7 . 75  CREATE   LUA   SET   SCRIPT   my_sum   ( a   DOUBLE )  \n            EMITS   ( count   DOUBLE ,   val   DOUBLE ,   sum   DOUBLE )   AS  function   run ( ctx ) \n     local   sum     =   0 \n     local   count   =   0 \n     repeat \n         if   ctx . a   ~=   null   then  \n             sum   =   sum   +   ctx . a \n             count   =   count   +   1 \n             ctx . emit ( count , ctx . a , sum ) \n         end \n     until   not   ctx . next ()  end   /  SELECT   my_sum ( x   ORDER   BY   x )   FROM   t ;  COUNT               VAL                 SUM                ----------------- ----------------- ----------------- \n                 1                   4                   4 \n                 2                   7                  11 \n                 3                   9                  20 \n                 4                  11                  31",
            "title": "Aggregate and analytical functions"
        },
        {
            "location": "/methods/exasol-udf/#dynamic-input-and-output-parameters",
            "text": "Instead of statically defining input and output parameters, you can\nuse the syntax  (...)  within  CREATE SCRIPT  to create extremely\nflexible scripts with dynamic parameters. The same script can then be\nused for any input data type (e.g. a maximum function independent of\nthe data type) and for a varying number of input columns.  Similarly,\nif you have an EMITS script, the number of output parameters and their\ntype can also be made dynamic.  In order to access and evaluate dynamic input parameters in UDF\n      scripts, extract the number of input parameters and their types from the\n      metadata and then access each parameter value by its index. For\n      instance, in Python the number of input parameters is stored in the\n      variable  exa.meta.input_column_count .  If the UDF script is defined with dynamic output parameters, the\n      actual output parameters and their types are determined dynamically\n      whenever the UDF is called. There are three possibilities:    You can specify the output parameters directly in the query\n          after the UDF call using the  EMITS  keyword followed by\n          the names and types the UDF shall output in this specific\n          call.    If the UDF is used in the top level  SELECT  of an\n           INSERT INTO SELECT  statement, the columns of the target\n          table are used as output parameters.    If neither  EMITS  is specified, nor  INSERT\n          INTO SELECT  is used, the database tries to call the function\n           default_output_columns()  (the name varies, here for\n          Python) which returns the output parameters dynamically, e.g. based\n          on the input parameters. This method can be implemented by the user.    In the example below you can see all three possibilities.  -- Define a pretty simple sampling script where the last parameter defines   -- the percentage of samples to be emitted.   CREATE   PYTHON   SCALAR   SCRIPT   sample_simple   (...)   EMITS   (...)   AS  from   random   import   randint ,   seed  seed ( 1001 )  def   run ( ctx ): \n   percent   =   ctx [ exa . meta . input_column_count - 1 ] \n   if   randint ( 0 , 100 )   <=   percent : \n     currentRow   =   [ ctx [ i ]   for   i   in   range ( 0 ,   exa . meta . input_column_count - 1 )] \n     ctx . emit ( * currentRow )  /  -- This is the same UDF, but output arguments are generated automatically   -- to avoid explicit EMITS definition in SELECT.  -- In default_output_columns(), a prefix 'c' is added to the column names   -- because the input columns are autogenerated numbers  CREATE   PYTHON   SCALAR   SCRIPT   sample   (...)   EMITS   (...)   AS  from   random   import   randint ,   seed  seed ( 1001 )  def   run ( ctx ): \n   percent   =   ctx [ exa . meta . input_column_count - 1 ] \n   if   randint ( 0 , 100 )   <=   percent : \n     currentRow   =   [ ctx [ i ]   for   i   in   range ( 0 ,   exa . meta . input_column_count - 1 )] \n     ctx . emit ( * currentRow )  def   default_output_columns (): \n   output_args   =   list () \n   for   i   in   range ( 0 ,   exa . meta . input_column_count - 1 ): \n     name   =   exa . meta . input_columns [ i ]. name \n     type   =   exa . meta . input_columns [ i ]. sql_type \n     output_args . append ( \"c\"   +   name   +   \" \"   +   type ) \n   return   str ( \", \" . join ( output_args ))  /  -- Example table  ID         USER_NAME   PAGE_VISITS  -------- --------- ----------- \n        1   Alice                12 \n        2   Bob                   4 \n        3   Pete                  0 \n        4   Hans                101 \n        5   John                 32 \n        6   Peter                65 \n        7   Graham               21 \n        8   Steve                 4 \n        9   Bill                 64 \n       10   Claudia             201   -- The first UDF requires to specify the output columns via EMITS.  -- Here, 20% of rows should be extracted randomly.  SELECT   sample_simple ( id ,   user_name ,   page_visits ,   20 ) \n  EMITS   ( id   INT ,   user_name   VARCHAR ( 100 ),   PAGE_VISITS   int ) \n  FROM   people ;  ID         USER_NAME   PAGE_VISITS  -------- --------- ----------- \n        2   Bob                   4 \n        5   John                 32  -- The second UDF computes the output columns dynamically  SELECT   SAMPLE ( id ,   user_name ,   page_visits ,   20 ) \n   FROM   people ;  C0         C1          C2  -------- --------- ----------- \n        2   Bob                   4 \n        5   John                 32  -- In case of INSERT INTO, the UDF uses the target types automatically  CREATE   TABLE   people_sample   LIKE   people ;  INSERT   INTO   people_sample \n   SELECT   sample_simple ( id ,   user_name ,   page_visits ,   20 )   FROM   people ;",
            "title": "Dynamic input and output parameters"
        },
        {
            "location": "/methods/exasol-udf/#mapreduce-programs",
            "text": "Due to its flexibility, the UDF scripts framework is able to\n      implement any kind of analyses you can imagine. To show you it's power,\n      we list an example of a MapReduce program which calculates the frequency\n      of single words within a text - a problem which cannot be solved with\n      standard SQL.  In the example, the script  map_words  extracts single\n      words out of a text and emits them. This script is integrated within a\n      SQL query without having the need for an additional aggregation script\n      (the typical Reduce step of MapReduce), because we can use the built-in\n      SQL function  COUNT . This reduces the\n      implementation efforts since a whole bunch of built-in SQL functions are\n      already available in Exasol. Additionally, the performance can be\n      increased by that since the SQL execution within the built-in functions\n      is more native.  CREATE   LUA   SCALAR   SCRIPT   map_words ( w   varchar ( 10000 ))  EMITS   ( words   varchar ( 100 ))   AS  function   run ( ctx ) \n     local   word   =   ctx . w \n     if   ( word   ~=   null ) \n     then \n         for   i   in   unicode . utf8 . gmatch ( word , '([%w%p]+)' ) \n         do \n             ctx . emit ( i ) \n         end \n     end  end  /  SELECT   words ,   COUNT ( * )   FROM  \n     ( SELECT   map_words ( l_comment )   FROM   tpc . lineitem )   GROUP   BY   words   ORDER   BY   2   desc   LIMIT   10 ;  WORDS                         COUNT ( * )             --------------------------- -------------------  the                                       1376964  slyly                                      649761  regular                                    619211  final                                      542206  carefully                                  535430  furiously                                  534054  ironic                                     519784  blithely                                   450438  even                                       425013  quickly                                    354712",
            "title": "MapReduce programs"
        },
        {
            "location": "/methods/exasol-udf/#access-to-external-services",
            "text": "Within scripts you can exchange data with external services which\n      increases your flexibility significantly.  In the following example, a list of URLs (stored in a table) is\n      processed, the corresponding documents are read from the webserver and\n      finally the length of the documents is calculated. Please note that\n      every script language provides different libraries to connect to the\n      internet.  CREATE   LUA   SCALAR   SCRIPT   length_of_doc   ( url   VARCHAR ( 50 ))  \n            EMITS   ( url   VARCHAR ( 50 ),   doc_length   DOUBLE )   AS  http   =   require ( \"socket.http\" )  function   run ( ctx )  \n     file   =   http . request ( ctx . url ) \n     if   file   ==   nil   then   error ( 'Cannot open URL '   ..   ctx . url )   end \n     ctx . emit ( ctx . url ,   unicode . utf8 . len ( file ))  end  /  SELECT   length_of_doc ( url )   FROM   t ;  URL                                                  DOC_LENGTH             -------------------------------------------------- -----------------  http : // en . wikipedia . org / wiki / Main_Page . htm                       59960  http : // en . wikipedia . org / wiki / Exasol                              30007",
            "title": "Access to external services"
        },
        {
            "location": "/methods/exasol-udf/#user-defined-etl-using-udfs",
            "text": "UDF scripts can also be used to implement very flexible ETL\n      processes by defining how to extract and convert data from external data\n      sources.",
            "title": "User-defined ETL using UDFs"
        },
        {
            "location": "/methods/exasol-udf/#bucketfs",
            "text": "When scripts are executed in parallel on the Exasol cluster, there\n    exist some use cases where all instances have to access the same external\n    data. Your algorithms could for example use a statistical model or weather\n    data. For such requirements, it's obviously possible to use an external\n    service (e.g. a file server). But in terms of performance, it would be\n    quite handy to have such data available locally on the cluster\n    nodes.  The Exasol BucketFS file system has been developed for such use\n    cases, where data should be stored synchronously and replicated across the\n    cluster. But we will also explain in the following sections how this\n    concept can be used to extend script languages and even to install\n    completely new script languages on the Exasol cluster.",
            "title": "BucketFS"
        },
        {
            "location": "/methods/exasol-udf/#what-is-bucketfs",
            "text": "The BucketFS file system is a synchronous file system which is\n      available in the Exasol cluster. This means that each cluster node can\n      connect to this service (e.g. through the http interface) and will see\n      exactly the same content.  The data is replicated locally on every server and automatically\n        synchronized. Hence, you shouldn't store large amounts of data\n        there.  The data in BucketFS is not part of the database backups and has\n        to be backed up manually if necessary.  One BucketFS service contains any number of so-called buckets, and\n      every bucket stores any number of files. Every bucket can have different\n      access privileges as we will explain later on. Folders are not supported\n      directly, but if you specify an upload path including folders, these\n      will be created transparently if they do not exist yet. If all files\n      from a folder are deleted, the folder will be dropped\n      automatically.  Writing data is an atomic operation. There don't exist any locks\n      on files, so the latest write operation will finally overwrite the file.\n      In contrast to the database itself, BucketFS is a pure file-based system\n      and has no transactional semantic.",
            "title": "What is BucketFS?"
        },
        {
            "location": "/methods/exasol-udf/#setting-up-bucketfs-and-creating-buckets",
            "text": "On the left part of the EXAoperation administration interface,\n      you'll find the link to the BucketFS configuration. You will find a\n      pre-installed default BucketFS service for the configured data disk. If\n      you want to create additional file system services, you need to specify\n      only the data disk and specify ports for http(s).  If you follow the link of an BucketFS Id, you can create and\n      configure any number of buckets within this BucketFS. Beside the bucket\n      name, you have to specify read/write passwords and define whether the\n      bucket should be public readable, i.e. accessible for everyone.  A default bucket already exists in the default BucketFS which\n        contains the pre-installed script languages (Java, Python, R).\n        However, for storing larger user data we highly recommend to create a\n        separate BucketFS instance on a separate partition.",
            "title": "Setting up BucketFS and creating buckets"
        },
        {
            "location": "/methods/exasol-udf/#access-and-access-control",
            "text": "From outside the cluster, it is possible to access the buckets and\n      the contained files through http(s) clients such as\n       curl. You only have to use one of the\n      database servers' IP address, the specified port and the bucket name,\n      and potentially adjust your internal firewall configuration.  For accessing a bucket through http(s) the users  r \n        and  w  are always configured and are associated with the\n        configured read and write passwords.  In the following example the http client  curl  is used\n      to list the existing buckets, upload the files  file1  and\n       tar1.tgz  into the bucket  bucket 1 and finally\n      display the list of contained files in this bucket. The relevant\n      parameters for our example are the port of the BucketFS\n      ( 1234 ), the name of the bucket ( bucket 1) and\n      the passwords ( readpw  and  writepw ).  $ >   curl   http : // 192 . 168 . 6 . 75 : 1234  default  bucket1  $ >   curl   - X   PUT   - T   file1   http : // w : writepw @ 192 . 168 . 6 . 75 : 1234 / bucket1 / file1  $ >   curl   - X   PUT   - T   tar1 . tgz   \\  http : // w : writepw @ 192 . 168 . 6 . 75 : 1234 / bucket1 / tar1 . tgz  $ >   curl   http : // r : readpw @ 192 . 168 . 6 . 75 : 1234 / bucket1  file1  tar1 . tgz   From scripts running on the Exasol cluster, you can access the\n      files locally for simplification reasons. You don't need to define any\n      IP address and can be sure that the data is used from the local node.\n      The corresponding path for a bucket can be found in EXAoperation in the\n      overview of a bucket.  The access control is organized by using a database CONNECTION\n      object,\n      because for the database, buckets look similar to normal external data\n      sources. The connection contains the path to the bucket and the read\n      password. After granting that connection to someone using the  GRANT  command, the bucket becomes\n      visible/accessible for that user. If you want to allow all users to\n      access a bucket, you can define that bucket in EXAoperation as\n       public .  Similar to external clients, write access from scripts is only\n        possible via http(s), but you still would have to be careful with the\n        parallelism of script processes.  In the following example, a connection to a bucket is defined and\n      granted. Afterwards, a script is created which lists the files from a\n      local path. You can see in the example that the equivalent local path\n      for the previously created bucket  bucket 1 is\n       /buckets/bfsdefault/bucket 1.  CREATE   CONNECTION   my_bucket_access   TO   'bucketfs:bfsdefault/bucket1'  \n   IDENTIFIED   BY   'readpw' ;  GRANT   CONNECTION   my_bucket_access   TO   my_user ;  CREATE   PYTHON   SCALAR   SCRIPT   ls ( my_path   VARCHAR ( 100 ))   EMITS   ( files   VARCHAR ( 100 ))   AS  import   subprocess  def   run ( c ): \n     try : \n         p   =   subprocess . Popen ( 'ls ' + c . my_path ,  \n                              stdout      =   subprocess . PIPE ,  \n                              stderr      =   subprocess . STDOUT ,  \n                              close_fds   =   True ,  \n                              shell       =   True ) \n         out ,   err   =   p . communicate () \n         for   line   in   out . strip (). split ( '\\n' ): \n             c . emit ( line ) \n     finally : \n         if   p   is   not   None : \n             try :   p . kill () \n             except :   pass  /  SELECT   ls ( '/buckets/bfsdefault/bucket1' );  FILES                                                                                                  ---------------------  file1  tar1  SELECT   ls ( '/buckets/bfsdefault/bucket1/tar1/' );  FILES                                                                                                  ---------------------  a  b          \nAs you might have recognized in the example, archives\n      ( .zip ,  .tar ,  .tar.gz  or\n       .tgz ) are always extracted for the script access on the\n      local file system. From outside (e.g. via  curl ) you see the\n      archive while you can locally use the extracted files from within the\n      scripts. If you store archives ( .zip ,  .tar ,\n         .tar.gz  or  .tgz ) in the BucketFS, both the\n        original files and the extracted files are stored and need therefore\n        storage space twice.  If you want to work on an archive directly, you can avoid the\n        extraction by renaming the file extension (e.g.  .zipx \n        instead of  .zip ).",
            "title": "Access and access control"
        },
        {
            "location": "/methods/exasol-udf/#expanding-script-languages-using-bucketfs",
            "text": "If Exasol's preconfigured set of script languages is sufficient for\n    your needs, you don't need to consider this chapter. But if you want to\n    expand the script languages (e.g. installing additional R packages) or\n    even integrate completely new languages into the script framework, you can\n    easily do that using BucketFS.",
            "title": "Expanding script languages using BucketFS"
        },
        {
            "location": "/methods/exasol-udf/#expanding-the-existing-script-languages",
            "text": "The first option is to expand the existing languages by adding\n      further packages. The corresponding procedure differs for every language\n      and will therefore be explained in the following sections.  The script language Lua is not expandable, because it is\n        natively compiled into the Exasol database software.",
            "title": "Expanding the existing script languages"
        },
        {
            "location": "/methods/exasol-udf/#java-files-jar",
            "text": "For Java, you can integrate .jar  files in Exasol\n        easily. You only have to save the file in a bucket and reference the\n        corresponding path directly in your Java script.  If you for instance want to use Google's library to process\n        telephone numbers ( http://mavensearch.io/repo/com.googlecode.libphonenumber/libphonenumber/4.2 ),\n        you could upload the file similarly to the examples above in the\n        bucket named  javalib . The corresponding local bucket path\n        would look like the following:\n         /buckets/bfsdefault/javalib/libphonenumber-4.2.jar .  In the script below you can see how this path is specified to be\n        able to import the library.  CREATE   JAVA   SCALAR   SCRIPT   jphone ( num   VARCHAR ( 2000 ))  RETURNS   VARCHAR ( 2000 )   AS  % jar   / buckets / bfsdefault / javalib / libphonenumber - 4 . 2 . jar ;  import   com . google . i18n . phonenumbers . PhoneNumberUtil ;  import   com . google . i18n . phonenumbers . NumberParseException ;  import   com . google . i18n . phonenumbers . Phonenumber . PhoneNumber ;  class   JPHONE   { \n   static   String   run ( ExaMetadata   exa ,   ExaIterator   ctx )   throws   Exception   { \n     PhoneNumberUtil   phoneUtil   =   PhoneNumberUtil . getInstance (); \n     try   { \n       PhoneNumber   swissNumberProto   =   phoneUtil . parse ( ctx . getString ( \"num\" ), \n                                                      \"DE\" ); \n       return   swissNumberProto . toString (); \n     }   catch   ( NumberParseException   e )   { \n       System . err . println ( \"NumberParseException thrown: \"   +   e . toString ()); \n     } \n     return   \"failed\" ; \n   }  }  /",
            "title": "Java files (.jar)"
        },
        {
            "location": "/methods/exasol-udf/#python-libraries",
            "text": "Python libraries are often provided in the form of\n         .whl  files. You can easily integrate such libraries into\n        Exasol by uploading the file to a bucket and extend the Python search\n        path appropriately.  If you for instance want to use Google's library for processing\n        phone numbers ( https://pypi.python.org/pypi/phonenumbers ), you could\n        upload the file into the bucket named  pylib . The\n        corresponding local path would look like the following:\n         /buckets/bfsdefault/pylib/phonenumbers-7.7.5-py2.py3-none-any.whl .  In the script below you can see how this path is specified to be\n        able to import the library.  CREATE   PYTHON   SCALAR   SCRIPT   phonenumber ( phone_number   VARCHAR ( 2000 ))   RETURNS   VARCHAR ( 2000 )   AS  import   sys  import   glob  sys . path . extend ( glob . glob ( '/buckets/bfsdefault/pylib/*' ))  import   phonenumbers  def   run ( c ): \n    return   str ( phonenumbers . parse ( c . phone_number , None ))  /  SELECT   phonenumber ( '+1-555-2368' )   AS   ghost_busters ;  GHOST_BUSTERS  -----------------------------------------  Country   Code :   1   National   Number :   55552368",
            "title": "Python libraries"
        },
        {
            "location": "/methods/exasol-udf/#r-packages",
            "text": "The installation of R packages is a bit more complex because\n        they have to be compiled using the c compiler. To manage that, you can\n        download the existing Exasol Linux container, compile the package in\n        that container and upload the resulting package into BucketFS. Details\n        about the Linux container will be explained in the following\n        chapter.  A simple method is using Docker as described in the following\n        example.  $> bucketfs = http://192.168.6.75:1234\n$> cont = /default/EXAClusterOS/ScriptLanguages-2016-10-21.tar.gz\n$> docker $bucketfs$cont import my_dock\n$> mkdir r_pkg\n$> docker run -v `pwd`/r_pkg:/r_pkg --name=my_dock -it my_dock /bin/bash  Again we want to use an existing package for processing phone\n        numbers, this time from Steve Myles ( https://cran.r-project.org/web/packages/phonenumber/index.html ).  Within the Docker container, you can start R and install\n        it:  # export R_LIBS=\"/r_pkg/\"\n# R\n> install.packages('phonenumber', repos=\"http://cran.r-project.org\")\nInstalling package into '/r_pkg'\n(as 'lib' is unspecified)\ntrying URL 'http://cran.r-project.org/src/contrib/phonenumber_0.2.2.tar.gz'\nContent type 'application/x-gzip' length 10516 bytes (10 KB)\n==================================================\ndownloaded 10 KB  The first line installs the package from the Linux container\n        into the subfolder  r_pkg  which can be accessed outside\n        the Docker container.  Afterwards, the resulting tgz archive is uploaded into the\n        bucket named  rlib :  $ >   bucketfs   =   http : // w : writepw @ 192 . 168 . 6 . 75 : 1234  $ >   curl   - vX   PUT   - T   r_pkg . tgz   $ bucketfs / rlib / r_pkg . tgz  \nIn the following script you can see how the resulting local path\n        ( /buckets/bfsdefault/rlib/r_pkg ) is specified to be able\n        to use the library.  CREATE   R   SCALAR   SCRIPT   tophone ( letters   VARCHAR ( 2000 ))   RETURNS   INT   AS  . libPaths (   c (   . libPaths (),   \"/buckets/bfsdefault/rlib/r_pkg\" )   )  library ( phonenumber )  run   <-   function ( ctx )   { \n    letterToNumber ( ctx$letters ,   qz   =   1 )  }  /",
            "title": "R packages"
        },
        {
            "location": "/methods/exasol-udf/#installing-new-script-languages",
            "text": "Due to Exasol's open script framework it is simply possible to\n      integrate completely new script languages into Exasol, following these 3\n      steps:   Creating a functioning language client  Upload the resulting client into a bucket  Define a script language alias   The language has to be expanded by some small\n      APIs which implement the communication between Exasol and the language.\n      Afterwards the client is compiled for the usage in the pre-installed\n      Exasol Linux Container, so that it can be started within a secure\n      process on the Exasol cluster.  The last step creates a link between the language client and\n      Exasol's SQL language, actually the  CREATE SCRIPT  command. This facilitates many\n      options to try out new versions of a language or completely new\n      languages and finally replace them completely.  If you for instance plan to migrate from Python 2 to Python 3, you\n      could upload a new client and link the alias  PYTHON \n      temporarily to the new version via  ALTER SESSION . After a thorough testing phase,\n      you can finally switch to the new version for all users via the  ALTER SYSTEM  statement.  On the other side, it is also possible to use both language\n      versions in parallel by defining two aliases separately (e.g.\n       PYTHON2  and  PYTHON3 ).",
            "title": "Installing new script languages"
        },
        {
            "location": "/methods/exasol-udf/#creating-a-script-client",
            "text": "The main task of installing new languages is developing the\n        script client. If you are interested in a certain language, you should\n        first check whether the corresponding client has already been\n        published in our open source repository (see  https://github.com/exasol/script-languages ). Otherwise\n        we would be very happy if you would contribute self-developed clients\n        to our open source community.  A script client is based on a Linux container which is stored in\n        BucketFS. The pre-installed script client for languages Python, Java\n        and R is located in the default bucket of the default BucketFS. Using\n        the Linux container technology, an encapsulated virtual machine is\n        started in a safe environment whenever an SQL query contains script\n        code. The Linux container includes a complete Linux distribution and\n        starts the corresponding script client for executing the script code.\n        In general, you could also upload your own Linux container and combine\n        it with your script client.  The script client has to implement a certain protocol that\n        controls the communication between the scripts and the database. For\n        that purpose, the established technologies ZeroMQ\n    and Google's Protocol Buffers are used. Because\n        of the length, the actual protocol definition is not included in this\n        user manual. For details, please have a look into our open source\n        repository where\n        you'll find the following:",
            "title": "Creating a script client"
        },
        {
            "location": "/methods/exasol-udf/#script-aliases-for-the-integration-into-sql",
            "text": "After building and uploading a script client, you have to\n        configure an alias within Exasol. The database then knows where each\n        script language has been installed.\nYou can change the script aliases via the commands  ALTER SESSION  and  ALTER SYSTEM , either session-wide or\n        system-wide. This is handy especially for using several language\n        versions in parallel, or migrating from one version to another.\nThe current session and system parameters can be found in  EXA_PARAMETERS . The scripts aliases are defined via the\n        parameter  SCRIPT_LANGUAGES :  SELECT   session_value   FROM   exa_parameters   WHERE   parameter_name = 'SCRIPT_LANGUAGES' ;  PARAMETER_NAME  ---------------------------------------------------  PYTHON = builtin_python   R = builtin_r   JAVA = builtin_java  \nThese values are not very meaningful since they are just\n        internal macros to make that parameter compact and to dynamically use\n        the last installed version. Written out, the alias for Java would look\n        like the following: JAVA=localzmq+protobuf:///bfsdefault/default/EXAClusterOS/ScriptLanguages-2016-10-21/?lang=java#buckets/bfsdefault/default/EXASolution-6.0.0/exaudfclient  That alias means that for all  CREATE JAVA SCRIPT \n        statements, the Exasol database will use the script client\n         exaudfclient  from local path\n         buckets/bfsdefault/default/EXASolution-2016-10-21/ ,\n        started within the Exasol Linux container from path\n         bfsdefault/default/EXAClusterOS/ScriptLanguages-2016-10-21 .\n        The used communication protocol is  localzmq+protobuf \n        (this is the only supported protocol so far).  For the three pre-installed languages (Python, R, Java), Exasol\n        uses one single script client which evaluates the parameter\n         lang=java  to differentiate between these languages.\n        That's why the internal macro for the Python alias looks nearly\n        identical. Script clients implemented by users can of course define\n        and evaluate such optional parameters individually.  In general, a script alias contains the following\n        elements: <alias>=localzmq+protobuf:///<path_to_linux-container>/[?<client_param_list>]#<path_to_client>  Please note that a script alias may not contain\n            spaces.  Maybe you have noticed in the Java alias that the path of the\n        Linux container begins with the BucketFS while the client path\n        contains the prefix  buckets/ . The reason for that is that\n        the client is started in a secure environment within the Linux\n        container and may only access the existing (and visible) buckets.\n        Access is granted just to the embedded buckets (via\n         mount ), but not to the real server's underlying file\n        system. The path  /buckets/  can be used read-only from\n        within scripts as described in the previous chapter.  You can define or rename any number of aliases. As mentioned\n        before, we recommend to test such adjustments at first in your own\n        session before making that change globally visible via  ALTER SYSTEM .",
            "title": "Script aliases for the integration into SQL"
        },
        {
            "location": "/methods/exasol-virtual-schema/",
            "text": "Exasol Virtual Schema\n\u00b6\n\n\nVirtual schemas provide a powerful abstraction to conveniently access\narbitrary data sources. Virtual schemas are a kind of read-only link\nto an external source and contain virtual tables which look like\nregular tables except that the actual data are not stored\nlocally.\n\n\nAfter creating a virtual schema, its included tables can be used in\nSQL queries and even combined with persistent tables stored directly\nin Exasol, or with other virtual tables from other virtual\nschemas. The SQL optimizer internally translates the virtual objects\ninto connections to the underlying systems and implicitly transfers\nthe necessary data. SQL conditions are tried to be pushed down to the\ndata sources to ensure minimal data transfer and optimal performance.\n\n\nThat's why this concept creates a kind of logical view on top of\nseveral data sources which could be databases or other data\nservices. By that, you can either implement a harmonized access layer\nfor your reporting tools. Or you can use this technology for agile and\nflexible ETL processing, since you don't need to change anything in\nExasol if you change or extend the objects in the underlying\nsystem.\n\n\nThe following basic example shows you how easy it is to create and use\na virtual schema by using our JDBC adapter to connect Exasol with a\nHive system.\n\n\n-- Create the schema by specifying some adapter access properties\n\n\nCREATE\n \nVIRTUAL\n \nSCHEMA\n \nhive\n\n \nUSING\n \nadapter\n.\njdbc_adapter\n\n \nWITH\n\n  \nCONNECTION_STRING\n \n=\n \n'jdbc:hive://myhost:21050;AuthMech=0'\n\n  \nUSERNAME\n          \n=\n \n'hive-user'\n\n  \nPASSWORD\n          \n=\n \n'secret-password'\n\n  \nSCHEMA_NAME\n       \n=\n \n'default'\n;\n\n\n\n-- Explore the tables in the virtual schema\n\n\nOPEN\n \nSCHEMA\n \nhive\n;\n\n\nSELECT\n \n*\n \nFROM\n \ncat\n;\n\n\nDESCRIBE\n \nclicks\n;\n\n\n\n-- Run queries against the virtual tables\n\n\nSELECT\n \ncount\n(\n*\n)\n \nFROM\n \nclicks\n;\n\n\nSELECT\n \nDISTINCT\n \nUSER_ID\n \nFROM\n \nclicks\n;\n\n\n-- queries can combine virtual and native tables\n\n\nSELECT\n \n*\n \nfrom\n \nclicks\n \nJOIN\n \nnative_schema\n.\nusers\n \nON\n \nclicks\n.\nuserid\n \n=\n \nusers\n.\nid\n;\n\n\n\n\n\nSQL Commands to manage virtual schemas:\n\n\n\n\n\n\nCREATE VIRTUAL SCHEMA\n\n\nCreating a virtual schema.\n\n\n\n\n\n\nDROP VIRTUAL SCHEMA\n\n\nDeleting a virtual schema and all contained  virtual tables.\n\n\n\n\n\n\nALTER VIRTUAL SCHEMA\n\n\nAdjust the properties of an virtual schema or refresh the metadata\nusing the REFRESH option.\n\n\n\n\n\n\nEXPLAIN VIRTUAL\n\n\nUseful to analyze which resulting queries for external systems are\ncreated by the Exasol compiler.\n\n\n\n\n\n\nInstead of shipping just a certain number of supported connectors\n(so-called adapters) to other technologies, we decided to provide\nusers an open, extensible framework where the connectivity logic is\nshared as open source. By that, you can easily use existing adapters,\noptimize them for your need or even create additional adapters to all\nkinds of data sources by your own without any need to wait for a new\nrelease from Exasol.\n\n\nAdapters and properties\n\u00b6\n\n\nIf you create a virtual schema, you have to specify the corresponding\nadapter - to be precise an adapter script - which implements the logic\nhow to access data from the external system. This varies from\ntechnology to technology, but always includes two main task:\n\n\n\n\n\n\nRead metadata\n\n\nReceive information about the objects included in the schema\n (tables, columns, types) and define the logic how to map the data\n types of the source system to the Exasol data types.\n\n\n\n\n\n\nPush down query\n\n\nPush down parts of the Exasol SQL query into an appropriate query\n the the external system. The adapter defines what kind of logic\n Exasol can push down (e.g. filters or certain functions). The\n Exasol optimizer will then try to push down as much as possible\n and execute the rest of the query locally on the Exasol cluster.\n\n\n\n\n\n\nAdapters are similar to UDF scripts. They can be implemented in one of\nthe supported programming languages, for example Java or Python, and\nthey can access the same metadata which is available within UDF\nscripts. To install an adapter you simply download and execute the SQL\nscripts which creates the adapter script in one of your normal\nschemas.\n\n\nThe existing open source adapters provided by Exasol can be found in\nour GitHub repository: \nhttps://www.github.com/exasol/virtual-schemas\n\n\nA very generic implementation is our JDBC adapter with which you can\nintegrate nearly any data source providing a Linux JDBC driver.  For\nsome database systems, an appropriate dialect was already implemented\nto push as much processing as possible down to the underlying\nsystem. Please note that for using this JDBC adapter you have to\nupload the corresponding in BucketFS for the access from adapter\nscripts. Additionally the driver has to be installed via EXAoperation,\nbecause the JDBC adapter executes an implicit IMPORT command).\n\n\nSQL Commands to manage adapter scripts:\n\n\n\n\n\n\nCREATE ADAPTER SCRIPT\n\n\nCreating an adapter script.\n\n\n\n\n\n\nDROP ADAPTER SCRIPT\n\n\nDeleting an adapter script.\n\n\n\n\n\n\nEXPLAIN VIRTUAL\n\n\nUseful to analyze which resulting queries for external systems are\ncreated by the Exasol compiler.\n\n\n\n\n\n\nThe following example shows a shortened adapter script written in\nPython.\n\n\nCREATE\n \nSCHEMA\n \nadapter\n;\n\n\nCREATE\n \nOR\n \nREPLACE\n \nPYTHON\n \nADAPTER\n \nSCRIPT\n \nadapter\n.\nmy_adapter\n \nAS\n\n\ndef\n \nadapter_call\n(\nrequest\n):\n\n  \n#\n \nImplement\n \nyour\n \nadapter\n \nhere\n.\n\n  \n#\n \n...\n\n  \n#\n \nIt\n \nhas\n \nto\n \nreturn\n \nan\n \nappropriate\n \nresponse\n.\n\n\n/\n\n\n\n\n\nAfterwards you can create virtual schemas by providing certain\nproperties which are required for the adapter script (see initial\nexample). These properties typically define the necessary information\nto establish a connection to the external system. In the example, this\nwas the jdbc connection string and the credentials.\n\n\nBut properties can flexibly defined and hence contain all kinds of\nauxiliary data which controls the logic how to use the data source. If\nyou implement or adjust your own adapter scripts, then you can define\nyour own properties and use them appropriately.\n\n\nThe list of specified properties of a specific virtual schema can be\nseen in the system table \nexa_all_virtual_schema_properties\n. After\ncreating the schema, you can adjust these properties using the SQL\ncommand \nalter_schema_statement\n.\n\n\nAfter the virtual schema was created in the described way, you can use\nthe contained tables in the same way as the normal tables in Exasol,\nand even combine them in SQL queries. And if you want to use this\ntechnology just for simple ETL jobs, you can of course simply\nmaterialize a query on the virtual tables:\n\n\nCREATE\n \nVIRTUAL\n \nSCHEMA\n \nhive\n\n \nUSING\n \nadapter\n.\njdbc_adapter\n\n \nWITH\n\n  \nCONNECTION_STRING\n \n=\n \n'jdbc:hive://myhost:21050;AuthMech=0'\n\n  \nUSERNAME\n          \n=\n \n'hive-user'\n\n  \nPASSWORD\n          \n=\n \n'secret-password'\n\n  \nSCHEMA_NAME\n       \n=\n \n'default'\n;\n\n\n\nCREATE\n \nTABLE\n \nmy_schema\n.\nclicks_copy\n \nFROM\n \n  \n(\nSELECT\n \n*\n \nFROM\n \nhive\n.\nclicks\n);\n\n\n\n\n\nGrant access on virtual tables\n\u00b6\n\n\nThe access to virtual data works similar to the creation of view by\nsimply granting the SELECT privilege for the virtual schema. In the\ncase of a virtual schema you can grant this right only for the schema\naltogether (via GRANT SELECT ON SCHEMA). Alternatively, the user is\nthe owner or the schema.\n\n\nInternally, this works similar to views since the check for privileges\nis executed in the name of the script owner. By that the details are\ncompletely encapsulated, i.e. the access to adapter scripts and the\ncredentials to the external system.\n\n\nIf you don't want to grant full access for a virtual schema but\nselectively for certain tables, you can do that by different\nalternatives:\n\n\n\n\n\n\nViews\n\n\nInstead of granting direct access to the virtual schema you can\nalso create views on that data and provide indirect access for\ncertain users.\n\n\n\n\n\n\nLogic within the adapter script\n\n\nIt is possible to solve this requirement directly within the\nadapter script. E.g. in our published JDBC adapter, there exists\nthe parameter \nTABLE_FILTER\n through which you can define a list\nof tables which should be visible (see\n\nhttps://www.github.com/exasol\n). If this virtual schema property is\nnot defined, then all available tables are made visible.\n\n\n\n\n\n\nIn most cases you also need access to the connection details (actually\nthe user and password), because the adapter script needs a direct\nconnection to read the metadata from the external in case of commands\nsuch as CREATE and REFRESH. For that purpose the special ACCESS\nprivilege has been introduced, because of the criticality of these\ndata protection relevant connection details. By the statement GRANT\nACCESS ON CONNECTION [FOR SCRIPT] you can also limit that access only\nto a specific script (FOR SCRIPT clause) and ensure that the\nadministrator cannot access that data himself (e.g. by creating a new\nscript which extracts and simply returns the credentials). Of course\nthat user should only be allowed to execute, but not alter the script\nby any means.\n\n\nIn the example below, the administrator gets the appropriate\nprivileges to create a virtual schema by using the adapter script\n(\njdbc_adapter\n) and a certain connection (\nhive_conn\n).\n\n\nGRANT\n \nCREATE\n \nVIRTUAL\n \nSCHEMA\n \nTO\n \nuser_hive_access\n;\n\n\nGRANT\n \nEXECUTE\n \nON\n \nSCRIPT\n \nadapter\n.\njdbc_adapter\n \nTO\n \nuser_hive_access\n;\n\n\nGRANT\n \nCONNECTION\n \nhive_conn\n \nTO\n \nuser_hive_access\n;\n\n\nGRANT\n \nACCESS\n \nON\n \nCONNECTION\n \nhive_conn\n \n  \nFOR\n \nSCRIPT\n \nadapter\n.\njdbc_adapter\n \nTO\n \nuser_hive_access\n;\n\n\n\n\n\nData Access\n\u00b6\n\n\nEvery time you access data of a virtual schema, on one node of the\ncluster a container of the corresponding language is started, e.g. a\nJVM or a Python container. Inside this container, the code of the\nadapter script will be loaded. Exasol interacts with the adapter\nscript using a simple request-response protocol encoded in JSON. The\ndatabase takes the active part sending the requests by invoking a\ncallback method.\n\n\nIn case of Python, this method is called \nadapter_call\n per\nconvention, but this can vary.\n\n\nLet's take a very easy example of accessing a virtual table.\n\nSELECT\n \nname\n \nFROM\n \nmy_virtual_schema\n.\nusers\n \nWHERE\n \nname\n \nlike\n \n'A%'\n;\n\n\n\n\nThe following happens behind the scenes:\n\n\n\n\n\n\nExasol determines that a virtual table is involved, looks up the\n  corresponding adapter script and starts the language container on\n  one single node in the Exasol cluster.\n\n\n\n\n\n\nExasol sends a request to the adapter, asking for the capabilities\n  of the adapter.\n\n\n\n\n\n\nThe adapter returns a response including the supported capabilities,\n  for example whether it supports specific WHERE clause filters or\n  specific scalar functions.\n\n\n\n\n\n\nExasol sends an appropriate pushdown request by considering the\n  specific adapter capabilities. For example, the information for\n  column projections (in the example above, only the single column\n  name is necessary) or filter conditions is included.\n\n\n\n\n\n\nThe adapter processes this request and sends back a certain SQL query\nin Exasol syntax which will be executed afterwards. This query is\ntypically an IMPORT statement or a SELECT statement including an\nrow-emitting UDF script which cares about the data processing.  The\nexample above could be transformed into these two alternatives (IMPORT\nand SELECT):\n\n\nSELECT\n \nname\n \nFROM\n \n(\n \nIMPORT\n \nFROM\n \nJDBC\n \nAT\n \n...\n \nSTATEMENT\n\n        \n'SELECT name from remoteschema.users WHERE name LIKE \"A%\"'\n\n        \n);\n\n\n\nSELECT\n \nname\n \nFROM\n \n(\n \nSELECT\n\n        \nGET_FROM_MY_DATA_SOURCE\n(\n'data-source-address'\n,\n \n'required_column=name'\n)\n\n        \n)\n \nWHERE\n \nname\n \nLIKE\n \n'A%'\n;\n\n\n\n\n\nIn the first alternative, the adapter can handle filter conditions and\ncreates an IMPORT command including a statement which is sent to the\nexternal system. In the second alternative, a UDF script is used with\ntwo parameters handing over the address of the data source and the\ncolumn projection, but not using any logic for the filter\ncondition. This would then be processed by Exasol rather than the data\nsource.\n\n\nPlease be aware that the examples show the fully transformed query\nwhile only the inner statements are created by the adapter.\n\n\nThe received data is directly integrated into the overall query\nexecution of Exasol.\n\n\nVirtual Schema API\n\u00b6\n\n\nThere are the following request and response types:\n\n\n\n\n\n\n\n\nType\n\n\nCalled ...\n\n\n\n\n\n\n\n\n\n\nCreate Virtual Schema\n\n\n... for each \nCREATE VIRTUAL SCHEMA ...\n statement\n\n\n\n\n\n\nRefresh\n\n\n... for each \nALTER VIRTUAL SCHEMA ... REFRESH ...\n statement.\n\n\n\n\n\n\nSet Properties\n\n\n... for each \nALTER VIRTUAL SCHEMA ... SET ...\n statement.\n\n\n\n\n\n\nDrop Virtual Schema\n\n\n... for each \nDROP VIRTUAL SCHEMA ...\n statement.\n\n\n\n\n\n\nGet Capabilities\n\n\n... whenever a virtual table is queried in a \nSELECT\n statement.\n\n\n\n\n\n\nPushdown\n\n\n... whenever a virtual table is queried in a \nSELECT\n statement.\n\n\n\n\n\n\n\n\nWe describe each of the types in the following sections.\n\n\n\n\nPlease note\n\n\nTo keep the documentation concise we defined the elements which are commonly in separate sections below, e.g.\n\nschemaMetadataInfo\n and \nschemaMetadata\n.\n\n\n\n\nRequests and Responses\n\u00b6\n\n\nCreate Virtual Schema\n\u00b6\n\n\nInforms the Adapter about the request to create a Virtual Schema, and asks the Adapter for the metadata (tables and\ncolumns).\n\n\nThe Adapter is allowed to throw an Exception if the user missed to provide mandatory properties or in case of any other\nproblems (e.g. connectivity).\n\n\nRequest:\n\n\n{\n\n    \n\"type\"\n:\n \n\"createVirtualSchema\"\n,\n\n    \n\"schemaMetadataInfo\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n\n\nResponse:\n\n\n{\n\n    \n\"type\"\n:\n \n\"createVirtualSchema\"\n,\n\n    \n\"schemaMetadata\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nNotes\n\n\nschemaMetadata\n is mandatory. However, it is allowed to contain no tables.\n\n\n\n\nRefresh\n\u00b6\n\n\nRequest to refresh the metadata for the whole Virtual Schema, or for specified tables.\n\n\nRequest:\n\n\n{\n\n    \n\"type\"\n:\n \n\"refresh\"\n,\n\n    \n\"schemaMetadataInfo\"\n:\n \n{\n\n        \n...\n\n    \n},\n\n    \n\"requestedTables\"\n:\n \n[\n\"T1\"\n,\n \n\"T2\"\n]\n\n\n}\n\n\n\n\n\n\n\nNotes\n\n\nrequestedTables\n is optional. If existing, only the specified tables shall be refreshed. The specified tables\ndo not have to exist, it just tell Adapter to update these tables (which might be changed, deleted, added, or\nnon-existing).\n\n\n\n\nResponse:\n\n\n{\n\n    \n\"type\"\n:\n \n\"refresh\"\n,\n\n    \n\"schemaMetadata\"\n:\n \n{\n\n        \n...\n\n    \n},\n\n    \n\"requestedTables\"\n:\n \n[\n\"T1\"\n,\n \n\"T2\"\n]\n\n\n}\n\n\n\n\n\n\n\nNotes\n\n\n\n\nschemaMetadata\n is optional. It can be skipped if the adapter does not want to refresh (e.g. because he\n  detected that there is no change).\n\n\nrequestedTables\n must exist if and only if the element existed in the request. The values must be the same\n  as in the request (to make sure that Adapter only refreshed these tables).\n\n\n\n\n\n\nSet Properties\n\u00b6\n\n\nRequest to set properties. The Adapter can decide whether he needs to send back new metadata. The Adapter is allowed to\nthrow an Exception if the user provided invalid properties or in case of any other problems (e.g. connectivity).\n\n\nRequest:\n\n\n{\n\n    \n\"type\"\n:\n \n\"setProperties\"\n,\n\n    \n\"schemaMetadataInfo\"\n:\n \n{\n\n        \n...\n\n    \n},\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"JDBC_CONNECTION_STRING\"\n:\n \n\"new-jdbc-connection-string\"\n,\n\n        \n\"NEW_PROPERTY\"\n:\n \n\"value of a not yet existing property\"\n\n        \n\"DELETED_PROPERTY\"\n:\n \nnull\n\n    \n}\n\n\n}\n\n\n\n\n\nResponse:\n\n\n{\n\n    \n\"type\"\n:\n \n\"setProperties\"\n,\n\n    \n\"schemaMetadata\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nNotes\n\n\n\n\nRequest:\n A property set to null means that this property was asked to be deleted. Properties set to null might\n  also not have existed before.\n\n\nResponse:\n \nschemaMetadata\n is optional. It only exists if the adapter wants to send back new metadata.\n  The existing metadata are overwritten completely.\n\n\n\n\n\n\nDrop Virtual Schema\n\u00b6\n\n\nInform the Adapter that a Virtual Schema is about to be dropped. The Adapter can update external dependencies if he has\nsuch. The Adapter is not expected to throw an exception, and if he does, it will be ignored.\n\n\nRequest:\n\n\n{\n\n    \n\"type\"\n:\n \n\"dropVirtualSchema\"\n,\n\n    \n\"schemaMetadataInfo\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n\n\nResponse:\n\n\n{\n\n    \n\"type\"\n:\n \n\"dropVirtualSchema\"\n\n\n}\n\n\n\n\n\nGet Capabilities\n\u00b6\n\n\nRequest the list of capabilities supported by the Adapter. Based on these capabilities, the database will collect\neverything that can be pushed down in the current query and sends a pushdown request afterwards.\n\n\nRequest:\n\n\n{\n\n    \n\"type\"\n:\n \n\"getCapabilities\"\n,\n\n    \n\"schemaMetadataInfo\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n\n\nResponse:\n\n\n{\n\n    \n\"type\"\n:\n \n\"getCapabilities\"\n,\n\n    \n\"capabilities\"\n:\n \n[\n\n        \n\"ORDER_BY_COLUMN\"\n,\n\n        \n\"AGGREGATE_SINGLE_GROUP\"\n,\n\n        \n\"LIMIT\"\n,\n\n        \n\"AGGREGATE_GROUP_BY_TUPLE\"\n,\n\n        \n\"FILTER_EXPRESSIONS\"\n,\n\n        \n\"SELECTLIST_EXPRESSIONS\"\n,\n\n        \n\"SELECTLIST_PROJECTION\"\n,\n\n        \n\"AGGREGATE_HAVING\"\n,\n\n        \n\"ORDER_BY_EXPRESSION\"\n,\n\n        \n\"AGGREGATE_GROUP_BY_EXPRESSION\"\n,\n\n        \n\"LIMIT_WITH_OFFSET\"\n,\n\n        \n\"AGGREGATE_GROUP_BY_COLUMN\"\n,\n\n        \n\"FN_PRED_LESSEQUALS\"\n,\n\n        \n\"FN_AGG_COUNT\"\n,\n\n        \n\"LITERAL_EXACTNUMERIC\"\n,\n\n        \n\"LITERAL_DATE\"\n,\n\n        \n\"LITERAL_INTERVAL\"\n,\n\n        \n\"LITERAL_TIMESTAMP_UTC\"\n,\n\n        \n\"LITERAL_TIMESTAMP\"\n,\n\n        \n\"LITERAL_NULL\"\n,\n\n        \n\"LITERAL_STRING\"\n,\n\n        \n\"LITERAL_DOUBLE\"\n,\n\n        \n\"LITERAL_BOOL\"\n\n    \n]\n\n\n}\n\n\n\n\n\nThe set of capabilities in the example above would be sufficient to pushdown all aspects of the following query:\n\nSELECT\n \nuser_id\n,\n \ncount\n(\nurl\n)\n \nFROM\n \nVS\n.\nclicks\n\n \nWHERE\n \nuser_id\n>\n1\n\n \nGROUP\n \nBY\n \nuser_id\n\n \nHAVING\n \ncount\n(\nurl\n)\n>\n1\n\n \nORDER\n \nBY\n \nuser_id\n\n \nLIMIT\n \n10\n;\n\n\n\n\nThe whole set of capabilities is a lot longer. The current list of supported Capabilities can be found in the sources of the JDBC Adapter:\n\n\n\n\nHigh Level Capabilities\n\n\nLiteral Capabilities\n\n\nPredicate Capabilities\n\n\nScalar Function Capabilities\n\n\nAggregate Function Capabilities\n\n\n\n\nPushdown\n\u00b6\n\n\nContains an abstract specification of what to be pushed down, and requests an pushdown SQL statement from the Adapter\nwhich can be used to retrieve the requested data.\n\n\nRequest:\n\n\nRunning the following query\n\nSELECT\n \nuser_id\n,\n \ncount\n(\nurl\n)\n \nFROM\n \nVS\n.\nclicks\n\n \nWHERE\n \nuser_id\n>\n1\n\n \nGROUP\n \nBY\n \nuser_id\n\n \nHAVING\n \ncount\n(\nurl\n)\n>\n1\n\n \nORDER\n \nBY\n \nuser_id\n\n \nLIMIT\n \n10\n;\n\n\n\nwill produce the following Request, assuming that the Adapter has all required capabilities.\n\n\n{\n\n    \n\"type\"\n:\n \n\"pushdown\"\n,\n\n    \n\"pushdownRequest\"\n:\n \n{\n\n        \n\"type\"\n \n:\n \n\"select\"\n,\n\n        \n\"aggregationType\"\n \n:\n \n\"group_by\"\n,\n\n        \n\"from\"\n \n:\n\n        \n{\n\n            \n\"type\"\n \n:\n \n\"table\"\n,\n\n            \n\"name\"\n \n:\n \n\"CLICKS\"\n\n        \n},\n\n        \n\"selectList\"\n \n:\n\n        \n[\n\n            \n{\n\n                \n\"type\"\n \n:\n \n\"column\"\n,\n\n                \n\"name\"\n \n:\n \n\"USER_ID\"\n,\n\n                \n\"columnNr\"\n \n:\n \n1\n,\n\n                \n\"tableName\"\n \n:\n \n\"CLICKS\"\n\n            \n},\n\n            \n{\n\n                \n\"type\"\n \n:\n \n\"function_aggregate\"\n,\n\n                \n\"name\"\n \n:\n \n\"count\"\n,\n\n                \n\"arguments\"\n \n:\n\n                \n[\n\n                    \n{\n\n                        \n\"type\"\n \n:\n \n\"column\"\n,\n\n                        \n\"name\"\n \n:\n \n\"URL\"\n,\n\n                        \n\"columnNr\"\n \n:\n \n2\n,\n\n                        \n\"tableName\"\n \n:\n \n\"CLICKS\"\n\n                    \n}\n\n                \n]\n\n            \n}\n\n        \n],\n\n        \n\"filter\"\n \n:\n\n        \n{\n\n            \n\"type\"\n \n:\n \n\"predicate_less\"\n,\n\n            \n\"left\"\n \n:\n\n            \n{\n\n                \n\"type\"\n \n:\n \n\"literal_exactnumeric\"\n,\n\n                \n\"value\"\n \n:\n \n\"1\"\n\n            \n},\n\n            \n\"right\"\n \n:\n\n            \n{\n\n                \n\"type\"\n \n:\n \n\"column\"\n,\n\n                \n\"name\"\n \n:\n \n\"USER_ID\"\n,\n\n                \n\"columnNr\"\n \n:\n \n1\n,\n\n                \n\"tableName\"\n \n:\n \n\"CLICKS\"\n\n            \n}\n\n        \n},\n\n        \n\"groupBy\"\n \n:\n\n        \n[\n\n            \n{\n\n                \n\"type\"\n \n:\n \n\"column\"\n,\n\n                \n\"name\"\n \n:\n \n\"USER_ID\"\n,\n\n                \n\"columnNr\"\n \n:\n \n1\n,\n\n                \n\"tableName\"\n \n:\n \n\"CLICKS\"\n\n            \n}\n\n        \n],\n\n        \n\"having\"\n \n:\n\n        \n{\n\n            \n\"type\"\n \n:\n \n\"predicate_less\"\n,\n\n            \n\"left\"\n \n:\n\n            \n{\n\n                \n\"type\"\n \n:\n \n\"literal_exactnumeric\"\n,\n\n                \n\"value\"\n \n:\n \n\"1\"\n\n            \n},\n\n            \n\"right\"\n \n:\n\n            \n{\n\n                \n\"type\"\n \n:\n \n\"function_aggregate\"\n,\n\n                \n\"name\"\n \n:\n \n\"count\"\n,\n\n                \n\"arguments\"\n \n:\n\n                \n[\n\n                    \n{\n\n                        \n\"type\"\n \n:\n \n\"column\"\n,\n\n                        \n\"name\"\n \n:\n \n\"URL\"\n,\n\n                        \n\"columnNr\"\n \n:\n \n2\n,\n\n                        \n\"tableName\"\n \n:\n \n\"CLICKS\"\n\n                    \n}\n\n                \n]\n\n            \n}\n\n        \n},\n\n        \n\"orderBy\"\n \n:\n\n        \n[\n\n            \n{\n\n                \n\"type\"\n \n:\n \n\"order_by_element\"\n,\n\n                \n\"expression\"\n \n:\n\n                \n{\n\n                    \n\"type\"\n \n:\n \n\"column\"\n,\n\n                    \n\"columnNr\"\n \n:\n \n1\n,\n\n                    \n\"name\"\n \n:\n \n\"USER_ID\"\n,\n\n                    \n\"tableName\"\n \n:\n \n\"CLICKS\"\n\n                \n},\n\n                \n\"isAscending\"\n \n:\n \ntrue\n,\n\n                \n\"nullsLast\"\n \n:\n \ntrue\n\n            \n}\n\n        \n],\n\n        \n\"limit\"\n \n:\n\n        \n{\n\n            \n\"numElements\"\n \n:\n \n10\n\n        \n}\n\n    \n},\n\n    \n\"involvedTables\"\n:\n \n[\n\n    \n{\n\n        \n\"name\"\n \n:\n \n\"CLICKS\"\n,\n\n        \n\"columns\"\n \n:\n\n        \n[\n\n            \n{\n\n                \n\"name\"\n \n:\n \n\"ID\"\n,\n\n                \n\"dataType\"\n \n:\n\n                \n{\n\n                    \n\"type\"\n \n:\n \n\"DECIMAL\"\n,\n\n                    \n\"precision\"\n \n:\n \n18\n,\n\n                    \n\"scale\"\n \n:\n \n0\n\n                \n}\n\n            \n},\n\n            \n{\n\n                \n\"name\"\n \n:\n \n\"USER_ID\"\n,\n\n                \n\"dataType\"\n \n:\n\n                \n{\n\n                   \n\"type\"\n \n:\n \n\"DECIMAL\"\n,\n\n                   \n\"precision\"\n \n:\n \n18\n,\n\n                    \n\"scale\"\n \n:\n \n0\n\n                \n}\n\n            \n},\n\n            \n{\n\n                \n\"name\"\n \n:\n \n\"URL\"\n,\n\n                \n\"dataType\"\n \n:\n\n                \n{\n\n                   \n\"type\"\n \n:\n \n\"VARCHAR\"\n,\n\n                   \n\"size\"\n \n:\n \n1000\n\n                \n}\n\n            \n},\n\n            \n{\n\n                \n\"name\"\n \n:\n \n\"REQUEST_TIME\"\n,\n\n                \n\"dataType\"\n \n:\n\n                \n{\n\n                    \n\"type\"\n \n:\n \n\"TIMESTAMP\"\n\n                \n}\n\n            \n}\n\n        \n]\n\n    \n}\n\n    \n],\n\n    \n\"schemaMetadataInfo\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nNotes\n\n\n\n\npushdownRequest\n: Specification what needs to be pushed down. You can think of it like a parsed SQL statement.\n\n\nfrom\n: The requested from clause. Currently only tables are supported, joins might be supported in future.\n\n\nselectList\n: The requested select list elements, a list of expression. The order of the selectlist elements\n    matters. If the select list is an empty list, we request at least a single column/expression, which could also\n    be constant TRUE.\n\n\nselectList.columnNr\n: Position of the column in the virtual table, starting with 0\n\n\nfilter\n: The requested filter (where clause), a single expression.\n\n\naggregationType\n: Optional element, set if an aggregation is requested. Either \ngroup_by\n or \nsingle_group\n,\n    if a aggregate function is used but no group by.\n\n\ngroupBy\n: The requested group by clause, a list of expressions.\n\n\nhaving\n: The requested having clause, a single expression.\n\n\norderBy\n: The requested order-by clause, a list of \norder_by_element\n elements. The field \nexpression\n\n    contains the expression to order by.\n\n\nlimit\n The requested limit of the result set, with an optional offset.\n\n\ninvolvedTables\n: Metadata of the involved tables, encoded like in schemaMetadata.\n\n\n\n\n\n\nResponse:\n\n\nFollowing the example above, a valid result could look like this:\n\n\n{\n\n    \n\"type\"\n:\n \n\"pushdown\"\n,\n\n    \n\"sql\"\n:\n \n\"IMPORT FROM JDBC AT 'jdbc:exa:remote-db:8563;schema=native' USER 'sys' IDENTIFIED BY 'exasol' STATEMENT 'SELECT USER_ID, count(URL) FROM NATIVE.CLICKS WHERE 1 < USER_ID GROUP BY USER_ID HAVING 1 < count(URL) ORDER BY USER_ID LIMIT 10'\"\n\n\n}\n\n\n\n\n\n\n\nNote\n\n\nsql\n: The pushdown SQL statement. It must be either an \nSELECT\n or \nIMPORT\n statement.\n\n\n\n\nEmbedded Commonly Used Json Elements\n\u00b6\n\n\nThe following Json objects can be embedded in a request or response. They have a fixed structure.\n\n\nSchema Metadata Info\n\u00b6\n\n\nThis document contains the most important metadata of the virtual schema and is sent to the adapter just \"for\ninformation\" with each request. It is the value of an element called \nschemaMetadataInfo\n.\n\n\n{\n\"schemaMetadataInfo\"\n:{\n\n    \n\"name\"\n:\n \n\"MY_HIVE_VSCHEMA\"\n,\n\n    \n\"adapterNotes\"\n:\n \n{\n\n        \n\"lastRefreshed\"\n:\n \n\"2015-03-01 12:10:01\"\n,\n\n        \n\"key\"\n:\n \n\"Any custom schema state here\"\n\n    \n},\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"HIVE_SERVER\"\n:\n \n\"my-hive-server\"\n,\n\n        \n\"HIVE_DB\"\n:\n \n\"my-hive-db\"\n,\n\n        \n\"HIVE_USER\"\n:\n \n\"my-hive-user\"\n\n    \n}\n\n\n}}\n\n\n\n\n\nSchema Metadata\n\u00b6\n\n\nThis document is usually embedded in responses from the Adapter and informs the database about all metadata of the\nVirtual Schema, especially the contained Virtual Tables and it's columns. The Adapter can store so called \nadapterNotes\n\non each level (schema, table, column), to remember information which might be relevant for the Adapter in future. In the\nexample below, the Adapter remembers the table partitioning and the data type of a column which is not directly\nsupported in EXASOL. The Adapter has these information during pushdown and can consider the table partitioning during\npushdown or can add an appropriate cast for the column.\n\n\n{\n\"schemaMetadata\"\n:{\n\n    \n\"adapterNotes\"\n:\n \n{\n\n        \n\"lastRefreshed\"\n:\n \n\"2015-03-01 12:10:01\"\n,\n\n        \n\"key\"\n:\n \n\"Any custom schema state here\"\n\n    \n},\n\n    \n\"tables\"\n:\n \n[\n\n    \n{\n\n        \n\"type\"\n:\n \n\"table\"\n,\n\n        \n\"name\"\n:\n \n\"EXASOL_CUSTOMERS\"\n,\n\n        \n\"adapterNotes\"\n:\n \n{\n\n            \n\"hivePartitionColumns\"\n:\n \n[\n\"CREATED\"\n,\n \n\"COUNTRY_ISO\"\n]\n\n        \n},\n\n        \n\"columns\"\n:\n \n[\n\n        \n{\n\n            \n\"name\"\n:\n \n\"ID\"\n,\n\n            \n\"dataType\"\n:\n \n{\n\n                \n\"type\"\n:\n \n\"DECIMAL\"\n,\n\n                \n\"precision\"\n:\n \n18\n,\n\n                \n\"scale\"\n:\n \n0\n\n            \n},\n\n            \n\"isIdentity\"\n:\n \ntrue\n\n        \n},\n\n        \n{\n\n            \n\"name\"\n:\n \n\"COMPANY_NAME\"\n,\n\n            \n\"dataType\"\n:\n \n{\n\n                \n\"type\"\n:\n \n\"VARCHAR\"\n,\n\n                \n\"size\"\n:\n \n1000\n,\n\n                \n\"characterSet\"\n:\n \n\"UTF8\"\n\n            \n},\n\n            \n\"default\"\n:\n \n\"foo\"\n,\n\n            \n\"isNullable\"\n:\n \nfalse\n,\n\n            \n\"comment\"\n:\n \n\"The official name of the company\"\n,\n\n            \n\"adapterNotes\"\n:\n \n{\n\n                \n\"hiveType\"\n:\n \n{\n\n                    \n\"dataType\"\n:\n \n\"List<String>\"\n\n                \n}\n\n            \n}\n\n        \n},\n\n        \n{\n\n            \n\"name\"\n:\n \n\"DISCOUNT_RATE\"\n,\n\n            \n\"dataType\"\n:\n \n{\n\n                \n\"type\"\n:\n \n\"DOUBLE\"\n\n            \n}\n\n        \n}\n\n        \n]\n\n    \n},\n\n    \n{\n\n        \n\"type\"\n:\n \n\"table\"\n,\n\n        \n\"name\"\n:\n \n\"TABLE_2\"\n,\n\n        \n\"columns\"\n:\n \n[\n\n        \n{\n\n            \n\"name\"\n:\n \n\"COL1\"\n,\n\n            \n\"dataType\"\n:\n \n{\n\n                \n\"type\"\n:\n \n\"DECIMAL\"\n,\n\n                \n\"precision\"\n:\n \n18\n,\n\n                \n\"scale\"\n:\n \n0\n\n            \n}\n\n        \n},\n\n        \n{\n\n            \n\"name\"\n:\n \n\"COL2\"\n,\n\n            \n\"dataType\"\n:\n \n{\n\n                \n\"type\"\n:\n \n\"VARCHAR\"\n,\n\n                \n\"size\"\n:\n \n1000\n\n            \n}\n\n        \n}\n\n        \n]\n\n    \n}\n\n    \n]\n\n\n}}\n\n\n\n\n\n\n\nNotes\n\n\n\n\nadapterNotes\n is an optional field which can be attached to the schema, a table or a column.\n  It can be an arbitrarily nested Json document.\n\n\n\n\n\n\nThe following EXASOL data types are supported:\n\n\nDecimal:\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_DECIMAL\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"DECIMAL\"\n,\n\n        \n\"precision\"\n:\n \n18\n,\n\n        \n\"scale\"\n:\n \n2\n\n    \n}\n\n\n}\n\n\n\n\n\nDouble:\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_DOUBLE\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"DOUBLE\"\n\n    \n}\n\n\n}\n\n\n\n\n\nVarchar:\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_VARCHAR_UTF8_1\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"VARCHAR\"\n,\n\n        \n\"size\"\n:\n \n10000\n,\n\n        \n\"characterSet\"\n:\n \n\"UTF8\"\n\n    \n}\n\n\n}\n\n\n\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_VARCHAR_UTF8_2\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"VARCHAR\"\n,\n\n        \n\"size\"\n:\n \n10000\n\n    \n}\n\n\n}\n\n\n\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_VARCHAR_ASCII\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"VARCHAR\"\n,\n\n        \n\"size\"\n:\n \n10000\n,\n\n        \n\"characterSet\"\n:\n \n\"ASCII\"\n\n    \n}\n\n\n}\n\n\n\n\n\nChar:\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_CHAR_UTF8_1\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"CHAR\"\n,\n\n        \n\"size\"\n:\n \n3\n\n    \n}\n\n\n}\n\n\n\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_CHAR_UTF8_2\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"CHAR\"\n,\n\n        \n\"size\"\n:\n \n3\n,\n\n        \n\"characterSet\"\n:\n \n\"UTF8\"\n\n    \n}\n\n\n}\n\n\n\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_CHAR_ASCII\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"CHAR\"\n,\n\n        \n\"size\"\n:\n \n3\n,\n\n        \n\"characterSet\"\n:\n \n\"ASCII\"\n\n    \n}\n\n\n}\n\n\n\n\n\nDate:\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_DATE\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"DATE\"\n\n    \n}\n\n\n}\n\n\n\n\n\nTimestamp:\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_TIMESTAMP_1\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"TIMESTAMP\"\n\n    \n}\n\n\n}\n\n\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_TIMESTAMP_2\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"TIMESTAMP\"\n,\n\n        \n\"withLocalTimeZone\"\n:\n \nfalse\n\n    \n}\n\n\n}\n\n\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_TIMESTAMP_3\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"TIMESTAMP\"\n,\n\n        \n\"withLocalTimeZone\"\n:\n \ntrue\n\n    \n}\n\n\n}\n\n\n\n\nBoolean:\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_BOOLEAN\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"BOOLEAN\"\n\n    \n}\n\n\n}\n\n\n\n\n\nGeometry:\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_GEOMETRY\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"GEOMETRY\"\n,\n\n        \n\"srid\"\n:\n \n1\n\n    \n}\n\n\n}\n\n\n\n\nInterval:\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_INTERVAL_DS_1\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"INTERVAL\"\n,\n\n        \n\"fromTo\"\n:\n \n\"DAY TO SECONDS\"\n\n    \n}\n\n\n}\n\n\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_INTERVAL_DS_2\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"INTERVAL\"\n,\n\n        \n\"fromTo\"\n:\n \n\"DAY TO SECONDS\"\n,\n\n        \n\"precision\"\n:\n \n3\n,\n\n        \n\"fraction\"\n:\n \n4\n\n    \n}\n\n\n}\n\n\n\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_INTERVAL_YM_1\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"INTERVAL\"\n,\n\n        \n\"fromTo\"\n:\n \n\"YEAR TO MONTH\"\n\n    \n}\n\n\n}\n\n\n\n\n\n{\n\n    \n\"name\"\n:\n \n\"C_INTERVAL_YM_2\"\n,\n\n    \n\"dataType\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"INTERVAL\"\n,\n\n        \n\"fromTo\"\n:\n \n\"YEAR TO MONTH\"\n,\n\n        \n\"precision\"\n:\n \n3\n\n    \n}\n\n\n}\n\n\n\n\n\nExpressions\n\u00b6\n\n\nThis section handles the expressions that can occur in a pushdown request. Expressions are consistently encoded in the\nfollowing way. This allows easy and consisting parsing and serialization.\n\n\n{\n\n    \n\"type\"\n:\n \n\"<type-of-expression>\"\n,\n\n    \n...\n\n\n}\n\n\n\n\n\nEach expression-type can have any number of additional fields of arbitrary type. In the following sections we define the\nknown expressions.\n\n\nTable\n\u00b6\n\n\nThis element currently only occurs in from clause\n\n\n{\n\n    \n\"type\"\n:\n \n\"table\"\n,\n\n    \n\"name\"\n:\n \n\"CLICKS\"\n\n\n}\n\n\n\n\n\nColumn Lookup\n\u00b6\n\n\n{\n\n    \n\"type\"\n:\n \n\"column\"\n,\n\n    \n\"tableName\"\n:\n \n\"T\"\n,\n\n    \n\"tablePosFromClause\"\n:\n \n0\n,\n\n    \n\"columnNr\"\n:\n \n0\n,\n\n    \n\"name\"\n:\n \n\"ID\"\n\n\n}\n\n\n\n\n\n\n\nNotes\n\n\n\n\ntablePosFromClause\n: Position of the table in the from clause, starting with 0. Required for joins where same\n  table occurs several times.\n\n\ncolumnNr\n: column number in the virtual table, starting with 0\n\n\n\n\n\n\nLiteral\n\u00b6\n\n\n{\n\n    \n\"type\"\n:\n \n\"literal_null\"\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"literal_string\"\n,\n\n    \n\"value\"\n:\n \n\"my string\"\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"literal_double\"\n,\n\n    \n\"value\"\n:\n \n\"1.234\"\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"literal_exactnumeric\"\n,\n\n    \n\"value\"\n:\n \n\"12345\"\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"literal_bool\"\n,\n\n    \n\"value\"\n:\n \ntrue\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"literal_date\"\n,\n\n    \n\"value\"\n:\n \n\"2015-12-01\"\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"literal_timestamp\"\n,\n\n    \n\"value\"\n:\n \n\"2015-12-01 12:01:01.1234\"\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"literal_timestamputc\"\n,\n\n    \n\"value\"\n:\n \n\"2015-12-01 12:01:01.1234\"\n\n\n}\n\n\n\n\n\nPredicates\n\u00b6\n\n\nWhenever there is \n...\n this is a shortcut for an arbitrary expression.\n\n\n{\n\n    \n\"type\"\n:\n \n\"predicate_and\"\n,\n\n    \n\"expressions\"\n:\n \n[\n\n        \n...\n\n    \n]\n\n\n}\n\n\n\n\n\nThe same can be used for \"predicate_or\".\n\n\n{\n\n    \n\"type\"\n:\n \n\"predicate_not\"\n,\n\n    \n\"expression\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"predicate_equals\"\n,\n\n    \n\"left\"\n:\n \n{\n\n        \n...\n\n    \n},\n\n    \n\"right\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n\n\nThe same can be used for \npredicate_notequals\n, \npredicate_less\n and \npredicate_lessequals\n.\n\n\n{\n\n    \n\"type\"\n:\n \n\"predicate_like\"\n,\n\n    \n\"expression\"\n:\n \n{\n\n        \n...\n\n    \n},\n\n    \n\"pattern\"\n:\n \n{\n\n        \n...\n\n    \n},\n\n    \n\"escapeChar\"\n:\n \n\"%\"\n\n\n}\n\n\n\n\n\nThe same can be used for \npredicate_like_regexp\n\n\n\n\nNotes\n\n\n\n\nescapeChar\n is optional\n\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"predicate_between\"\n,\n\n    \n\"expression\"\n:\n \n{\n\n        \n...\n\n    \n},\n\n    \n\"left\"\n:\n \n{\n\n        \n...\n\n    \n},\n\n    \n\"right\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n\n}\n\n\n\n\n\n IN (\n, \n)\n\n\n{\n\n    \n\"type\"\n:\n \n\"predicate_in_constlist\"\n,\n\n    \n\"expression\"\n:\n \n{\n\n        \n...\n\n    \n}\n\n    \n\"arguments\"\n:\n \n[\n\n        \n...\n\n    \n]\n\n\n}\n\n\n\n\n\nScalar Functions\n\u00b6\n\n\nSingle argument (consistent with multiple argument version)\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_scalar\"\n,\n\n    \n\"numArgs\"\n:\n \n1\n,\n\n    \n\"name\"\n:\n \n\"ABS\"\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\nMultiple arguments\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_scalar\"\n,\n\n    \n\"numArgs\"\n:\n \n2\n,\n\n    \n\"name\"\n:\n \n\"POWER\"\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n},\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_scalar\"\n,\n\n    \n\"variableInputArgs\"\n:\n \ntrue\n,\n\n    \n\"name\"\n:\n \n\"CONCAT\"\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n},\n\n    \n{\n\n        \n...\n\n    \n},\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\n\nNote\n\n\nvariableInputArgs\n: default value is \nfalse\n. If true, \nnumArgs\n is not defined.\n\n\n\n\nArithmetic operators have following names: ADD, SUB, MULT, FLOAT_DIV. They are defined as infix (just a hint, not\nnecessary)\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_scalar\"\n,\n\n    \n\"numArgs\"\n:\n \n2\n,\n\n    \n\"name\"\n:\n \n\"ADD\"\n,\n\n    \n\"infix\"\n:\n \ntrue\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n},\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\nSpecial cases\n\n\nEXTRACT(toExtract FROM exp1) (requires scalar-function capability EXTRACT) \n\n\n{\n\n    \n\"type\"\n:\n \n\"function_scalar_extract\"\n,\n\n    \n\"name\"\n:\n \n\"EXTRACT\"\n,\n\n    \n\"toExtract\"\n:\n \n\"MINUTE\"\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n],\n\n\n}\n\n\n\nCAST(exp1 AS dataType) (requires scalar-function capability CAST) \n\n\n{\n\n    \n\"type\"\n:\n \n\"function_scalar_cast\"\n,\n\n    \n\"name\"\n:\n \n\"CAST\"\n,\n\n    \n\"dataType\"\n:\n \n    \n{\n\n        \n\"type\"\n \n:\n \n\"VARCHAR\"\n,\n\n        \n\"size\"\n \n:\n \n10000\n\n    \n},\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n],\n\n\n}\n\n\n\n\n\nCASE (requires scalar-function capability CAST)\n\n\nCASE\n \nbasis\n \nWHEN\n \nexp1\n \nTHEN\n \nresult1\n\n           \nWHEN\n \nexp2\n \nTHEN\n \nresult2\n\n           \nELSE\n \nresult3\n\n           \nEND\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_scalar_case\"\n,\n\n    \n\"name\"\n:\n \n\"CASE\"\n,\n\n    \n\"basis\"\n \n:\n\n    \n{\n\n        \n\"type\"\n \n:\n \n\"column\"\n,\n\n        \n\"columnNr\"\n \n:\n \n0\n,\n\n        \n\"name\"\n \n:\n \n\"NUMERIC_GRADES\"\n,\n\n        \n\"tableName\"\n \n:\n \n\"GRADES\"\n\n    \n},\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n        \n        \n\"type\"\n \n:\n \n\"literal_exactnumeric\"\n,\n\n        \n\"value\"\n \n:\n \n\"1\"\n\n    \n},\n       \n    \n{\n        \n        \n\"type\"\n \n:\n \n\"literal_exactnumeric\"\n,\n\n        \n\"value\"\n \n:\n \n\"2\"\n\n    \n}\n\n    \n],\n\n    \n\"results\"\n:\n \n[\n\n    \n{\n        \n        \n\"type\"\n \n:\n \n\"literal_string\"\n,\n\n        \n\"value\"\n \n:\n \n\"VERY GOOD\"\n\n    \n},\n       \n    \n{\n        \n        \n\"type\"\n \n:\n \n\"literal_string\"\n,\n\n        \n\"value\"\n \n:\n \n\"GOOD\"\n\n    \n},\n\n    \n{\n        \n        \n\"type\"\n \n:\n \n\"literal_string\"\n,\n\n        \n\"value\"\n \n:\n \n\"INVALID\"\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\n\nNotes\n\n\n\n\narguments\n: The different cases.\n\n\nresults\n: The different results in the same order as the arguments. If present, the ELSE result\n  is the last entry in the \nresults\n array.\n\n\n\n\n\n\nAggregate Functions\n\u00b6\n\n\nConsistent with scalar functions. To be detailed: star-operator, distinct, ...\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_aggregate\"\n,\n\n    \n\"name\"\n:\n \n\"SUM\"\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_aggregate\"\n,\n\n    \n\"name\"\n:\n \n\"CORR\"\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n},\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\nSpecial cases\n\n\nCOUNT(exp)     (requires set-function capability COUNT)\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_aggregate\"\n,\n\n    \n\"name\"\n:\n \n\"COUNT\"\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\nCOUNT(*) (requires set-function capability COUNT and COUNT_STAR)\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_aggregate\"\n,\n\n    \n\"name\"\n:\n \n\"COUNT\"\n\n\n}\n\n\n\n\n\nCOUNT(DISTINCT exp)    (requires set-function capability COUNT and COUNT_DISTINCT)\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_aggregate\"\n,\n\n    \n\"name\"\n:\n \n\"COUNT\"\n,\n\n    \n\"distinct\"\n:\n \ntrue\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\nCOUNT((exp1, exp2))   (requires set-function capability COUNT and COUNT_TUPLE)\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_aggregate\"\n,\n\n    \n\"name\"\n:\n \n\"COUNT\"\n,\n\n    \n\"distinct\"\n:\n \ntrue\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n},\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\nAVG(exp)     (requires set-function capability AVG)\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_aggregate\"\n,\n\n    \n\"name\"\n:\n \n\"AVG\"\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\nAVG(DISTINCT exp)    (requires set-function capability AVG and AVG_DISTINCT)\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_aggregate\"\n,\n\n    \n\"name\"\n:\n \n\"AVG\"\n,\n\n    \n\"distinct\"\n:\n \ntrue\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n]\n\n\n}\n\n\n\n\n\nGROUP_CONCAT(DISTINCT exp1 orderBy SEPARATOR ', ') (requires set-function capability GROUP_CONCAT)\n\n\n{\n\n    \n\"type\"\n:\n \n\"function_aggregate_group_concat\"\n,\n\n    \n\"name\"\n:\n \n\"GROUP_CONCAT\"\n,\n\n    \n\"distinct\"\n:\n \ntrue\n,\n\n    \n\"arguments\"\n:\n \n[\n\n    \n{\n\n        \n...\n\n    \n}\n\n    \n],\n\n    \n\"orderBy\"\n \n:\n \n[\n\n        \n{\n\n            \n\"type\"\n \n:\n \n\"order_by_element\"\n,\n\n            \n\"expression\"\n \n:\n\n            \n{\n\n              \n\"type\"\n \n:\n \n\"column\"\n,\n\n               \n\"columnNr\"\n \n:\n \n1\n,\n\n                \n\"name\"\n \n:\n \n\"USER_ID\"\n,\n\n                \n\"tableName\"\n \n:\n \n\"CLICKS\"\n\n            \n},\n\n            \n\"isAscending\"\n \n:\n \ntrue\n,\n\n            \n\"nullsLast\"\n \n:\n \ntrue\n\n        \n}\n\n    \n],\n\n    \n\"separator\"\n:\n \n\", \"\n\n\n}\n\n\n\n\n\n\n\nNotes\n\n\n\n\ndistinct\n: Optional. Requires set-function capability \nGROUP_CONCAT_DISTINCT\n.\n\n\norderBy\n: Optional. The requested order-by clause, a list of \norder_by_element\n elements.\n  The field \nexpression\n contains the expression to order by. The group-by clause of a SELECT query uses the\n  same \norder_by_element\n element type. The clause requires the set-function capability \nGROUP_CONCAT_ORDER_BY\n.\n\n\nseparator\n: Optional. Requires set-function capability \nGROUP_CONCAT_SEPARATOR\n.",
            "title": "Exasol Virtual Schema"
        },
        {
            "location": "/methods/exasol-virtual-schema/#exasol-virtual-schema",
            "text": "Virtual schemas provide a powerful abstraction to conveniently access\narbitrary data sources. Virtual schemas are a kind of read-only link\nto an external source and contain virtual tables which look like\nregular tables except that the actual data are not stored\nlocally.  After creating a virtual schema, its included tables can be used in\nSQL queries and even combined with persistent tables stored directly\nin Exasol, or with other virtual tables from other virtual\nschemas. The SQL optimizer internally translates the virtual objects\ninto connections to the underlying systems and implicitly transfers\nthe necessary data. SQL conditions are tried to be pushed down to the\ndata sources to ensure minimal data transfer and optimal performance.  That's why this concept creates a kind of logical view on top of\nseveral data sources which could be databases or other data\nservices. By that, you can either implement a harmonized access layer\nfor your reporting tools. Or you can use this technology for agile and\nflexible ETL processing, since you don't need to change anything in\nExasol if you change or extend the objects in the underlying\nsystem.  The following basic example shows you how easy it is to create and use\na virtual schema by using our JDBC adapter to connect Exasol with a\nHive system.  -- Create the schema by specifying some adapter access properties  CREATE   VIRTUAL   SCHEMA   hive \n  USING   adapter . jdbc_adapter \n  WITH \n   CONNECTION_STRING   =   'jdbc:hive://myhost:21050;AuthMech=0' \n   USERNAME            =   'hive-user' \n   PASSWORD            =   'secret-password' \n   SCHEMA_NAME         =   'default' ;  -- Explore the tables in the virtual schema  OPEN   SCHEMA   hive ;  SELECT   *   FROM   cat ;  DESCRIBE   clicks ;  -- Run queries against the virtual tables  SELECT   count ( * )   FROM   clicks ;  SELECT   DISTINCT   USER_ID   FROM   clicks ;  -- queries can combine virtual and native tables  SELECT   *   from   clicks   JOIN   native_schema . users   ON   clicks . userid   =   users . id ;   SQL Commands to manage virtual schemas:    CREATE VIRTUAL SCHEMA  Creating a virtual schema.    DROP VIRTUAL SCHEMA  Deleting a virtual schema and all contained  virtual tables.    ALTER VIRTUAL SCHEMA  Adjust the properties of an virtual schema or refresh the metadata\nusing the REFRESH option.    EXPLAIN VIRTUAL  Useful to analyze which resulting queries for external systems are\ncreated by the Exasol compiler.    Instead of shipping just a certain number of supported connectors\n(so-called adapters) to other technologies, we decided to provide\nusers an open, extensible framework where the connectivity logic is\nshared as open source. By that, you can easily use existing adapters,\noptimize them for your need or even create additional adapters to all\nkinds of data sources by your own without any need to wait for a new\nrelease from Exasol.",
            "title": "Exasol Virtual Schema"
        },
        {
            "location": "/methods/exasol-virtual-schema/#adapters-and-properties",
            "text": "If you create a virtual schema, you have to specify the corresponding\nadapter - to be precise an adapter script - which implements the logic\nhow to access data from the external system. This varies from\ntechnology to technology, but always includes two main task:    Read metadata  Receive information about the objects included in the schema\n (tables, columns, types) and define the logic how to map the data\n types of the source system to the Exasol data types.    Push down query  Push down parts of the Exasol SQL query into an appropriate query\n the the external system. The adapter defines what kind of logic\n Exasol can push down (e.g. filters or certain functions). The\n Exasol optimizer will then try to push down as much as possible\n and execute the rest of the query locally on the Exasol cluster.    Adapters are similar to UDF scripts. They can be implemented in one of\nthe supported programming languages, for example Java or Python, and\nthey can access the same metadata which is available within UDF\nscripts. To install an adapter you simply download and execute the SQL\nscripts which creates the adapter script in one of your normal\nschemas.  The existing open source adapters provided by Exasol can be found in\nour GitHub repository:  https://www.github.com/exasol/virtual-schemas  A very generic implementation is our JDBC adapter with which you can\nintegrate nearly any data source providing a Linux JDBC driver.  For\nsome database systems, an appropriate dialect was already implemented\nto push as much processing as possible down to the underlying\nsystem. Please note that for using this JDBC adapter you have to\nupload the corresponding in BucketFS for the access from adapter\nscripts. Additionally the driver has to be installed via EXAoperation,\nbecause the JDBC adapter executes an implicit IMPORT command).  SQL Commands to manage adapter scripts:    CREATE ADAPTER SCRIPT  Creating an adapter script.    DROP ADAPTER SCRIPT  Deleting an adapter script.    EXPLAIN VIRTUAL  Useful to analyze which resulting queries for external systems are\ncreated by the Exasol compiler.    The following example shows a shortened adapter script written in\nPython.  CREATE   SCHEMA   adapter ;  CREATE   OR   REPLACE   PYTHON   ADAPTER   SCRIPT   adapter . my_adapter   AS  def   adapter_call ( request ): \n   #   Implement   your   adapter   here . \n   #   ... \n   #   It   has   to   return   an   appropriate   response .  /   Afterwards you can create virtual schemas by providing certain\nproperties which are required for the adapter script (see initial\nexample). These properties typically define the necessary information\nto establish a connection to the external system. In the example, this\nwas the jdbc connection string and the credentials.  But properties can flexibly defined and hence contain all kinds of\nauxiliary data which controls the logic how to use the data source. If\nyou implement or adjust your own adapter scripts, then you can define\nyour own properties and use them appropriately.  The list of specified properties of a specific virtual schema can be\nseen in the system table  exa_all_virtual_schema_properties . After\ncreating the schema, you can adjust these properties using the SQL\ncommand  alter_schema_statement .  After the virtual schema was created in the described way, you can use\nthe contained tables in the same way as the normal tables in Exasol,\nand even combine them in SQL queries. And if you want to use this\ntechnology just for simple ETL jobs, you can of course simply\nmaterialize a query on the virtual tables:  CREATE   VIRTUAL   SCHEMA   hive \n  USING   adapter . jdbc_adapter \n  WITH \n   CONNECTION_STRING   =   'jdbc:hive://myhost:21050;AuthMech=0' \n   USERNAME            =   'hive-user' \n   PASSWORD            =   'secret-password' \n   SCHEMA_NAME         =   'default' ;  CREATE   TABLE   my_schema . clicks_copy   FROM  \n   ( SELECT   *   FROM   hive . clicks );",
            "title": "Adapters and properties"
        },
        {
            "location": "/methods/exasol-virtual-schema/#grant-access-on-virtual-tables",
            "text": "The access to virtual data works similar to the creation of view by\nsimply granting the SELECT privilege for the virtual schema. In the\ncase of a virtual schema you can grant this right only for the schema\naltogether (via GRANT SELECT ON SCHEMA). Alternatively, the user is\nthe owner or the schema.  Internally, this works similar to views since the check for privileges\nis executed in the name of the script owner. By that the details are\ncompletely encapsulated, i.e. the access to adapter scripts and the\ncredentials to the external system.  If you don't want to grant full access for a virtual schema but\nselectively for certain tables, you can do that by different\nalternatives:    Views  Instead of granting direct access to the virtual schema you can\nalso create views on that data and provide indirect access for\ncertain users.    Logic within the adapter script  It is possible to solve this requirement directly within the\nadapter script. E.g. in our published JDBC adapter, there exists\nthe parameter  TABLE_FILTER  through which you can define a list\nof tables which should be visible (see https://www.github.com/exasol ). If this virtual schema property is\nnot defined, then all available tables are made visible.    In most cases you also need access to the connection details (actually\nthe user and password), because the adapter script needs a direct\nconnection to read the metadata from the external in case of commands\nsuch as CREATE and REFRESH. For that purpose the special ACCESS\nprivilege has been introduced, because of the criticality of these\ndata protection relevant connection details. By the statement GRANT\nACCESS ON CONNECTION [FOR SCRIPT] you can also limit that access only\nto a specific script (FOR SCRIPT clause) and ensure that the\nadministrator cannot access that data himself (e.g. by creating a new\nscript which extracts and simply returns the credentials). Of course\nthat user should only be allowed to execute, but not alter the script\nby any means.  In the example below, the administrator gets the appropriate\nprivileges to create a virtual schema by using the adapter script\n( jdbc_adapter ) and a certain connection ( hive_conn ).  GRANT   CREATE   VIRTUAL   SCHEMA   TO   user_hive_access ;  GRANT   EXECUTE   ON   SCRIPT   adapter . jdbc_adapter   TO   user_hive_access ;  GRANT   CONNECTION   hive_conn   TO   user_hive_access ;  GRANT   ACCESS   ON   CONNECTION   hive_conn  \n   FOR   SCRIPT   adapter . jdbc_adapter   TO   user_hive_access ;",
            "title": "Grant access on virtual tables"
        },
        {
            "location": "/methods/exasol-virtual-schema/#data-access",
            "text": "Every time you access data of a virtual schema, on one node of the\ncluster a container of the corresponding language is started, e.g. a\nJVM or a Python container. Inside this container, the code of the\nadapter script will be loaded. Exasol interacts with the adapter\nscript using a simple request-response protocol encoded in JSON. The\ndatabase takes the active part sending the requests by invoking a\ncallback method.  In case of Python, this method is called  adapter_call  per\nconvention, but this can vary.  Let's take a very easy example of accessing a virtual table. SELECT   name   FROM   my_virtual_schema . users   WHERE   name   like   'A%' ;   The following happens behind the scenes:    Exasol determines that a virtual table is involved, looks up the\n  corresponding adapter script and starts the language container on\n  one single node in the Exasol cluster.    Exasol sends a request to the adapter, asking for the capabilities\n  of the adapter.    The adapter returns a response including the supported capabilities,\n  for example whether it supports specific WHERE clause filters or\n  specific scalar functions.    Exasol sends an appropriate pushdown request by considering the\n  specific adapter capabilities. For example, the information for\n  column projections (in the example above, only the single column\n  name is necessary) or filter conditions is included.    The adapter processes this request and sends back a certain SQL query\nin Exasol syntax which will be executed afterwards. This query is\ntypically an IMPORT statement or a SELECT statement including an\nrow-emitting UDF script which cares about the data processing.  The\nexample above could be transformed into these two alternatives (IMPORT\nand SELECT):  SELECT   name   FROM   (   IMPORT   FROM   JDBC   AT   ...   STATEMENT \n         'SELECT name from remoteschema.users WHERE name LIKE \"A%\"' \n         );  SELECT   name   FROM   (   SELECT \n         GET_FROM_MY_DATA_SOURCE ( 'data-source-address' ,   'required_column=name' ) \n         )   WHERE   name   LIKE   'A%' ;   In the first alternative, the adapter can handle filter conditions and\ncreates an IMPORT command including a statement which is sent to the\nexternal system. In the second alternative, a UDF script is used with\ntwo parameters handing over the address of the data source and the\ncolumn projection, but not using any logic for the filter\ncondition. This would then be processed by Exasol rather than the data\nsource.  Please be aware that the examples show the fully transformed query\nwhile only the inner statements are created by the adapter.  The received data is directly integrated into the overall query\nexecution of Exasol.",
            "title": "Data Access"
        },
        {
            "location": "/methods/exasol-virtual-schema/#virtual-schema-api",
            "text": "There are the following request and response types:     Type  Called ...      Create Virtual Schema  ... for each  CREATE VIRTUAL SCHEMA ...  statement    Refresh  ... for each  ALTER VIRTUAL SCHEMA ... REFRESH ...  statement.    Set Properties  ... for each  ALTER VIRTUAL SCHEMA ... SET ...  statement.    Drop Virtual Schema  ... for each  DROP VIRTUAL SCHEMA ...  statement.    Get Capabilities  ... whenever a virtual table is queried in a  SELECT  statement.    Pushdown  ... whenever a virtual table is queried in a  SELECT  statement.     We describe each of the types in the following sections.   Please note  To keep the documentation concise we defined the elements which are commonly in separate sections below, e.g. schemaMetadataInfo  and  schemaMetadata .",
            "title": "Virtual Schema API"
        },
        {
            "location": "/methods/exasol-virtual-schema/#requests-and-responses",
            "text": "",
            "title": "Requests and Responses"
        },
        {
            "location": "/methods/exasol-virtual-schema/#create-virtual-schema",
            "text": "Informs the Adapter about the request to create a Virtual Schema, and asks the Adapter for the metadata (tables and\ncolumns).  The Adapter is allowed to throw an Exception if the user missed to provide mandatory properties or in case of any other\nproblems (e.g. connectivity).  Request:  { \n     \"type\" :   \"createVirtualSchema\" , \n     \"schemaMetadataInfo\" :   { \n         ... \n     }  }   Response:  { \n     \"type\" :   \"createVirtualSchema\" , \n     \"schemaMetadata\" :   { \n         ... \n     }  }    Notes  schemaMetadata  is mandatory. However, it is allowed to contain no tables.",
            "title": "Create Virtual Schema"
        },
        {
            "location": "/methods/exasol-virtual-schema/#refresh",
            "text": "Request to refresh the metadata for the whole Virtual Schema, or for specified tables.  Request:  { \n     \"type\" :   \"refresh\" , \n     \"schemaMetadataInfo\" :   { \n         ... \n     }, \n     \"requestedTables\" :   [ \"T1\" ,   \"T2\" ]  }    Notes  requestedTables  is optional. If existing, only the specified tables shall be refreshed. The specified tables\ndo not have to exist, it just tell Adapter to update these tables (which might be changed, deleted, added, or\nnon-existing).   Response:  { \n     \"type\" :   \"refresh\" , \n     \"schemaMetadata\" :   { \n         ... \n     }, \n     \"requestedTables\" :   [ \"T1\" ,   \"T2\" ]  }    Notes   schemaMetadata  is optional. It can be skipped if the adapter does not want to refresh (e.g. because he\n  detected that there is no change).  requestedTables  must exist if and only if the element existed in the request. The values must be the same\n  as in the request (to make sure that Adapter only refreshed these tables).",
            "title": "Refresh"
        },
        {
            "location": "/methods/exasol-virtual-schema/#set-properties",
            "text": "Request to set properties. The Adapter can decide whether he needs to send back new metadata. The Adapter is allowed to\nthrow an Exception if the user provided invalid properties or in case of any other problems (e.g. connectivity).  Request:  { \n     \"type\" :   \"setProperties\" , \n     \"schemaMetadataInfo\" :   { \n         ... \n     }, \n     \"properties\" :   { \n         \"JDBC_CONNECTION_STRING\" :   \"new-jdbc-connection-string\" , \n         \"NEW_PROPERTY\" :   \"value of a not yet existing property\" \n         \"DELETED_PROPERTY\" :   null \n     }  }   Response:  { \n     \"type\" :   \"setProperties\" , \n     \"schemaMetadata\" :   { \n         ... \n     }  }    Notes   Request:  A property set to null means that this property was asked to be deleted. Properties set to null might\n  also not have existed before.  Response:   schemaMetadata  is optional. It only exists if the adapter wants to send back new metadata.\n  The existing metadata are overwritten completely.",
            "title": "Set Properties"
        },
        {
            "location": "/methods/exasol-virtual-schema/#drop-virtual-schema",
            "text": "Inform the Adapter that a Virtual Schema is about to be dropped. The Adapter can update external dependencies if he has\nsuch. The Adapter is not expected to throw an exception, and if he does, it will be ignored.  Request:  { \n     \"type\" :   \"dropVirtualSchema\" , \n     \"schemaMetadataInfo\" :   { \n         ... \n     }  }   Response:  { \n     \"type\" :   \"dropVirtualSchema\"  }",
            "title": "Drop Virtual Schema"
        },
        {
            "location": "/methods/exasol-virtual-schema/#get-capabilities",
            "text": "Request the list of capabilities supported by the Adapter. Based on these capabilities, the database will collect\neverything that can be pushed down in the current query and sends a pushdown request afterwards.  Request:  { \n     \"type\" :   \"getCapabilities\" , \n     \"schemaMetadataInfo\" :   { \n         ... \n     }  }   Response:  { \n     \"type\" :   \"getCapabilities\" , \n     \"capabilities\" :   [ \n         \"ORDER_BY_COLUMN\" , \n         \"AGGREGATE_SINGLE_GROUP\" , \n         \"LIMIT\" , \n         \"AGGREGATE_GROUP_BY_TUPLE\" , \n         \"FILTER_EXPRESSIONS\" , \n         \"SELECTLIST_EXPRESSIONS\" , \n         \"SELECTLIST_PROJECTION\" , \n         \"AGGREGATE_HAVING\" , \n         \"ORDER_BY_EXPRESSION\" , \n         \"AGGREGATE_GROUP_BY_EXPRESSION\" , \n         \"LIMIT_WITH_OFFSET\" , \n         \"AGGREGATE_GROUP_BY_COLUMN\" , \n         \"FN_PRED_LESSEQUALS\" , \n         \"FN_AGG_COUNT\" , \n         \"LITERAL_EXACTNUMERIC\" , \n         \"LITERAL_DATE\" , \n         \"LITERAL_INTERVAL\" , \n         \"LITERAL_TIMESTAMP_UTC\" , \n         \"LITERAL_TIMESTAMP\" , \n         \"LITERAL_NULL\" , \n         \"LITERAL_STRING\" , \n         \"LITERAL_DOUBLE\" , \n         \"LITERAL_BOOL\" \n     ]  }   The set of capabilities in the example above would be sufficient to pushdown all aspects of the following query: SELECT   user_id ,   count ( url )   FROM   VS . clicks \n  WHERE   user_id > 1 \n  GROUP   BY   user_id \n  HAVING   count ( url ) > 1 \n  ORDER   BY   user_id \n  LIMIT   10 ;   The whole set of capabilities is a lot longer. The current list of supported Capabilities can be found in the sources of the JDBC Adapter:   High Level Capabilities  Literal Capabilities  Predicate Capabilities  Scalar Function Capabilities  Aggregate Function Capabilities",
            "title": "Get Capabilities"
        },
        {
            "location": "/methods/exasol-virtual-schema/#pushdown",
            "text": "Contains an abstract specification of what to be pushed down, and requests an pushdown SQL statement from the Adapter\nwhich can be used to retrieve the requested data.  Request:  Running the following query SELECT   user_id ,   count ( url )   FROM   VS . clicks \n  WHERE   user_id > 1 \n  GROUP   BY   user_id \n  HAVING   count ( url ) > 1 \n  ORDER   BY   user_id \n  LIMIT   10 ;  \nwill produce the following Request, assuming that the Adapter has all required capabilities.  { \n     \"type\" :   \"pushdown\" , \n     \"pushdownRequest\" :   { \n         \"type\"   :   \"select\" , \n         \"aggregationType\"   :   \"group_by\" , \n         \"from\"   : \n         { \n             \"type\"   :   \"table\" , \n             \"name\"   :   \"CLICKS\" \n         }, \n         \"selectList\"   : \n         [ \n             { \n                 \"type\"   :   \"column\" , \n                 \"name\"   :   \"USER_ID\" , \n                 \"columnNr\"   :   1 , \n                 \"tableName\"   :   \"CLICKS\" \n             }, \n             { \n                 \"type\"   :   \"function_aggregate\" , \n                 \"name\"   :   \"count\" , \n                 \"arguments\"   : \n                 [ \n                     { \n                         \"type\"   :   \"column\" , \n                         \"name\"   :   \"URL\" , \n                         \"columnNr\"   :   2 , \n                         \"tableName\"   :   \"CLICKS\" \n                     } \n                 ] \n             } \n         ], \n         \"filter\"   : \n         { \n             \"type\"   :   \"predicate_less\" , \n             \"left\"   : \n             { \n                 \"type\"   :   \"literal_exactnumeric\" , \n                 \"value\"   :   \"1\" \n             }, \n             \"right\"   : \n             { \n                 \"type\"   :   \"column\" , \n                 \"name\"   :   \"USER_ID\" , \n                 \"columnNr\"   :   1 , \n                 \"tableName\"   :   \"CLICKS\" \n             } \n         }, \n         \"groupBy\"   : \n         [ \n             { \n                 \"type\"   :   \"column\" , \n                 \"name\"   :   \"USER_ID\" , \n                 \"columnNr\"   :   1 , \n                 \"tableName\"   :   \"CLICKS\" \n             } \n         ], \n         \"having\"   : \n         { \n             \"type\"   :   \"predicate_less\" , \n             \"left\"   : \n             { \n                 \"type\"   :   \"literal_exactnumeric\" , \n                 \"value\"   :   \"1\" \n             }, \n             \"right\"   : \n             { \n                 \"type\"   :   \"function_aggregate\" , \n                 \"name\"   :   \"count\" , \n                 \"arguments\"   : \n                 [ \n                     { \n                         \"type\"   :   \"column\" , \n                         \"name\"   :   \"URL\" , \n                         \"columnNr\"   :   2 , \n                         \"tableName\"   :   \"CLICKS\" \n                     } \n                 ] \n             } \n         }, \n         \"orderBy\"   : \n         [ \n             { \n                 \"type\"   :   \"order_by_element\" , \n                 \"expression\"   : \n                 { \n                     \"type\"   :   \"column\" , \n                     \"columnNr\"   :   1 , \n                     \"name\"   :   \"USER_ID\" , \n                     \"tableName\"   :   \"CLICKS\" \n                 }, \n                 \"isAscending\"   :   true , \n                 \"nullsLast\"   :   true \n             } \n         ], \n         \"limit\"   : \n         { \n             \"numElements\"   :   10 \n         } \n     }, \n     \"involvedTables\" :   [ \n     { \n         \"name\"   :   \"CLICKS\" , \n         \"columns\"   : \n         [ \n             { \n                 \"name\"   :   \"ID\" , \n                 \"dataType\"   : \n                 { \n                     \"type\"   :   \"DECIMAL\" , \n                     \"precision\"   :   18 , \n                     \"scale\"   :   0 \n                 } \n             }, \n             { \n                 \"name\"   :   \"USER_ID\" , \n                 \"dataType\"   : \n                 { \n                    \"type\"   :   \"DECIMAL\" , \n                    \"precision\"   :   18 , \n                     \"scale\"   :   0 \n                 } \n             }, \n             { \n                 \"name\"   :   \"URL\" , \n                 \"dataType\"   : \n                 { \n                    \"type\"   :   \"VARCHAR\" , \n                    \"size\"   :   1000 \n                 } \n             }, \n             { \n                 \"name\"   :   \"REQUEST_TIME\" , \n                 \"dataType\"   : \n                 { \n                     \"type\"   :   \"TIMESTAMP\" \n                 } \n             } \n         ] \n     } \n     ], \n     \"schemaMetadataInfo\" :   { \n         ... \n     }  }    Notes   pushdownRequest : Specification what needs to be pushed down. You can think of it like a parsed SQL statement.  from : The requested from clause. Currently only tables are supported, joins might be supported in future.  selectList : The requested select list elements, a list of expression. The order of the selectlist elements\n    matters. If the select list is an empty list, we request at least a single column/expression, which could also\n    be constant TRUE.  selectList.columnNr : Position of the column in the virtual table, starting with 0  filter : The requested filter (where clause), a single expression.  aggregationType : Optional element, set if an aggregation is requested. Either  group_by  or  single_group ,\n    if a aggregate function is used but no group by.  groupBy : The requested group by clause, a list of expressions.  having : The requested having clause, a single expression.  orderBy : The requested order-by clause, a list of  order_by_element  elements. The field  expression \n    contains the expression to order by.  limit  The requested limit of the result set, with an optional offset.  involvedTables : Metadata of the involved tables, encoded like in schemaMetadata.    Response:  Following the example above, a valid result could look like this:  { \n     \"type\" :   \"pushdown\" , \n     \"sql\" :   \"IMPORT FROM JDBC AT 'jdbc:exa:remote-db:8563;schema=native' USER 'sys' IDENTIFIED BY 'exasol' STATEMENT 'SELECT USER_ID, count(URL) FROM NATIVE.CLICKS WHERE 1 < USER_ID GROUP BY USER_ID HAVING 1 < count(URL) ORDER BY USER_ID LIMIT 10'\"  }    Note  sql : The pushdown SQL statement. It must be either an  SELECT  or  IMPORT  statement.",
            "title": "Pushdown"
        },
        {
            "location": "/methods/exasol-virtual-schema/#embedded-commonly-used-json-elements",
            "text": "The following Json objects can be embedded in a request or response. They have a fixed structure.",
            "title": "Embedded Commonly Used Json Elements"
        },
        {
            "location": "/methods/exasol-virtual-schema/#schema-metadata-info",
            "text": "This document contains the most important metadata of the virtual schema and is sent to the adapter just \"for\ninformation\" with each request. It is the value of an element called  schemaMetadataInfo .  { \"schemaMetadataInfo\" :{ \n     \"name\" :   \"MY_HIVE_VSCHEMA\" , \n     \"adapterNotes\" :   { \n         \"lastRefreshed\" :   \"2015-03-01 12:10:01\" , \n         \"key\" :   \"Any custom schema state here\" \n     }, \n     \"properties\" :   { \n         \"HIVE_SERVER\" :   \"my-hive-server\" , \n         \"HIVE_DB\" :   \"my-hive-db\" , \n         \"HIVE_USER\" :   \"my-hive-user\" \n     }  }}",
            "title": "Schema Metadata Info"
        },
        {
            "location": "/methods/exasol-virtual-schema/#schema-metadata",
            "text": "This document is usually embedded in responses from the Adapter and informs the database about all metadata of the\nVirtual Schema, especially the contained Virtual Tables and it's columns. The Adapter can store so called  adapterNotes \non each level (schema, table, column), to remember information which might be relevant for the Adapter in future. In the\nexample below, the Adapter remembers the table partitioning and the data type of a column which is not directly\nsupported in EXASOL. The Adapter has these information during pushdown and can consider the table partitioning during\npushdown or can add an appropriate cast for the column.  { \"schemaMetadata\" :{ \n     \"adapterNotes\" :   { \n         \"lastRefreshed\" :   \"2015-03-01 12:10:01\" , \n         \"key\" :   \"Any custom schema state here\" \n     }, \n     \"tables\" :   [ \n     { \n         \"type\" :   \"table\" , \n         \"name\" :   \"EXASOL_CUSTOMERS\" , \n         \"adapterNotes\" :   { \n             \"hivePartitionColumns\" :   [ \"CREATED\" ,   \"COUNTRY_ISO\" ] \n         }, \n         \"columns\" :   [ \n         { \n             \"name\" :   \"ID\" , \n             \"dataType\" :   { \n                 \"type\" :   \"DECIMAL\" , \n                 \"precision\" :   18 , \n                 \"scale\" :   0 \n             }, \n             \"isIdentity\" :   true \n         }, \n         { \n             \"name\" :   \"COMPANY_NAME\" , \n             \"dataType\" :   { \n                 \"type\" :   \"VARCHAR\" , \n                 \"size\" :   1000 , \n                 \"characterSet\" :   \"UTF8\" \n             }, \n             \"default\" :   \"foo\" , \n             \"isNullable\" :   false , \n             \"comment\" :   \"The official name of the company\" , \n             \"adapterNotes\" :   { \n                 \"hiveType\" :   { \n                     \"dataType\" :   \"List<String>\" \n                 } \n             } \n         }, \n         { \n             \"name\" :   \"DISCOUNT_RATE\" , \n             \"dataType\" :   { \n                 \"type\" :   \"DOUBLE\" \n             } \n         } \n         ] \n     }, \n     { \n         \"type\" :   \"table\" , \n         \"name\" :   \"TABLE_2\" , \n         \"columns\" :   [ \n         { \n             \"name\" :   \"COL1\" , \n             \"dataType\" :   { \n                 \"type\" :   \"DECIMAL\" , \n                 \"precision\" :   18 , \n                 \"scale\" :   0 \n             } \n         }, \n         { \n             \"name\" :   \"COL2\" , \n             \"dataType\" :   { \n                 \"type\" :   \"VARCHAR\" , \n                 \"size\" :   1000 \n             } \n         } \n         ] \n     } \n     ]  }}    Notes   adapterNotes  is an optional field which can be attached to the schema, a table or a column.\n  It can be an arbitrarily nested Json document.    The following EXASOL data types are supported:  Decimal:  { \n     \"name\" :   \"C_DECIMAL\" , \n     \"dataType\" :   { \n         \"type\" :   \"DECIMAL\" , \n         \"precision\" :   18 , \n         \"scale\" :   2 \n     }  }   Double:  { \n     \"name\" :   \"C_DOUBLE\" , \n     \"dataType\" :   { \n         \"type\" :   \"DOUBLE\" \n     }  }   Varchar:  { \n     \"name\" :   \"C_VARCHAR_UTF8_1\" , \n     \"dataType\" :   { \n         \"type\" :   \"VARCHAR\" , \n         \"size\" :   10000 , \n         \"characterSet\" :   \"UTF8\" \n     }  }   { \n     \"name\" :   \"C_VARCHAR_UTF8_2\" , \n     \"dataType\" :   { \n         \"type\" :   \"VARCHAR\" , \n         \"size\" :   10000 \n     }  }   { \n     \"name\" :   \"C_VARCHAR_ASCII\" , \n     \"dataType\" :   { \n         \"type\" :   \"VARCHAR\" , \n         \"size\" :   10000 , \n         \"characterSet\" :   \"ASCII\" \n     }  }   Char:  { \n     \"name\" :   \"C_CHAR_UTF8_1\" , \n     \"dataType\" :   { \n         \"type\" :   \"CHAR\" , \n         \"size\" :   3 \n     }  }   { \n     \"name\" :   \"C_CHAR_UTF8_2\" , \n     \"dataType\" :   { \n         \"type\" :   \"CHAR\" , \n         \"size\" :   3 , \n         \"characterSet\" :   \"UTF8\" \n     }  }   { \n     \"name\" :   \"C_CHAR_ASCII\" , \n     \"dataType\" :   { \n         \"type\" :   \"CHAR\" , \n         \"size\" :   3 , \n         \"characterSet\" :   \"ASCII\" \n     }  }   Date:  { \n     \"name\" :   \"C_DATE\" , \n     \"dataType\" :   { \n         \"type\" :   \"DATE\" \n     }  }   Timestamp:  { \n     \"name\" :   \"C_TIMESTAMP_1\" , \n     \"dataType\" :   { \n         \"type\" :   \"TIMESTAMP\" \n     }  }   { \n     \"name\" :   \"C_TIMESTAMP_2\" , \n     \"dataType\" :   { \n         \"type\" :   \"TIMESTAMP\" , \n         \"withLocalTimeZone\" :   false \n     }  }   { \n     \"name\" :   \"C_TIMESTAMP_3\" , \n     \"dataType\" :   { \n         \"type\" :   \"TIMESTAMP\" , \n         \"withLocalTimeZone\" :   true \n     }  }   Boolean:  { \n     \"name\" :   \"C_BOOLEAN\" , \n     \"dataType\" :   { \n         \"type\" :   \"BOOLEAN\" \n     }  }   Geometry:  { \n     \"name\" :   \"C_GEOMETRY\" , \n     \"dataType\" :   { \n         \"type\" :   \"GEOMETRY\" , \n         \"srid\" :   1 \n     }  }   Interval:  { \n     \"name\" :   \"C_INTERVAL_DS_1\" , \n     \"dataType\" :   { \n         \"type\" :   \"INTERVAL\" , \n         \"fromTo\" :   \"DAY TO SECONDS\" \n     }  }   { \n     \"name\" :   \"C_INTERVAL_DS_2\" , \n     \"dataType\" :   { \n         \"type\" :   \"INTERVAL\" , \n         \"fromTo\" :   \"DAY TO SECONDS\" , \n         \"precision\" :   3 , \n         \"fraction\" :   4 \n     }  }   { \n     \"name\" :   \"C_INTERVAL_YM_1\" , \n     \"dataType\" :   { \n         \"type\" :   \"INTERVAL\" , \n         \"fromTo\" :   \"YEAR TO MONTH\" \n     }  }   { \n     \"name\" :   \"C_INTERVAL_YM_2\" , \n     \"dataType\" :   { \n         \"type\" :   \"INTERVAL\" , \n         \"fromTo\" :   \"YEAR TO MONTH\" , \n         \"precision\" :   3 \n     }  }",
            "title": "Schema Metadata"
        },
        {
            "location": "/methods/exasol-virtual-schema/#expressions",
            "text": "This section handles the expressions that can occur in a pushdown request. Expressions are consistently encoded in the\nfollowing way. This allows easy and consisting parsing and serialization.  { \n     \"type\" :   \"<type-of-expression>\" , \n     ...  }   Each expression-type can have any number of additional fields of arbitrary type. In the following sections we define the\nknown expressions.",
            "title": "Expressions"
        },
        {
            "location": "/methods/exasol-virtual-schema/#table",
            "text": "This element currently only occurs in from clause  { \n     \"type\" :   \"table\" , \n     \"name\" :   \"CLICKS\"  }",
            "title": "Table"
        },
        {
            "location": "/methods/exasol-virtual-schema/#column-lookup",
            "text": "{ \n     \"type\" :   \"column\" , \n     \"tableName\" :   \"T\" , \n     \"tablePosFromClause\" :   0 , \n     \"columnNr\" :   0 , \n     \"name\" :   \"ID\"  }    Notes   tablePosFromClause : Position of the table in the from clause, starting with 0. Required for joins where same\n  table occurs several times.  columnNr : column number in the virtual table, starting with 0",
            "title": "Column Lookup"
        },
        {
            "location": "/methods/exasol-virtual-schema/#literal",
            "text": "{ \n     \"type\" :   \"literal_null\"  }   { \n     \"type\" :   \"literal_string\" , \n     \"value\" :   \"my string\"  }   { \n     \"type\" :   \"literal_double\" , \n     \"value\" :   \"1.234\"  }   { \n     \"type\" :   \"literal_exactnumeric\" , \n     \"value\" :   \"12345\"  }   { \n     \"type\" :   \"literal_bool\" , \n     \"value\" :   true  }   { \n     \"type\" :   \"literal_date\" , \n     \"value\" :   \"2015-12-01\"  }   { \n     \"type\" :   \"literal_timestamp\" , \n     \"value\" :   \"2015-12-01 12:01:01.1234\"  }   { \n     \"type\" :   \"literal_timestamputc\" , \n     \"value\" :   \"2015-12-01 12:01:01.1234\"  }",
            "title": "Literal"
        },
        {
            "location": "/methods/exasol-virtual-schema/#predicates",
            "text": "Whenever there is  ...  this is a shortcut for an arbitrary expression.  { \n     \"type\" :   \"predicate_and\" , \n     \"expressions\" :   [ \n         ... \n     ]  }   The same can be used for \"predicate_or\".  { \n     \"type\" :   \"predicate_not\" , \n     \"expression\" :   { \n         ... \n     }  }   { \n     \"type\" :   \"predicate_equals\" , \n     \"left\" :   { \n         ... \n     }, \n     \"right\" :   { \n         ... \n     }  }   The same can be used for  predicate_notequals ,  predicate_less  and  predicate_lessequals .  { \n     \"type\" :   \"predicate_like\" , \n     \"expression\" :   { \n         ... \n     }, \n     \"pattern\" :   { \n         ... \n     }, \n     \"escapeChar\" :   \"%\"  }   The same can be used for  predicate_like_regexp   Notes   escapeChar  is optional    { \n     \"type\" :   \"predicate_between\" , \n     \"expression\" :   { \n         ... \n     }, \n     \"left\" :   { \n         ... \n     }, \n     \"right\" :   { \n         ... \n     }  }    IN ( ,  )  { \n     \"type\" :   \"predicate_in_constlist\" , \n     \"expression\" :   { \n         ... \n     } \n     \"arguments\" :   [ \n         ... \n     ]  }",
            "title": "Predicates"
        },
        {
            "location": "/methods/exasol-virtual-schema/#scalar-functions",
            "text": "Single argument (consistent with multiple argument version)  { \n     \"type\" :   \"function_scalar\" , \n     \"numArgs\" :   1 , \n     \"name\" :   \"ABS\" , \n     \"arguments\" :   [ \n     { \n         ... \n     } \n     ]  }   Multiple arguments  { \n     \"type\" :   \"function_scalar\" , \n     \"numArgs\" :   2 , \n     \"name\" :   \"POWER\" , \n     \"arguments\" :   [ \n     { \n         ... \n     }, \n     { \n         ... \n     } \n     ]  }   { \n     \"type\" :   \"function_scalar\" , \n     \"variableInputArgs\" :   true , \n     \"name\" :   \"CONCAT\" , \n     \"arguments\" :   [ \n     { \n         ... \n     }, \n     { \n         ... \n     }, \n     { \n         ... \n     } \n     ]  }    Note  variableInputArgs : default value is  false . If true,  numArgs  is not defined.   Arithmetic operators have following names: ADD, SUB, MULT, FLOAT_DIV. They are defined as infix (just a hint, not\nnecessary)  { \n     \"type\" :   \"function_scalar\" , \n     \"numArgs\" :   2 , \n     \"name\" :   \"ADD\" , \n     \"infix\" :   true , \n     \"arguments\" :   [ \n     { \n         ... \n     }, \n     { \n         ... \n     } \n     ]  }   Special cases  EXTRACT(toExtract FROM exp1) (requires scalar-function capability EXTRACT)   { \n     \"type\" :   \"function_scalar_extract\" , \n     \"name\" :   \"EXTRACT\" , \n     \"toExtract\" :   \"MINUTE\" , \n     \"arguments\" :   [ \n     { \n         ... \n     } \n     ],  }  \nCAST(exp1 AS dataType) (requires scalar-function capability CAST)   { \n     \"type\" :   \"function_scalar_cast\" , \n     \"name\" :   \"CAST\" , \n     \"dataType\" :  \n     { \n         \"type\"   :   \"VARCHAR\" , \n         \"size\"   :   10000 \n     }, \n     \"arguments\" :   [ \n     { \n         ... \n     } \n     ],  }   CASE (requires scalar-function capability CAST)  CASE   basis   WHEN   exp1   THEN   result1 \n            WHEN   exp2   THEN   result2 \n            ELSE   result3 \n            END   { \n     \"type\" :   \"function_scalar_case\" , \n     \"name\" :   \"CASE\" , \n     \"basis\"   : \n     { \n         \"type\"   :   \"column\" , \n         \"columnNr\"   :   0 , \n         \"name\"   :   \"NUMERIC_GRADES\" , \n         \"tableName\"   :   \"GRADES\" \n     }, \n     \"arguments\" :   [ \n     {         \n         \"type\"   :   \"literal_exactnumeric\" , \n         \"value\"   :   \"1\" \n     },        \n     {         \n         \"type\"   :   \"literal_exactnumeric\" , \n         \"value\"   :   \"2\" \n     } \n     ], \n     \"results\" :   [ \n     {         \n         \"type\"   :   \"literal_string\" , \n         \"value\"   :   \"VERY GOOD\" \n     },        \n     {         \n         \"type\"   :   \"literal_string\" , \n         \"value\"   :   \"GOOD\" \n     }, \n     {         \n         \"type\"   :   \"literal_string\" , \n         \"value\"   :   \"INVALID\" \n     } \n     ]  }    Notes   arguments : The different cases.  results : The different results in the same order as the arguments. If present, the ELSE result\n  is the last entry in the  results  array.",
            "title": "Scalar Functions"
        },
        {
            "location": "/methods/exasol-virtual-schema/#aggregate-functions",
            "text": "Consistent with scalar functions. To be detailed: star-operator, distinct, ...  { \n     \"type\" :   \"function_aggregate\" , \n     \"name\" :   \"SUM\" , \n     \"arguments\" :   [ \n     { \n         ... \n     } \n     ]  }   { \n     \"type\" :   \"function_aggregate\" , \n     \"name\" :   \"CORR\" , \n     \"arguments\" :   [ \n     { \n         ... \n     }, \n     { \n         ... \n     } \n     ]  }   Special cases  COUNT(exp)     (requires set-function capability COUNT)  { \n     \"type\" :   \"function_aggregate\" , \n     \"name\" :   \"COUNT\" , \n     \"arguments\" :   [ \n     { \n         ... \n     } \n     ]  }   COUNT(*) (requires set-function capability COUNT and COUNT_STAR)  { \n     \"type\" :   \"function_aggregate\" , \n     \"name\" :   \"COUNT\"  }   COUNT(DISTINCT exp)    (requires set-function capability COUNT and COUNT_DISTINCT)  { \n     \"type\" :   \"function_aggregate\" , \n     \"name\" :   \"COUNT\" , \n     \"distinct\" :   true , \n     \"arguments\" :   [ \n     { \n         ... \n     } \n     ]  }   COUNT((exp1, exp2))   (requires set-function capability COUNT and COUNT_TUPLE)  { \n     \"type\" :   \"function_aggregate\" , \n     \"name\" :   \"COUNT\" , \n     \"distinct\" :   true , \n     \"arguments\" :   [ \n     { \n         ... \n     }, \n     { \n         ... \n     } \n     ]  }  \nAVG(exp)     (requires set-function capability AVG)  { \n     \"type\" :   \"function_aggregate\" , \n     \"name\" :   \"AVG\" , \n     \"arguments\" :   [ \n     { \n         ... \n     } \n     ]  }   AVG(DISTINCT exp)    (requires set-function capability AVG and AVG_DISTINCT)  { \n     \"type\" :   \"function_aggregate\" , \n     \"name\" :   \"AVG\" , \n     \"distinct\" :   true , \n     \"arguments\" :   [ \n     { \n         ... \n     } \n     ]  }   GROUP_CONCAT(DISTINCT exp1 orderBy SEPARATOR ', ') (requires set-function capability GROUP_CONCAT)  { \n     \"type\" :   \"function_aggregate_group_concat\" , \n     \"name\" :   \"GROUP_CONCAT\" , \n     \"distinct\" :   true , \n     \"arguments\" :   [ \n     { \n         ... \n     } \n     ], \n     \"orderBy\"   :   [ \n         { \n             \"type\"   :   \"order_by_element\" , \n             \"expression\"   : \n             { \n               \"type\"   :   \"column\" , \n                \"columnNr\"   :   1 , \n                 \"name\"   :   \"USER_ID\" , \n                 \"tableName\"   :   \"CLICKS\" \n             }, \n             \"isAscending\"   :   true , \n             \"nullsLast\"   :   true \n         } \n     ], \n     \"separator\" :   \", \"  }    Notes   distinct : Optional. Requires set-function capability  GROUP_CONCAT_DISTINCT .  orderBy : Optional. The requested order-by clause, a list of  order_by_element  elements.\n  The field  expression  contains the expression to order by. The group-by clause of a SELECT query uses the\n  same  order_by_element  element type. The clause requires the set-function capability  GROUP_CONCAT_ORDER_BY .  separator : Optional. Requires set-function capability  GROUP_CONCAT_SEPARATOR .",
            "title": "Aggregate Functions"
        },
        {
            "location": "/methods/exasol-websockets/",
            "text": "Exasol JSON over WebSockets API\n\u00b6\n\n\nWhy an WebSockets API?\n\u00b6\n\n\nThe JSON over WebSockets client-server protocol allows customers to \nimplement their own drivers for all kinds of platforms using a \nconnection-based web protocol. \n\n\nThe main advantages are flexibility regarding the programming languages \nyou want to integrate EXASOL into, and a more native access compared to \nthe standardized ways of communicating with a database, such as JDBC, \nODBC or ADO.NET, which are mostly old and static standards and create\nadditional complexity due to the necessary driver managers.\n\n\nClients support\n\u00b6\n\n\nCurrently a native Python driver using this WebSocket API has been\nimplemented. By that you don't need any pyodbc bridge anymore, but \ncan connect your Python directly with EXASOL. PyODBC is not ideal due\nto the need for an ODBC driver manager and certain restrictions in \ndata type conversions.\n\n\nFurther languages will be added in the future, and we encourage you\nto provide us feedback what languages you are interested in, and \nmaybe you are even keen to support our community with own developments. \nIt would then be nice if you could share your work with us, and \nwe will of course help you by any means. \n\n\nWebSocket Protocol v1\n\u00b6\n\n\nWebSocket Protocol v1 requires an EXASOL client/server protocol of\nat least v14. It follows the standards IETF as RFC 6455.\n\n\nThe connection server identifies the initial GET request by the client.\nThis request contains information about the used protocol version.\nDepending on this information the matching login and protocol class is\nchosen.\n\n\nAfter the handshake the process is identical to a connection using the\nstandard drivers like JDBC or ODBC: The connection server listens to\nincoming messages and forwards the requests to the database. \n\n\nLogin: Establishes a connection to EXASOL\n\u00b6\n\n\nThis command invokes the login process which establishes a connection\nbetween the client and EXASOL. As long as the connection is open,\nthe user can interact with EXASOL using the commands specified\nbelow.\n\n\nThe login process is composed of four steps:\n\n\n\n\n\n\nThe client sends the login command including the requested protocol \n   version.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"login\"\n\n\nprotocolVersion (number) => requested WebSocket protocol version, (e.g., 1)\n\n\n\n\nRequest JSON format\n   \n{\n\n  \n\"command\"\n:\n \n\"login\"\n,\n\n  \n\"protocolVersion\"\n:\n \n<number>\n\n\n}\n\n\n\n\n\n\n\n\nThe server returns a public key which is used to encode the\n   user's password. The public key can be obtained in one of two ways:\n    a. importing the key using the publicKeyPem field\n    b. constructing the key using the publicKeyModulus and publicKeyExponent fields\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\npublicKeyPem (string) => PEM-formatted, 1024-bit RSA public key used to encode the user's password (see 3.)\n\n\npublicKeyModulus (string) => hexadecimal modulus of the 1024-bit RSA public key used to encode the user's password (see 3.)\n\n\npublicKeyExponent (string) => hexadecimal exponent of the 1024-bit RSA public key used to encode the user's password (see 3.)\n\n\nexception (object, optional) => only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\nResponse JSON format\n   \n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n             \n\"publicKeyPem\"\n:\n \n<string>\n,\n\n             \n\"publicKeyModulus\"\n:\n \n<string>\n,\n\n             \n\"publicKeyExponent\"\n:\n \n<string>\n\n \n},\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\n\n\n\n\nThe client sends the username, encrypted password, and optionally\n   other client information.\n\n\nRequest fields:\n\n\n\n\nusername (string) => EXASOL user name to use for the login process\n\n\npassword (string) => user's password, which is encrypted using publicKey (see 2.) and PKCS #1 v1.5 padding, encoded in Base64 format\n\n\nuseCompression (boolean) => use compression for messages during the session (beginning after the login process is completed)\n\n\nsessionId (number, optional) => requested session ID\n\n\nclientName (string, optional) => client program name, (e.g., \"EXAplus\")\n\n\ndriverName (string, optional) => driver name, (e.g., \"EXA Python\")\n\n\nclientOs (string, optional) => name and version of the client operating system\n\n\nclientOsUsername (string, optional) => client's operating system user name\n\n\nclientLanguage (string, optional) => language setting of the client system\n\n\nclientVersion (string, optional) => client version number\n\n\nclientRuntime (string, optional) => name and version of the client runtime\n\n\nattributes (object, optional) => array of attributes to set for the connection (see below)\n\n\n\n\nRequest JSON format\n   \n \n{\n\n     \n\"username\"\n:\n \n<string>\n,\n\n     \n\"password\"\n:\n \n<string>\n,\n\n     \n\"useCompression\"\n:\n \n<boolean>\n,\n\n     \n\"sessionId\"\n:\n \n<number>\n,\n\n     \n\"clientName\"\n:\n \n<string>\n,\n\n     \n\"driverName\"\n:\n \n<string>\n,\n\n     \n\"clientOs\"\n:\n \n<string>\n,\n\n     \n\"clientOsUsername\"\n:\n \n<string>\n,\n\n     \n\"clientLanguage\"\n:\n \n<string>\n,\n\n     \n\"clientVersion\"\n:\n \n<string>\n,\n\n     \n\"clientRuntime\"\n:\n \n<string>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n}\n\n \n}\n\n\n\n\n\n\n\n\nThe server uses username and password (see 3.) to authenticate the\n   user. If successful, the server replies with an \"ok\" response and a\n   connection is established. If authentication of the user fails, the\n   server sends an \"error\" response to the client indicating that the login\n   process failed and a connection couldn't be established.\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nsessionId (number) => current session ID\n\n\nprotocolVersion (number) => protocol version of the connection (e.g., 14)\n\n\nreleaseVersion (string) => EXASOL version (e.g. \"6.0.0\")\n\n\ndatabaseName (string) => database name (e.g., \"productionDB1\")\n\n\nproductName (string) => EXASOL product name: \"EXASolution\"\n\n\nmaxDataMessageSize (number) => maximum size of a data message in bytes\n\n\nmaxIdentifierLength (number) => maximum length of identifiers\n\n\nmaxVarcharLength (number) =>  maximum length of VARCHAR values\n\n\nidentifierQuoteString (string) => value of the identifier quote string (e.g., \"'\")\n\n\ntimeZone (string) => name of the session time zone\n\n\ntimeZoneBehavior (string) => value of the session option \"TIME_ZONE_BEHAVIOR\"\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\nResponse JSON format\n   \n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n             \n\"sessionId\"\n:\n \n<number>\n,\n\n             \n\"protocolVersion\"\n:\n \n<number>\n,\n\n             \n\"releaseVersion\"\n:\n \n<string>\n,\n\n             \n\"databaseName\"\n:\n \n<string>\n,\n\n             \n\"productName\"\n:\n \n<string>\n,\n\n             \n\"maxDataMessageSize\"\n:\n \n<number>\n,\n\n             \n\"maxIdentifierLength\"\n:\n \n<number>\n,\n\n             \n\"maxVarcharLength\"\n:\n \n<number>\n,\n\n             \n\"identifierQuoteString\"\n:\n \n<string>\n,\n\n             \n\"timeZone\"\n:\n \n<string>\n,\n\n             \n\"timeZoneBehavior\"\n:\n \n<string>\n\n     \n},\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\n\n\n\n\nSubLogin: Establishes a subconnection to EXASOL\n\u00b6\n\n\nThis command invokes the login process, which establishes a\nsubconnection between the client and EXASOL. Using subconnections,\nthe user can interact with EXASOL in parallel using the commands\nspecified below.\n\n\nThe login process is composed of four steps:\n\n\n\n\n\n\nThe client sends the login command including the requested protocol\n   version.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"login\"\n\n\nprotocolVersion (number) => requested WebSocket protocol version, (e.g., 1)\n\n\n\n\nRequest JSON format\n   \n{\n\n     \n\"command\"\n:\n \n\"subLogin\"\n,\n\n     \n\"protocolVersion\"\n:\n \n<number>\n\n\n}\n\n\n\n\n\n\n\n\nThe server returns a public key which is used to encode the\n   user's password. The public key can be obtained in one of two ways:\n    a. importing the key using the publicKeyPem field\n    b. constructing the key using the publicKeyModulus and publicKeyExponent fields.\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\npublicKeyPem (string) => PEM-formatted, 1024-bit RSA public key used to encode the user's password (see 3.)\n\n\npublicKeyModulus (string) => hexadecimal modulus of the 1024-bit RSA public key used to encode the user's password (see 3.)\n\n\npublicKeyExponent (string) => hexadecimal exponent of the 1024-bit RSA public key used to encode the user's password (see 3.)\n\n\nexception (object, optional) => only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\nResponse JSON format\n   \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n             \n\"publicKeyPem\"\n:\n \n<string>\n,\n\n             \n\"publicKeyModulus\"\n:\n \n<string>\n,\n\n             \n\"publicKeyExponent\"\n:\n \n<string>\n\n     \n},\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n\n}\n\n\n\n\n\n\n\n\nThe client sends the username, encrypted password, and token.\n\n\nRequest fields:\n\n\n\n\nusername (string) => EXASOL user name to use for the login process\n\n\npassword (string) => user's password, which is encrypted using publicKey (see 2.) and PKCS #1 v1.5 padding, encoded in Base64 format\n\n\ntoken (number) => token required for subconnection logins (see, EnterParallel)\n\n\n\n\nRequest JSON format\n   \n{\n\n     \n\"username\"\n:\n \n<string>\n,\n\n     \n\"password\"\n:\n \n<string>\n,\n\n     \n\"token\"\n:\n \n<number>\n\n\n}\n\n\n\n\n\n\n\n\nThe server uses username, password, and token (see 3.) to\n   authenticate the user. If successful, the server replies with an\n   \"ok\" response and a subconnection is established. If authentication of\n   the user fails, the server sends an \"error\" response to the client\n   indicating that the login process failed and a subconnection couldn't\n   be established.\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nsessionId (number) => current session ID\n\n\nprotocolVersion (number) => protocol version of the connection (e.g., 14)\n\n\nreleaseVersion (string) => EXASOL version (e.g. \"6.0.0\")\n\n\ndatabaseName (string) => database name (e.g., \"productionDB1\")\n\n\nproductName (string) => EXASOL product name: \"EXASolution\"\n\n\nmaxDataMessageSize (number) => maximum size of a data message in bytes\n\n\nmaxIdentifierLength (number) => maximum length of identifiers\n\n\nmaxVarcharLength (number) =>  maximum length of VARCHAR values\n\n\nidentifierQuoteString (string) => value of the identifier quote string (e.g., \"'\")\n\n\ntimeZone (string) => name of the session time zone\n\n\ntimeZoneBehavior (string) => value of the session option \"TIME_ZONE_BEHAVIOR\"\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\nResponse JSON format\n   \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n             \n\"sessionId\"\n:\n \n<number>\n,\n\n             \n\"protocolVersion\"\n:\n \n<number>\n,\n\n             \n\"releaseVersion\"\n:\n \n<string>\n,\n\n             \n\"databaseName\"\n:\n \n<string>\n,\n\n             \n\"productName\"\n:\n \n<string>\n,\n\n             \n\"maxDataMessageSize\"\n:\n \n<number>\n,\n\n             \n\"maxIdentifierLength\"\n:\n \n<number>\n,\n\n             \n\"maxVarcharLength\"\n:\n \n<number>\n,\n\n             \n\"identifierQuoteString\"\n:\n \n<string>\n,\n\n             \n\"timeZone\"\n:\n \n<string>\n,\n\n             \n\"timeZoneBehavior\"\n:\n \n<string>\n\n     \n},\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n\n}\n\n\n\n\n\n\n\n\nCommands\n\u00b6\n\n\nDisconnect: Closes a connection to EXASOL\n\u00b6\n\n\nThis command closes the connection between the client and EXASOL.\nAfter the connection is closed, it cannot be used for further\ninteraction with EXASOL anymore.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"disconnect\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\n\n\nRequest JSON format\n\n{\n\n     \n\"command\"\n:\n \n\"disconnect\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n}\n\n\n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nattributes (object, optional) => attributes set for the connection (see below)\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n\n}\n\n\n\n\nGetAttributes: Gets the session attribute values\n\u00b6\n\n\nThis command retrieves the session attribute values.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"getAttributes\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\n\n\nJSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"getAttributes\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nattributes (object, optional) => attributes set for the connection (see below)\n\n\nexception(object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nReponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nSetAttributes: Sets the given session attribute values\n\u00b6\n\n\nThis command sets the specified session attribute values.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"getAttributes\"\n\n\nattributes (object, optional) =>  attributes to set for the connection (see below)\n\n\n\n\nRequest JSON format\n\n{\n\n  \n\"command\"\n:\n \n\"setAttributes\"\n,\n\n  \n\"attributes\"\n:\n \n{\n\n    \n//\n \nas\n \ndefined\n \nseparately\n\n  \n}\n\n\n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nattributes (object, optional) =>  attributes set for the connection (see below)\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nCreatePreparedStatement: Creates a prepared statement\n\u00b6\n\n\nThis command creates a prepared statement.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"createPreparedStatement\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\nsqlText (string) => SQL statement\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"createPreparedStatement\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n\"sqlText\"\n:\n \n<string>\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nattributes (object, optional) => attributes set for the connection (see below)\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nstatementHandle (number) => prepared statement handle\n\n\nparameterData (object) => prepared statement parameter information\n\n\nnumColumns (number) => number of columns\n\n\ncolumns (object[]) => array of column metadata objects\n\n\nname (string) => column name: always \"\" as named parameters are not supported\n\n\ndataType (object) => column metadata\n\n\ntype (string) => column data type\n\n\nprecision (number, optional) => column precision\n\n\nscale (number, optional) => column scale\n\n\nsize (number, optional) => maximum size in bytes of a column value\n\n\ncharacterSet (string, optional) => character encoding of a text column\n\n\nwithLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone\n\n\nfraction (number, optional) => fractional part of number\n\n\nsrid (number, optional) => spatial reference system identifier\n\n\n\n\n\n\nnumResults (number) => number of result objects\n\n\nresults (object[]) => array of result objects\n\n\nresultType (string) => type of result: \"resultSet\" or \"rowCount\"\n\n\nrowCount (number, optional) => present if resultType is \"rowCount\", number of rows\n\n\nresultSet (object, optional) => present if resultType is \"resultSet\", result set\n\n\nresultSetHandle (number, optional) => result set handle\n\n\nnumColumns (number) => number of columns in the result set\n\n\nnumRows (number) => number of rows in the result set\n\n\nnumRowsInMessage (number) => number of rows in the current message\n\n\ncolumns (object[]) => array of column metadata objects\n\n\nname (string) => column name\n\n\ndataType (object) => column metadata\n\n\ntype (string) => column data type\n\n\nprecision (number, optional) => column precision\n\n\nscale (number, optional) => column scale\n\n\nsize (number, optional) => maximum size in bytes of a column value\n\n\ncharacterSet (string, optional) => character encoding of a text column\n\n\nwithLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone\n\n\nfraction (number, optional) => fractional part of number\n\n\nsrid (number, optional) => spatial reference system identifier\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n         \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n         \n\"statementHandle\"\n:\n \n<number>\n,\n\n         \n\"parameterData\"\n:\n \n{\n\n             \n\"numColumns\"\n:\n \n<number>\n,\n\n             \n\"columns\"\n:\n \n[\n \n{\n\n                 \n\"name\"\n:\n \n<string>\n,\n\n                 \n\"dataType\"\n:\n \n{\n\n                     \n\"type\"\n:\n \n<string>\n,\n\n                     \n\"precision\"\n:\n \n<number>\n,\n\n                     \n\"scale\"\n:\n \n<number>\n,\n\n                     \n\"size\"\n:\n \n<number>\n,\n\n                     \n\"characterSet\"\n:\n \n<number>\n,\n\n                     \n\"withLocalTimeZone\"\n:\n \n<\ntrue\n \n|\n \nfalse\n>\n,\n\n                     \n\"fraction\"\n:\n \n<number>\n,\n\n                     \n\"srid\"\n:\n \n<number>\n\n                 \n}\n\n             \n}\n \n]\n\n         \n},\n\n         \n\"numResults\"\n:\n \n<number>\n,\n\n         \n\"results\"\n:\n \n[\n \n{\n\n             \n\"resultType\"\n:\n \n<\n\"resultSet\"\n \n|\n \n\"rowCount\"\n>\n,\n\n             \n//\n \nif\n \ntype\n \nis\n \n\"rowCount\"\n\n             \n\"rowCount\"\n:\n \n<number>\n,\n\n             \n//\n \nif\n \ntype\n \nis\n \n\"resultSet\"\n\n             \n\"resultSet\"\n:\n \n{\n\n                 \n\"resultSetHandle\"\n:\n \n<number>\n,\n\n                 \n\"numColumns\"\n:\n \n<number>\n,\n\n                 \n\"numRows\"\n:\n \n<number>\n,\n\n                 \n\"numRowsInMessage\"\n:\n \n<number>\n,\n\n                 \n\"columns\"\n:\n \n[\n \n{\n\n                     \n\"name\"\n:\n \n<string>\n,\n\n                     \n\"dataType\"\n:\n \n{\n\n                         \n\"type\"\n:\n \n<string>\n,\n\n                         \n\"precision\"\n:\n \n<number>\n,\n\n                         \n\"scale\"\n:\n \n<number>\n,\n\n                         \n\"size\"\n:\n \n<number>\n,\n\n                         \n\"characterSet\"\n:\n \n<number>\n,\n\n                         \n\"withLocalTimeZone\"\n:\n \n<\ntrue\n \n|\n \nfalse\n>\n,\n\n                         \n\"fraction\"\n:\n \n<number>\n,\n\n                         \n\"srid\"\n:\n \n<number>\n\n                     \n}\n\n                 \n}\n \n]\n\n             \n}\n\n         \n}\n \n]\n\n     \n},\n\n     \n//\n \nif\n \nstatus\n \nis\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n         \n\"text\"\n:\n \n<string>\n,\n\n         \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nExecutePreparedStatement: Executes a prepared statement\n\u00b6\n\n\nThis command executes a prepared statement which has already been\ncreated.\n\n\nIf the SQL statement returns a result set which has less than 1,000 rows of data, the data will be provided in the data field of resultSet. However if the SQL statement returns a result set which has 1,000 or more rows of data, a result set will be opened whose handle is returned in the resultSetHandle field of resultSet. Using this handle, the data from the result set can be retrieved using the Fetch command. Once the result set is no longer needed, it should be closed using the CloseResultSet command.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"executePreparedStatement\"\n\n\nattributes (object, optional) =>   attributes to set for the connection (see below)\n\n\nstatementHandle (number) => prepared statement handle\n\n\nnumColumns (number) => number of columns in data\n\n\nnumRows (number) => number of rows in data\n\n\ncolumns (object[], optional) => array of column metadata objects\n\n\nname (string) => column name\n\n\ndataType (object) => column metadata\n\n\ntype (string) => column data type\n\n\nprecision (number, optional) => column precision\n\n\nscale (number, optional) => column scale\n\n\nsize (number, optional) => maximum size in bytes of a column value\n\n\ncharacterSet (string, optional) => character encoding of a text column\n\n\nwithLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone\n\n\nfraction (number, optional) => fractional part of number\n\n\nsrid (number, optional) => spatial reference system identifier\n\n\n\n\n\n\ndata (array[], optional) => array containing the data for the prepared statement in column-major order\n\n\n\n\nRequest JSON format\n\n \n{\n\n    \n\"command\"\n:\n \n\"executePreparedStatement\"\n,\n\n    \n\"attributes\"\n:\n \n{\n\n        \n//\n \nas\n \ndefined\n \nseparately\n\n    \n},\n\n    \n\"statementHandle\"\n:\n \n<number>\n,\n\n    \n\"numColumns\"\n:\n \n<number>\n,\n\n    \n\"numRows\"\n:\n \n<number>\n,\n\n    \n\"columns\"\n:\n \n[\n \n{\n\n        \n\"name\"\n:\n \n<string>\n,\n\n        \n\"dataType\"\n:\n \n{\n\n            \n\"type\"\n:\n \n<string>\n,\n\n            \n\"precision\"\n:\n \n<number>\n,\n\n            \n\"scale\"\n:\n \n<number>\n,\n\n            \n\"size\"\n:\n \n<number>\n,\n\n            \n\"characterSet\"\n:\n \n<number>\n,\n\n            \n\"withLocalTimeZone\"\n:\n \n<boolean>\n,\n\n            \n\"fraction\"\n:\n \n<number>\n,\n\n            \n\"srid\"\n:\n \n<number>\n\n        \n}\n\n    \n}\n \n],\n\n    \n\"data\"\n:\n \n[\n\n        \n[\n\n            \n<string\n \n|\n \nnumber\n \n|\n \ntrue\n \n|\n \nfalse\n \n|\n \nnull\n>\n\n        \n]\n\n    \n]\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nattributes (object, optional) => attributes set for the connection (see below)\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nnumResults (number) => number of result objects\n\n\nresults (object[]) => array of result objects\n\n\nresultType (string) => type of result: \"resultSet\" or \"rowCount\"\n\n\nrowCount (number, optional) => present if resultType is \"rowCount\", number of rows\n\n\nresultSet (object, optional) => present if resultType is \"resultSet\", result set\n\n\nresultSetHandle (number, optional) => result set handle\n\n\nnumColumns (number) => number of columns in the result set\n\n\nnumRows (number) => number of rows in the result set\n\n\nnumRowsInMessage (number) => number of rows in the current message\n\n\ncolumns (object[]) => array of column metadata objects\n\n\nname (string) => column name\n\n\ndataType (object) => column metadata\n\n\ntype (string) => column data type\n\n\nprecision (number, optional) => column precision\n\n\nscale (number, optional) => column scale\n\n\nsize (number, optional) => maximum size in bytes of a column value\n\n\ncharacterSet (string, optional) => character encoding of a text column\n\n\nwithLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone\n\n\nfraction (number, optional) => fractional part of number\n\n\nsrid (number, optional) => spatial reference system identifier\n\n\n\n\n\n\ndata (array[], optional) => object containing the data for the prepared statement in column-major order\n\n\n\n\n\n\n\n\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n         \n// as defined separately\n\n     \n},\n\n     \n// in case of \"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n         \n\"numResults\"\n:\n \n<\nnumber\n>\n,\n\n         \n\"results\"\n:\n \n[\n \n{\n\n             \n\"resultType\"\n:\n \n<\n\"resultSet\"\n \n|\n \n\"rowCount\"\n>\n,\n\n             \n// if type is \"rowCount\"\n\n             \n\"rowCount\"\n:\n \n<\nnumber\n>\n,\n\n             \n// if type is \"resultSet\"\n\n             \n\"resultSet\"\n:\n \n{\n\n                 \n\"resultSetHandle\"\n:\n \n<\nnumber\n>\n,\n\n                 \n\"numColumns\"\n:\n \n<\nnumber\n>\n,\n\n                 \n\"numRows\"\n:\n \n<\nnumber\n>\n,\n\n                 \n\"numRowsInMessage\"\n:\n \n<\nnumber\n>\n,\n\n                 \n\"columns\"\n:\n \n[\n \n{\n\n                     \n\"name\"\n:\n \n<\nstring\n>\n,\n\n                     \n\"dataType\"\n:\n \n{\n\n                         \n\"type\"\n:\n \n<\nstring\n>\n,\n\n                         \n\"precision\"\n:\n \n<\nnumber\n>\n,\n\n                         \n\"scale\"\n:\n \n<\nnumber\n>\n,\n\n                         \n\"size\"\n:\n \n<\nnumber\n>\n,\n\n                         \n\"characterSet\"\n:\n \n<\nnumber\n>\n,\n\n                         \n\"withLocalTimeZone\"\n:\n \n<\ntrue\n \n|\n \nfalse\n>\n,\n\n                         \n\"fraction\"\n:\n \n<\nnumber\n>\n,\n\n                         \n\"srid\"\n:\n \n<\nnumber\n>\n\n                     \n}\n\n                 \n}\n \n],\n\n                 \n\"data\"\n:\n \n[\n\n                     \n[\n\n                         \n<\nstring\n \n|\n \nnumber\n \n|\n \ntrue\n \n|\n \nfalse\n \n|\n \nnull\n>\n\n                     \n]\n\n                 \n]\n\n             \n}\n\n         \n}\n \n]\n\n     \n},\n\n     \n// in case of \"error\"\n\n     \n\"exception\"\n:\n \n{\n\n         \n\"text\"\n:\n \n<\nstring\n>\n,\n\n         \n\"sqlCode\"\n:\n \n<\nstring\n>\n\n     \n}\n\n \n}\n\n\n\n\nClosePreparedStatement: Closes a prepared statement\n\u00b6\n\n\nThis command closes a prepared statement which has already been\ncreated.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"closePreparedStatement\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\nstatementHandle (number) => prepared statement handle\n\n\n\n\nRequest JSON format\n\n{\n\n    \n\"command\"\n:\n \n\"closePreparedStatement\"\n,\n\n    \n\"attributes\"\n:\n \n{\n\n      \n//\n \nas\n \ndefined\n \nseparately\n\n    \n},\n\n    \n\"statementHandle\"\n:\n \n<number>\n\n\n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nattributes (object) => attributes set for the connection (see below)\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nExecute: Executes an SQL statement\n\u00b6\n\n\nThis command executes an SQL statement.\n\n\nIf the SQL statement returns a result set which has less than 1,000 rows of data, the data will be provided in the data field of resultSet. However if the SQL statement returns a result set which has 1,000 or more rows of data, a result set will be opened whose handle is returned in the resultSetHandle field of resultSet. Using this handle, the data from the result set can be retrieved using the Fetch command. Once the result set is no longer needed, it should be closed using the CloseResultSet command.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"executePreparedStatement\"\n\n\nattributes (object) => attributes to set for the connection (see below)\n\n\nsqlText (string) => SQL statement to execute\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"execute\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n\"sqlText\"\n:\n \n<string>\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nattributes (object, optional) => attributes set for the connection (see below)\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nnumResults (number) => number of result objects\n\n\nresults (object[]) => array of result objects\n\n\nresultType (string) => type of result: \"resultSet\" or \"rowCount\"\n\n\nrowCount (number, optional) => present if resultType is \"rowCount\", number of rows\n\n\nresultSet (object, optional) => present if resultType is \"resultSet\", result set\n\n\nresultSetHandle (number) => result set handle\n\n\nnumColumns (number) => number of columns in the result set\n\n\nnumRows (number) => number of rows in the result set\n\n\nnumRowsInMessage (number, optional) => number of rows in the current message\n\n\ncolumns (object[]) => array of column metadata objects\n\n\nname (string) => column name\n\n\ndataType (object) => column metadata\n\n\ntype (string) => column data type\n\n\nprecision (number, optional) => column precision\n\n\nscale (number, optional) => column scale\n\n\nsize (number, optional) => maximum size in bytes of a column value\n\n\ncharacterSet (string, optional) => character encoding of a text column\n\n\nwithLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone\n\n\nfraction (number, optional) => fractional part of number\n\n\nsrid (number, optional) => spatial reference system identifier\n\n\n\n\n\n\ndata (array[], optional) => object containing the data for the prepared statement in column-major order\n\n\n\n\n\n\n\n\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n         \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n \n//\n \nOptional:\n \nok\n\n         \n\"numResults\"\n:\n \n<number>\n,\n\n         \n\"results\"\n:\n \n[\n \n{\n\n             \n\"resultType\"\n:\n \n<\n\"resultSet\"\n \n|\n \n\"rowCount\"\n>\n,\n\n             \n//\n \nif\n \ntype\n \nis\n \n\"rowCount\"\n\n             \n\"rowCount\"\n:\n \n<number>\n,\n\n             \n//\n \nif\n \ntype\n \nis\n \n\"resultSet\"\n\n             \n\"resultSet\"\n:\n \n{\n\n                 \n\"resultSetHandle\"\n:\n \n<number>\n,\n\n                 \n\"numColumns\"\n:\n \n<number>\n,\n\n                 \n\"numRows\"\n:\n \n<number>\n,\n\n                 \n\"numRowsInMessage\"\n:\n \n<number>\n,\n\n                 \n\"columns\"\n:\n \n[\n \n{\n\n                     \n\"name\"\n:\n \n<string>\n,\n\n                     \n\"dataType\"\n:\n \n{\n\n                         \n\"type\"\n:\n \n<string>\n,\n\n                         \n\"precision\"\n:\n \n<number>\n,\n\n                         \n\"scale\"\n:\n \n<number>\n,\n\n                         \n\"size\"\n:\n \n<number>\n,\n\n                         \n\"characterSet\"\n:\n \n<number>\n,\n\n                         \n\"withLocalTimeZone\"\n:\n \n<\ntrue\n \n|\n \nfalse\n>\n,\n\n                         \n\"fraction\"\n:\n \n<number>\n,\n\n                         \n\"srid\"\n:\n \n<number>\n\n                     \n}\n\n                 \n}\n \n],\n\n                 \n\"data\"\n:\n \n[\n\n                     \n[\n\n                         \n<string\n \n|\n \nnumber\n \n|\n \ntrue\n \n|\n \nfalse\n \n|\n \nnull\n>\n\n                     \n]\n\n                 \n]\n\n             \n}\n\n         \n}\n \n]\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n         \n\"text\"\n:\n \n<string>\n,\n\n         \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nFetch: Retrieves data from a result set\n\u00b6\n\n\nThis command retrieves data from a result set.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"fetch\"\n\n\nattributes (object, optional) =>  attributes to set for the connection (see below)\n\n\nresultSetHandle (number) => result set handle\n\n\nstartPosition (number) => row offset (0-based) from which to begin data retrieval\n\n\nnumBytes (number) => number of bytes to retrieve (max: 64MB)\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"fetch\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n\"resultSetHandle\"\n:\n \n<number>\n,\n\n     \n\"startPosition\"\n:\n \n<number>\n,\n\n     \n\"numBytes\"\n:\n \n<number>\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nnumRows (number) => number of rows fetched from the result set\n\n\ndata (array[]) => object containing the data for the prepared statement in column-major order\n\n\n\n\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n             \n\"numRows\"\n:\n \n<number>\n,\n\n             \n\"data\"\n:\n \n[\n\n                 \n[\n\n                     \n<string\n \n|\n \nnumber\n \n|\n \ntrue\n \n|\n \nfalse\n \n|\n \nnull\n>\n\n                 \n]\n\n             \n]\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nCloseResultSet: Closes a result set\n\u00b6\n\n\nThis command closes result sets.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"closeResultSet\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\nresultSetHandles (number[]) => array of result set handles\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"closeResultSet\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n\"resultSetHandles\"\n:\n \n[\n \n<number>\n \n]\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n \n//\n \nOptional:\n \nerror\n\n             \n\"text\"\n:\n \n<string>\n,\n \n//\n \nException\n \ntext\n\n             \n\"sqlCode\"\n:\n \n<string>\n \n//\n \nFive-character\n \nexception\n \ncode\n \nif\n \nknown\n,\n \notherwise\n \n\"00000\"\n\n     \n}\n\n \n}\n\n\n\n\nGetHosts: Gets the hosts in a cluster\n\u00b6\n\n\nThis command gets the number hosts and the IP address of each host in\nan EXASOL cluster.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"getHosts\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"getHosts\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n}\n\n \n}\n\n\n\n\nResponse fields:\n  * status (string) => command status: \"ok\" or \"error\"\n  * responseData (object, optional) => only present if status is \"ok\"\n    * numNodes (number) => number of nodes in the cluster\n    * nodes (string[]) => array of cluster node IP addresses\n  * exception (object, optional) =>  only present if status is \"error\"\n    * text (string) => exception message which provides error\n         details\n    * sqlCode (string) => five-character exception code if known,\n         otherwise \"00000\"\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n             \n\"numNodes\"\n:\n \n<number>\n,\n\n             \n\"nodes\"\n:\n \n[\n\n                     \n<string>\n\n             \n]\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nExecuteBatch: Executes multiple SQL statements as a batch\n\u00b6\n\n\nThis command executes multiple SQL statements sequentially as a batch.\n\n\nIf the SQL statement returns a result set which has less than 1,000 rows of data, the data will be provided in the data field of resultSet. However if the SQL statement returns a result set which has 1,000 or more rows of data, a result set will be opened whose handle is returned in the resultSetHandle field of resultSet. Using this handle, the data from the result set can be retrieved using the Fetch command. Once the result set is no longer needed, it should be closed using the CloseResultSet command.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"executeBatch\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\nsqlTexts (string[]) => array of SQL statement to execute\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"executeBatch\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n\"sqlTexts\"\n:\n \n[\n\n             \n<string>\n\n     \n]\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nattributes (object, optional) => attributes set for the connection (see below)\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nnumResults (number) => number of result objects\n\n\nresults (object[]) => array of result objects\n\n\nresultType (string) => type of result: \"resultSet\" or \"rowCount\"\n\n\nrowCount (number, optional) => present if resultType is \"rowCount\", number of rows\n\n\nresultSet (object, optional) => present if resultType is \"resultSet\", result set\n\n\nresultSetHandle (number, optional) => result set handle\n\n\nnumColumns (number) => number of columns in the result set\n\n\nnumRows (number) => number of rows in the result set\n\n\nnumRowsInMessage (number) => number of rows in the current message\n\n\ncolumns (object[]) => array of column metadata objects\n\n\nname (string) => column name\n\n\ndataType (object) => column metadata\n\n\ntype (string) => column data type\n\n\nprecision (number, optional) => column precision\n\n\nscale (number, optional) => column scale\n\n\nsize (number, optional) => maximum size in bytes of a column value\n\n\ncharacterSet (string, optional) => character encoding of a text column\n\n\nwithLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone\n\n\nfraction (number, optional) => fractional part of number\n\n\nsrid (number, optional) => spatial reference system identifier\n\n\n\n\n\n\ndata (array[], optional) => object containing the data for the prepared statement in column-major order\n\n\n\n\n\n\n\n\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n         \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n         \n\"numResults\"\n:\n \n<number>\n,\n\n         \n\"results\"\n:\n \n[\n \n{\n\n             \n\"resultType\"\n:\n \n<\n\"resultSet\"\n \n|\n \n\"rowCount\"\n>\n,\n\n             \n//\n \nif\n \ntype\n \nis\n \n\"rowCount\"\n\n             \n\"rowCount\"\n:\n \n<number>\n,\n\n             \n//\n \nif\n \ntype\n \nis\n \n\"resultSet\"\n\n             \n\"resultSet\"\n:\n \n{\n\n                 \n\"resultSetHandle\"\n:\n \n<number>\n,\n\n                 \n\"numColumns\"\n:\n \n<number>\n,\n\n                 \n\"numRows\"\n:\n \n<number>\n,\n\n                 \n\"numRowsInMessage\"\n:\n \n<number>\n,\n\n                 \n\"columns\"\n:\n \n[\n \n{\n\n                     \n\"name\"\n:\n \n<string>\n,\n\n                     \n\"dataType\"\n:\n \n{\n\n                         \n\"type\"\n:\n \n<string>\n,\n\n                         \n\"precision\"\n:\n \n<number>\n,\n\n                         \n\"scale\"\n:\n \n<number>\n,\n\n                         \n\"size\"\n:\n \n<number>\n,\n\n                         \n\"characterSet\"\n:\n \n<number>\n,\n\n                         \n\"withLocalTimeZone\"\n:\n \n<\ntrue\n \n|\n \nfalse\n>\n,\n\n                         \n\"fraction\"\n:\n \n<number>\n,\n\n                         \n\"srid\"\n:\n \n<number>\n\n                     \n}\n\n                 \n}\n \n],\n\n                 \n\"data\"\n:\n \n[\n\n                     \n[\n\n                         \n<string\n \n|\n \nnumber\n \n|\n \ntrue\n \n|\n \nfalse\n \n|\n \nnull\n>\n\n                     \n]\n\n                 \n]\n\n             \n}\n\n         \n}\n \n]\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n         \n\"text\"\n:\n \n<string>\n,\n\n         \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nEnterParallel: Opens subconnections for parallel execution\n\u00b6\n\n\nThis command opens subconnections, which are additional connections to\nother nodes in the cluster, for the purpose of parallel execution. If\nthe requested number of subconnections is 0, all open subconnections\nare closed.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"enterParallel\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\nnumRequestedConnections (number) => number of subconnections to open. If 0, all open subconnections are closed.\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"enterParallel\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n\"numRequestedConnections\"\n:\n \n<number>\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nnumOpenConnections (number) => number of subconnections actually opened\n\n\ntoken (number) => token required for the login of subconnections\n\n\nnodes (string[]) => IP addresses and ports of the nodes, to which subconnections may be established\n\n\n\n\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n             \n\"numOpenConnections\"\n:\n \n<number>\n,\n\n             \n\"token\"\n:\n \n<number>\n,\n\n             \n\"nodes\"\n:\n \n[\n\n                     \n<string>\n\n             \n]\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nGetResultSetHeader: Gets a result set header\n\u00b6\n\n\nThis command retrieves a header (i.e., empty result set) which contains\nthe metadata for an open result set.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"getResultSetHeader\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\nresultSetHandles (number[]) => array of open result set handles\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"getResultSetHeader\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n\"resultSetHandles\"\n:\n \n[\n\n         \n<number>\n\n     \n]\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nattributes (object, optional) => attributes set for the connection (see below)\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nnumResults (number) => number of result objects\n\n\nresults (object[]) => array of result objects\n\n\nresultType (string) => type of result: \"resultSet\"\n\n\nresultSet (object) => result set\n\n\nresultSetHandle (number, optional) => result set handle\n\n\nnumColumns (number) => number of columns in the result set\n\n\nnumRows (number) => number of rows in the result set\n\n\nnumRowsInMessage (number) => number of rows in the current message\n\n\ncolumns (object[]) => array of column metadata objects\n\n\nname (string) => column name\n\n\ndataType (object) => column metadata\n\n\ntype (string) => column data type\n\n\nprecision (number, optional) => column precision\n\n\nscale (number, optional) => column scale\n\n\nsize (number, optional) => maximum size in bytes of a column value\n\n\ncharacterSet (string, optional) => character encoding of a text column\n\n\nwithLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone\n\n\nfraction (number, optional) => fractional part of number\n\n\nsrid (number, optional) => spatial reference system identifier\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n\"attributes\"\n:\n \n{\n\n         \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n         \n\"numResults\"\n:\n \n<number>\n,\n\n         \n\"results\"\n:\n \n[\n \n{\n\n             \n\"resultType\"\n:\n \n\"resultSet\"\n,\n\n             \n\"resultSet\"\n:\n \n{\n\n                 \n\"resultSetHandle\"\n:\n \n<number>\n,\n\n                 \n\"numColumns\"\n:\n \n<number>\n,\n\n                 \n\"numRows\"\n:\n \n<number>\n,\n\n                 \n\"numRowsInMessage\"\n:\n \n<number>\n,\n\n                 \n\"columns\"\n:\n \n[\n \n{\n\n                     \n\"name\"\n:\n \n<string>\n,\n\n                     \n\"dataType\"\n:\n \n{\n\n                         \n\"type\"\n:\n \n<string>\n,\n\n                         \n\"precision\"\n:\n \n<number>\n,\n\n                         \n\"scale\"\n:\n \n<number>\n,\n\n                         \n\"size\"\n:\n \n<number>\n,\n\n                         \n\"characterSet\"\n:\n \n<number>\n,\n\n                         \n\"withLocalTimeZone\"\n:\n \n<\ntrue\n \n|\n \nfalse\n>\n,\n\n                         \n\"fraction\"\n:\n \n<number>\n,\n\n                         \n\"srid\"\n:\n \n<number>\n\n                     \n}\n\n                 \n}\n \n]\n\n             \n}\n\n         \n}\n \n]\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n         \n\"text\"\n:\n \n<string>\n,\n\n         \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nGetOffset: Gets the row offset of a result set\n\u00b6\n\n\nThis command retrieves the row offset of the result set of this\n(sub)connection. This is the row number of the first row of the current\n(sub)connection's result set in the global result set.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"getOffset\"\n\n\nattributes (object, optional) => attributes to set for the connection (see below)\n\n\nresultSetHandle (number) => open result set handle\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"getOffset\"\n,\n\n     \n\"attributes\"\n:\n \n{\n\n             \n//\n \nas\n \ndefined\n \nseparately\n\n     \n},\n\n     \n\"resultSetHandle\"\n:\n \n<number>\n\n \n}\n\n\n\n\nResponse fields:\n\n\n\n\nstatus (string) => command status: \"ok\" or \"error\"\n\n\nresponseData (object, optional) => only present if status is \"ok\"\n\n\nrowOffset (number) => row offset of connection's result set\n\n\n\n\n\n\nexception (object, optional) =>  only present if status is \"error\"\n\n\ntext (string) => exception message which provides error details\n\n\nsqlCode (string) => five-character exception code if known, otherwise \"00000\"\n\n\n\n\n\n\n\n\nResponse JSON format\n\n \n{\n\n     \n\"status\"\n:\n \n<\n\"ok\"\n \n|\n \n\"error\"\n>\n,\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"ok\"\n\n     \n\"responseData\"\n:\n \n{\n\n             \n\"rowOffset\"\n:\n \n<number>\n\n     \n},\n\n     \n//\n \nin\n \ncase\n \nof\n \n\"error\"\n\n     \n\"exception\"\n:\n \n{\n\n             \n\"text\"\n:\n \n<string>\n,\n\n             \n\"sqlCode\"\n:\n \n<string>\n\n     \n}\n\n \n}\n\n\n\n\nAbortQuery: Aborts a running query\n\u00b6\n\n\nThis command aborts a running query. It does not have a response.\n\n\nRequest fields:\n\n\n\n\ncommand (string) => command name: \"abortQuery\"\n\n\n\n\nRequest JSON format\n\n \n{\n\n     \n\"command\"\n:\n \n\"abortQuery\"\n\n \n}\n\n\n\n\nAttributes\n\u00b6\n\n\nAttributes: Session and database properties\n\u00b6\n\n\nAttributes can be queried with the GetAttributes command and some of\nthem can be modified with the SetAttributes command. Modified\nattributes are included in command replies.\n\n\n\n\n\n\n\n\nName\n\n\nJSON value\n\n\nRead-only\n\n\nCommittable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nautocommit\n\n\ntrue | false\n\n\nno\n\n\nno\n\n\nIf true, commit() will be executed automatically after each statement. If false, commit() and rollback() must be executed manually.\n\n\n\n\n\n\ncompressionEnabled\n\n\ntrue | false\n\n\nyes\n\n\nno\n\n\nIf true, the WebSocket data frame payload data is compressed. If false, it is not compressed.\n\n\n\n\n\n\ncurrentSchema\n\n\nstring\n\n\nno\n\n\nyes\n\n\nCurrent schema name\n\n\n\n\n\n\ndateFormat\n\n\nstring\n\n\nyes\n\n\nyes\n\n\nDate format\n\n\n\n\n\n\ndateLanguage\n\n\nstring\n\n\nyes\n\n\nyes\n\n\nLanguage used for the day and month of dates.\n\n\n\n\n\n\ndatetimeFormat\n\n\nstring\n\n\nyes\n\n\nyes\n\n\nTimestamp format\n\n\n\n\n\n\ndefaultLikeEscapeCharacter\n\n\nstring\n\n\nyes\n\n\nyes\n\n\nEscape character in LIKE expressions.\n\n\n\n\n\n\nfeedbackInterval\n\n\nnumber\n\n\nno\n\n\nno\n\n\nTime interval (in seconds) specifying how often heartbeat/feedback packets are sent to the client during query execution.\n\n\n\n\n\n\nnumericCharacters\n\n\nstring\n\n\nno\n\n\nyes\n\n\nCharacters specifying the group and decimal separators (NLS_NUMERIC_CHARACTERS). For example, \",.\" would result in \"123,456,789.123\".\n\n\n\n\n\n\nopenTransaction\n\n\ntrue | false\n\n\nyes\n\n\nno\n\n\nIf true, a transaction is open. If false, a transaction is not open.\n\n\n\n\n\n\nqueryTimeout\n\n\nnumber\n\n\nno\n\n\nyes\n\n\nQuery timeout value (in seconds). If a query runs longer than the specified time, it will be aborted.\n\n\n\n\n\n\nsnapshotTransactionsEnabled\n\n\ntrue | false\n\n\nno\n\n\nno\n\n\nIf true, snapshot transactions will be used. If false, they will not be used.\n\n\n\n\n\n\ntimestampUtcEnabled\n\n\ntrue | false\n\n\nno\n\n\nno\n\n\nIf true, timestamps will be converted to UTC. If false, UTC will not be used.\n\n\n\n\n\n\ntimezone\n\n\nstring\n\n\nyes\n\n\nyes\n\n\nTimezone of the session.\n\n\n\n\n\n\ntimeZoneBehavior\n\n\nstring\n\n\nyes\n\n\nyes\n\n\nSpecifies the conversion behavior of UTC timestamps to local timestamps when the time value occurs during a time shift because of daylight saving time (TIME_ZONE_BEHAVIOR).\n\n\n\n\n\n\n\n\nAttributes are specified as an object of name/value pairs. Multiple attributes are separated by a comma.\n\n\nAttribute JSON format\n\n \n{\n\n     \n//\n \nname:\n \nvalue\n\n     \n<string>:\n \n<string\n \n|\n \nnumber\n \n|\n \ntrue\n \n|\n \nfalse>\n\n \n}\n\n\n\n\nData Types\n\u00b6\n\n\nData Types: Type names and properties\n\u00b6\n\n\nThe following data types and properties can be used to specify column\ntypes in the executePreparedStatement request.\n\n\n\n\n\n\n\n\nType\n\n\nRequired Properties\n\n\nOptional Properties\n\n\n\n\n\n\n\n\n\n\nBOOLEAN\n\n\n\n\n\n\n\n\n\n\nCHAR\n\n\nsize\n\n\n\n\n\n\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\nDECIMAL\n\n\nprecision, scale\n\n\n\n\n\n\n\n\nDOUBLE\n\n\n\n\n\n\n\n\n\n\nGEOMETRY\n\n\n\n\n\n\n\n\n\n\nINTERVAL DAY TO SECOND\n\n\nprecision, fraction\n\n\n\n\n\n\n\n\nINTERVAL YEAR TO MONTH\n\n\nprecision\n\n\n\n\n\n\n\n\nTIMESTAMP\n\n\n\n\nwithLocalTimeZone\n\n\n\n\n\n\nTIMESTAMP WITH LOCAL TIME ZONE\n\n\n\n\nwithLocalTimeZone\n\n\n\n\n\n\nVARCHAR\n\n\nsize\n\n\n\n\n\n\n\n\n\n\nThe following data types and properties are used to specify column\ntypes in responses from EXASOL.\n\n\n\n\n\n\n\n\nType\n\n\nProperties\n\n\n\n\n\n\n\n\n\n\nBOOLEAN\n\n\n\n\n\n\n\n\nCHAR\n\n\nsize, characterSet\n\n\n\n\n\n\nDATE\n\n\nsize\n\n\n\n\n\n\nDECIMAL\n\n\nprecision, scale\n\n\n\n\n\n\nDOUBLE\n\n\n\n\n\n\n\n\nGEOMETRY\n\n\nsize, srid\n\n\n\n\n\n\nINTERVAL DAY TO SECOND\n\n\nsize, precision, fraction\n\n\n\n\n\n\nINTERVAL YEAR TO MONTH\n\n\nsize, precision\n\n\n\n\n\n\nTIMESTAMP\n\n\nsize, withLocalTimeZone\n\n\n\n\n\n\nTIMESTAMP WITH LOCAL TIME ZONE\n\n\nsize, withLocalTimeZone\n\n\n\n\n\n\nVARCHAR\n\n\nsize, characterSet\n\n\n\n\n\n\n\n\nCompression\n\u00b6\n\n\nThe data in the WebSocket data frames may be compressed using zlib. In\norder to enable compression, the client must set the useCompression\nfield in the login command to true. If compression is enabled during\nlogin, all messages sent and received after login completion must be\nbinary data frames, in which the payload data (i.e., command\nrequest/response) is zlib-compressed.\n\n\nHeartbeat/Feedback Messages\n\u00b6\n\n\nThe feedbackInterval session attribute specifies how often (in seconds)\nunidirectional heartbeat/feedback messages are sent to the client\nduring query execution. These messages are sent using Pong WebSocket\ncontrol frames (see RFC 6455), and thus a response is not expected.\n\n\nThe client may send Ping WebSocket control frames (see RFC 6455) to\nEXASOL, for example, as client-initiated keepalives. EXASOL\nwill respond to a Ping frame with a Pong response.\n\n\nEXASOL will not send Ping frames to the client.",
            "title": "Exasol JSON over WebSockets API"
        },
        {
            "location": "/methods/exasol-websockets/#exasol-json-over-websockets-api",
            "text": "",
            "title": "Exasol JSON over WebSockets API"
        },
        {
            "location": "/methods/exasol-websockets/#why-an-websockets-api",
            "text": "The JSON over WebSockets client-server protocol allows customers to \nimplement their own drivers for all kinds of platforms using a \nconnection-based web protocol.   The main advantages are flexibility regarding the programming languages \nyou want to integrate EXASOL into, and a more native access compared to \nthe standardized ways of communicating with a database, such as JDBC, \nODBC or ADO.NET, which are mostly old and static standards and create\nadditional complexity due to the necessary driver managers.",
            "title": "Why an WebSockets API?"
        },
        {
            "location": "/methods/exasol-websockets/#clients-support",
            "text": "Currently a native Python driver using this WebSocket API has been\nimplemented. By that you don't need any pyodbc bridge anymore, but \ncan connect your Python directly with EXASOL. PyODBC is not ideal due\nto the need for an ODBC driver manager and certain restrictions in \ndata type conversions.  Further languages will be added in the future, and we encourage you\nto provide us feedback what languages you are interested in, and \nmaybe you are even keen to support our community with own developments. \nIt would then be nice if you could share your work with us, and \nwe will of course help you by any means.",
            "title": "Clients support"
        },
        {
            "location": "/methods/exasol-websockets/#websocket-protocol-v1",
            "text": "WebSocket Protocol v1 requires an EXASOL client/server protocol of\nat least v14. It follows the standards IETF as RFC 6455.  The connection server identifies the initial GET request by the client.\nThis request contains information about the used protocol version.\nDepending on this information the matching login and protocol class is\nchosen.  After the handshake the process is identical to a connection using the\nstandard drivers like JDBC or ODBC: The connection server listens to\nincoming messages and forwards the requests to the database.",
            "title": "WebSocket Protocol v1"
        },
        {
            "location": "/methods/exasol-websockets/#login-establishes-a-connection-to-exasol",
            "text": "This command invokes the login process which establishes a connection\nbetween the client and EXASOL. As long as the connection is open,\nthe user can interact with EXASOL using the commands specified\nbelow.  The login process is composed of four steps:    The client sends the login command including the requested protocol \n   version.  Request fields:   command (string) => command name: \"login\"  protocolVersion (number) => requested WebSocket protocol version, (e.g., 1)   Request JSON format\n    { \n   \"command\" :   \"login\" , \n   \"protocolVersion\" :   <number>  }     The server returns a public key which is used to encode the\n   user's password. The public key can be obtained in one of two ways:\n    a. importing the key using the publicKeyPem field\n    b. constructing the key using the publicKeyModulus and publicKeyExponent fields  Response fields:   status (string) => command status: \"ok\" or \"error\"  responseData (object, optional) => only present if status is \"ok\"  publicKeyPem (string) => PEM-formatted, 1024-bit RSA public key used to encode the user's password (see 3.)  publicKeyModulus (string) => hexadecimal modulus of the 1024-bit RSA public key used to encode the user's password (see 3.)  publicKeyExponent (string) => hexadecimal exponent of the 1024-bit RSA public key used to encode the user's password (see 3.)  exception (object, optional) => only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"   Response JSON format\n      { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      //   if   status   is   \"ok\" \n      \"responseData\" :   { \n              \"publicKeyPem\" :   <string> , \n              \"publicKeyModulus\" :   <string> , \n              \"publicKeyExponent\" :   <string> \n  }, \n      //   if   status   is   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      } \n  }     The client sends the username, encrypted password, and optionally\n   other client information.  Request fields:   username (string) => EXASOL user name to use for the login process  password (string) => user's password, which is encrypted using publicKey (see 2.) and PKCS #1 v1.5 padding, encoded in Base64 format  useCompression (boolean) => use compression for messages during the session (beginning after the login process is completed)  sessionId (number, optional) => requested session ID  clientName (string, optional) => client program name, (e.g., \"EXAplus\")  driverName (string, optional) => driver name, (e.g., \"EXA Python\")  clientOs (string, optional) => name and version of the client operating system  clientOsUsername (string, optional) => client's operating system user name  clientLanguage (string, optional) => language setting of the client system  clientVersion (string, optional) => client version number  clientRuntime (string, optional) => name and version of the client runtime  attributes (object, optional) => array of attributes to set for the connection (see below)   Request JSON format\n      { \n      \"username\" :   <string> , \n      \"password\" :   <string> , \n      \"useCompression\" :   <boolean> , \n      \"sessionId\" :   <number> , \n      \"clientName\" :   <string> , \n      \"driverName\" :   <string> , \n      \"clientOs\" :   <string> , \n      \"clientOsUsername\" :   <string> , \n      \"clientLanguage\" :   <string> , \n      \"clientVersion\" :   <string> , \n      \"clientRuntime\" :   <string> , \n      \"attributes\" :   { \n              //   as   defined   separately \n      } \n  }     The server uses username and password (see 3.) to authenticate the\n   user. If successful, the server replies with an \"ok\" response and a\n   connection is established. If authentication of the user fails, the\n   server sends an \"error\" response to the client indicating that the login\n   process failed and a connection couldn't be established.  Response fields:   status (string) => command status: \"ok\" or \"error\"  responseData (object, optional) => only present if status is \"ok\"  sessionId (number) => current session ID  protocolVersion (number) => protocol version of the connection (e.g., 14)  releaseVersion (string) => EXASOL version (e.g. \"6.0.0\")  databaseName (string) => database name (e.g., \"productionDB1\")  productName (string) => EXASOL product name: \"EXASolution\"  maxDataMessageSize (number) => maximum size of a data message in bytes  maxIdentifierLength (number) => maximum length of identifiers  maxVarcharLength (number) =>  maximum length of VARCHAR values  identifierQuoteString (string) => value of the identifier quote string (e.g., \"'\")  timeZone (string) => name of the session time zone  timeZoneBehavior (string) => value of the session option \"TIME_ZONE_BEHAVIOR\"  exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"   Response JSON format\n      { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      //   if   status   is   \"ok\" \n      \"responseData\" :   { \n              \"sessionId\" :   <number> , \n              \"protocolVersion\" :   <number> , \n              \"releaseVersion\" :   <string> , \n              \"databaseName\" :   <string> , \n              \"productName\" :   <string> , \n              \"maxDataMessageSize\" :   <number> , \n              \"maxIdentifierLength\" :   <number> , \n              \"maxVarcharLength\" :   <number> , \n              \"identifierQuoteString\" :   <string> , \n              \"timeZone\" :   <string> , \n              \"timeZoneBehavior\" :   <string> \n      }, \n      //   if   status   is   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      } \n  }",
            "title": "Login: Establishes a connection to EXASOL"
        },
        {
            "location": "/methods/exasol-websockets/#sublogin-establishes-a-subconnection-to-exasol",
            "text": "This command invokes the login process, which establishes a\nsubconnection between the client and EXASOL. Using subconnections,\nthe user can interact with EXASOL in parallel using the commands\nspecified below.  The login process is composed of four steps:    The client sends the login command including the requested protocol\n   version.  Request fields:   command (string) => command name: \"login\"  protocolVersion (number) => requested WebSocket protocol version, (e.g., 1)   Request JSON format\n    { \n      \"command\" :   \"subLogin\" , \n      \"protocolVersion\" :   <number>  }     The server returns a public key which is used to encode the\n   user's password. The public key can be obtained in one of two ways:\n    a. importing the key using the publicKeyPem field\n    b. constructing the key using the publicKeyModulus and publicKeyExponent fields.  Response fields:   status (string) => command status: \"ok\" or \"error\"  responseData (object, optional) => only present if status is \"ok\"  publicKeyPem (string) => PEM-formatted, 1024-bit RSA public key used to encode the user's password (see 3.)  publicKeyModulus (string) => hexadecimal modulus of the 1024-bit RSA public key used to encode the user's password (see 3.)  publicKeyExponent (string) => hexadecimal exponent of the 1024-bit RSA public key used to encode the user's password (see 3.)  exception (object, optional) => only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"   Response JSON format\n    { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      //   if   status   is   \"ok\" \n      \"responseData\" :   { \n              \"publicKeyPem\" :   <string> , \n              \"publicKeyModulus\" :   <string> , \n              \"publicKeyExponent\" :   <string> \n      }, \n      //   if   status   is   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      }  }     The client sends the username, encrypted password, and token.  Request fields:   username (string) => EXASOL user name to use for the login process  password (string) => user's password, which is encrypted using publicKey (see 2.) and PKCS #1 v1.5 padding, encoded in Base64 format  token (number) => token required for subconnection logins (see, EnterParallel)   Request JSON format\n    { \n      \"username\" :   <string> , \n      \"password\" :   <string> , \n      \"token\" :   <number>  }     The server uses username, password, and token (see 3.) to\n   authenticate the user. If successful, the server replies with an\n   \"ok\" response and a subconnection is established. If authentication of\n   the user fails, the server sends an \"error\" response to the client\n   indicating that the login process failed and a subconnection couldn't\n   be established.  Response fields:   status (string) => command status: \"ok\" or \"error\"  responseData (object, optional) => only present if status is \"ok\"  sessionId (number) => current session ID  protocolVersion (number) => protocol version of the connection (e.g., 14)  releaseVersion (string) => EXASOL version (e.g. \"6.0.0\")  databaseName (string) => database name (e.g., \"productionDB1\")  productName (string) => EXASOL product name: \"EXASolution\"  maxDataMessageSize (number) => maximum size of a data message in bytes  maxIdentifierLength (number) => maximum length of identifiers  maxVarcharLength (number) =>  maximum length of VARCHAR values  identifierQuoteString (string) => value of the identifier quote string (e.g., \"'\")  timeZone (string) => name of the session time zone  timeZoneBehavior (string) => value of the session option \"TIME_ZONE_BEHAVIOR\"  exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"   Response JSON format\n    { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      //   if   status   is   \"ok\" \n      \"responseData\" :   { \n              \"sessionId\" :   <number> , \n              \"protocolVersion\" :   <number> , \n              \"releaseVersion\" :   <string> , \n              \"databaseName\" :   <string> , \n              \"productName\" :   <string> , \n              \"maxDataMessageSize\" :   <number> , \n              \"maxIdentifierLength\" :   <number> , \n              \"maxVarcharLength\" :   <number> , \n              \"identifierQuoteString\" :   <string> , \n              \"timeZone\" :   <string> , \n              \"timeZoneBehavior\" :   <string> \n      }, \n      //   if   status   is   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      }  }",
            "title": "SubLogin: Establishes a subconnection to EXASOL"
        },
        {
            "location": "/methods/exasol-websockets/#commands",
            "text": "",
            "title": "Commands"
        },
        {
            "location": "/methods/exasol-websockets/#disconnect-closes-a-connection-to-exasol",
            "text": "This command closes the connection between the client and EXASOL.\nAfter the connection is closed, it cannot be used for further\ninteraction with EXASOL anymore.  Request fields:   command (string) => command name: \"disconnect\"  attributes (object, optional) => attributes to set for the connection (see below)   Request JSON format { \n      \"command\" :   \"disconnect\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  attributes (object, optional) => attributes set for the connection (see below)  exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      //   if   status   is   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      }  }",
            "title": "Disconnect: Closes a connection to EXASOL"
        },
        {
            "location": "/methods/exasol-websockets/#getattributes-gets-the-session-attribute-values",
            "text": "This command retrieves the session attribute values.  Request fields:   command (string) => command name: \"getAttributes\"  attributes (object, optional) => attributes to set for the connection (see below)   JSON format   { \n      \"command\" :   \"getAttributes\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  attributes (object, optional) => attributes set for the connection (see below)  exception(object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Reponse JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      //   if   status   is   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      } \n  }",
            "title": "GetAttributes: Gets the session attribute values"
        },
        {
            "location": "/methods/exasol-websockets/#setattributes-sets-the-given-session-attribute-values",
            "text": "This command sets the specified session attribute values.  Request fields:   command (string) => command name: \"getAttributes\"  attributes (object, optional) =>  attributes to set for the connection (see below)   Request JSON format { \n   \"command\" :   \"setAttributes\" , \n   \"attributes\" :   { \n     //   as   defined   separately \n   }  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  attributes (object, optional) =>  attributes set for the connection (see below)  exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      //   if   status   is   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      } \n  }",
            "title": "SetAttributes: Sets the given session attribute values"
        },
        {
            "location": "/methods/exasol-websockets/#createpreparedstatement-creates-a-prepared-statement",
            "text": "This command creates a prepared statement.  Request fields:   command (string) => command name: \"createPreparedStatement\"  attributes (object, optional) => attributes to set for the connection (see below)  sqlText (string) => SQL statement   Request JSON format   { \n      \"command\" :   \"createPreparedStatement\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      \"sqlText\" :   <string> \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  attributes (object, optional) => attributes set for the connection (see below)  responseData (object, optional) => only present if status is \"ok\"  statementHandle (number) => prepared statement handle  parameterData (object) => prepared statement parameter information  numColumns (number) => number of columns  columns (object[]) => array of column metadata objects  name (string) => column name: always \"\" as named parameters are not supported  dataType (object) => column metadata  type (string) => column data type  precision (number, optional) => column precision  scale (number, optional) => column scale  size (number, optional) => maximum size in bytes of a column value  characterSet (string, optional) => character encoding of a text column  withLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone  fraction (number, optional) => fractional part of number  srid (number, optional) => spatial reference system identifier    numResults (number) => number of result objects  results (object[]) => array of result objects  resultType (string) => type of result: \"resultSet\" or \"rowCount\"  rowCount (number, optional) => present if resultType is \"rowCount\", number of rows  resultSet (object, optional) => present if resultType is \"resultSet\", result set  resultSetHandle (number, optional) => result set handle  numColumns (number) => number of columns in the result set  numRows (number) => number of rows in the result set  numRowsInMessage (number) => number of rows in the current message  columns (object[]) => array of column metadata objects  name (string) => column name  dataType (object) => column metadata  type (string) => column data type  precision (number, optional) => column precision  scale (number, optional) => column scale  size (number, optional) => maximum size in bytes of a column value  characterSet (string, optional) => character encoding of a text column  withLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone  fraction (number, optional) => fractional part of number  srid (number, optional) => spatial reference system identifier        exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      \"attributes\" :   { \n          //   as   defined   separately \n      }, \n      //   if   status   is   \"ok\" \n      \"responseData\" :   { \n          \"statementHandle\" :   <number> , \n          \"parameterData\" :   { \n              \"numColumns\" :   <number> , \n              \"columns\" :   [   { \n                  \"name\" :   <string> , \n                  \"dataType\" :   { \n                      \"type\" :   <string> , \n                      \"precision\" :   <number> , \n                      \"scale\" :   <number> , \n                      \"size\" :   <number> , \n                      \"characterSet\" :   <number> , \n                      \"withLocalTimeZone\" :   < true   |   false > , \n                      \"fraction\" :   <number> , \n                      \"srid\" :   <number> \n                  } \n              }   ] \n          }, \n          \"numResults\" :   <number> , \n          \"results\" :   [   { \n              \"resultType\" :   < \"resultSet\"   |   \"rowCount\" > , \n              //   if   type   is   \"rowCount\" \n              \"rowCount\" :   <number> , \n              //   if   type   is   \"resultSet\" \n              \"resultSet\" :   { \n                  \"resultSetHandle\" :   <number> , \n                  \"numColumns\" :   <number> , \n                  \"numRows\" :   <number> , \n                  \"numRowsInMessage\" :   <number> , \n                  \"columns\" :   [   { \n                      \"name\" :   <string> , \n                      \"dataType\" :   { \n                          \"type\" :   <string> , \n                          \"precision\" :   <number> , \n                          \"scale\" :   <number> , \n                          \"size\" :   <number> , \n                          \"characterSet\" :   <number> , \n                          \"withLocalTimeZone\" :   < true   |   false > , \n                          \"fraction\" :   <number> , \n                          \"srid\" :   <number> \n                      } \n                  }   ] \n              } \n          }   ] \n      }, \n      //   if   status   is   \"error\" \n      \"exception\" :   { \n          \"text\" :   <string> , \n          \"sqlCode\" :   <string> \n      } \n  }",
            "title": "CreatePreparedStatement: Creates a prepared statement"
        },
        {
            "location": "/methods/exasol-websockets/#executepreparedstatement-executes-a-prepared-statement",
            "text": "This command executes a prepared statement which has already been\ncreated.  If the SQL statement returns a result set which has less than 1,000 rows of data, the data will be provided in the data field of resultSet. However if the SQL statement returns a result set which has 1,000 or more rows of data, a result set will be opened whose handle is returned in the resultSetHandle field of resultSet. Using this handle, the data from the result set can be retrieved using the Fetch command. Once the result set is no longer needed, it should be closed using the CloseResultSet command.  Request fields:   command (string) => command name: \"executePreparedStatement\"  attributes (object, optional) =>   attributes to set for the connection (see below)  statementHandle (number) => prepared statement handle  numColumns (number) => number of columns in data  numRows (number) => number of rows in data  columns (object[], optional) => array of column metadata objects  name (string) => column name  dataType (object) => column metadata  type (string) => column data type  precision (number, optional) => column precision  scale (number, optional) => column scale  size (number, optional) => maximum size in bytes of a column value  characterSet (string, optional) => character encoding of a text column  withLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone  fraction (number, optional) => fractional part of number  srid (number, optional) => spatial reference system identifier    data (array[], optional) => array containing the data for the prepared statement in column-major order   Request JSON format   { \n     \"command\" :   \"executePreparedStatement\" , \n     \"attributes\" :   { \n         //   as   defined   separately \n     }, \n     \"statementHandle\" :   <number> , \n     \"numColumns\" :   <number> , \n     \"numRows\" :   <number> , \n     \"columns\" :   [   { \n         \"name\" :   <string> , \n         \"dataType\" :   { \n             \"type\" :   <string> , \n             \"precision\" :   <number> , \n             \"scale\" :   <number> , \n             \"size\" :   <number> , \n             \"characterSet\" :   <number> , \n             \"withLocalTimeZone\" :   <boolean> , \n             \"fraction\" :   <number> , \n             \"srid\" :   <number> \n         } \n     }   ], \n     \"data\" :   [ \n         [ \n             <string   |   number   |   true   |   false   |   null > \n         ] \n     ] \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  attributes (object, optional) => attributes set for the connection (see below)  responseData (object, optional) => only present if status is \"ok\"  numResults (number) => number of result objects  results (object[]) => array of result objects  resultType (string) => type of result: \"resultSet\" or \"rowCount\"  rowCount (number, optional) => present if resultType is \"rowCount\", number of rows  resultSet (object, optional) => present if resultType is \"resultSet\", result set  resultSetHandle (number, optional) => result set handle  numColumns (number) => number of columns in the result set  numRows (number) => number of rows in the result set  numRowsInMessage (number) => number of rows in the current message  columns (object[]) => array of column metadata objects  name (string) => column name  dataType (object) => column metadata  type (string) => column data type  precision (number, optional) => column precision  scale (number, optional) => column scale  size (number, optional) => maximum size in bytes of a column value  characterSet (string, optional) => character encoding of a text column  withLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone  fraction (number, optional) => fractional part of number  srid (number, optional) => spatial reference system identifier    data (array[], optional) => object containing the data for the prepared statement in column-major order      exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      \"attributes\" :   { \n          // as defined separately \n      }, \n      // in case of \"ok\" \n      \"responseData\" :   { \n          \"numResults\" :   < number > , \n          \"results\" :   [   { \n              \"resultType\" :   < \"resultSet\"   |   \"rowCount\" > , \n              // if type is \"rowCount\" \n              \"rowCount\" :   < number > , \n              // if type is \"resultSet\" \n              \"resultSet\" :   { \n                  \"resultSetHandle\" :   < number > , \n                  \"numColumns\" :   < number > , \n                  \"numRows\" :   < number > , \n                  \"numRowsInMessage\" :   < number > , \n                  \"columns\" :   [   { \n                      \"name\" :   < string > , \n                      \"dataType\" :   { \n                          \"type\" :   < string > , \n                          \"precision\" :   < number > , \n                          \"scale\" :   < number > , \n                          \"size\" :   < number > , \n                          \"characterSet\" :   < number > , \n                          \"withLocalTimeZone\" :   < true   |   false > , \n                          \"fraction\" :   < number > , \n                          \"srid\" :   < number > \n                      } \n                  }   ], \n                  \"data\" :   [ \n                      [ \n                          < string   |   number   |   true   |   false   |   null > \n                      ] \n                  ] \n              } \n          }   ] \n      }, \n      // in case of \"error\" \n      \"exception\" :   { \n          \"text\" :   < string > , \n          \"sqlCode\" :   < string > \n      } \n  }",
            "title": "ExecutePreparedStatement: Executes a prepared statement"
        },
        {
            "location": "/methods/exasol-websockets/#closepreparedstatement-closes-a-prepared-statement",
            "text": "This command closes a prepared statement which has already been\ncreated.  Request fields:   command (string) => command name: \"closePreparedStatement\"  attributes (object, optional) => attributes to set for the connection (see below)  statementHandle (number) => prepared statement handle   Request JSON format { \n     \"command\" :   \"closePreparedStatement\" , \n     \"attributes\" :   { \n       //   as   defined   separately \n     }, \n     \"statementHandle\" :   <number>  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  attributes (object) => attributes set for the connection (see below)  exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      //   in   case   of   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      } \n  }",
            "title": "ClosePreparedStatement: Closes a prepared statement"
        },
        {
            "location": "/methods/exasol-websockets/#execute-executes-an-sql-statement",
            "text": "This command executes an SQL statement.  If the SQL statement returns a result set which has less than 1,000 rows of data, the data will be provided in the data field of resultSet. However if the SQL statement returns a result set which has 1,000 or more rows of data, a result set will be opened whose handle is returned in the resultSetHandle field of resultSet. Using this handle, the data from the result set can be retrieved using the Fetch command. Once the result set is no longer needed, it should be closed using the CloseResultSet command.  Request fields:   command (string) => command name: \"executePreparedStatement\"  attributes (object) => attributes to set for the connection (see below)  sqlText (string) => SQL statement to execute   Request JSON format   { \n      \"command\" :   \"execute\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      \"sqlText\" :   <string> \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  attributes (object, optional) => attributes set for the connection (see below)  responseData (object, optional) => only present if status is \"ok\"  numResults (number) => number of result objects  results (object[]) => array of result objects  resultType (string) => type of result: \"resultSet\" or \"rowCount\"  rowCount (number, optional) => present if resultType is \"rowCount\", number of rows  resultSet (object, optional) => present if resultType is \"resultSet\", result set  resultSetHandle (number) => result set handle  numColumns (number) => number of columns in the result set  numRows (number) => number of rows in the result set  numRowsInMessage (number, optional) => number of rows in the current message  columns (object[]) => array of column metadata objects  name (string) => column name  dataType (object) => column metadata  type (string) => column data type  precision (number, optional) => column precision  scale (number, optional) => column scale  size (number, optional) => maximum size in bytes of a column value  characterSet (string, optional) => character encoding of a text column  withLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone  fraction (number, optional) => fractional part of number  srid (number, optional) => spatial reference system identifier    data (array[], optional) => object containing the data for the prepared statement in column-major order      exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      \"attributes\" :   { \n          //   as   defined   separately \n      }, \n      //   in   case   of   \"ok\" \n      \"responseData\" :   {   //   Optional:   ok \n          \"numResults\" :   <number> , \n          \"results\" :   [   { \n              \"resultType\" :   < \"resultSet\"   |   \"rowCount\" > , \n              //   if   type   is   \"rowCount\" \n              \"rowCount\" :   <number> , \n              //   if   type   is   \"resultSet\" \n              \"resultSet\" :   { \n                  \"resultSetHandle\" :   <number> , \n                  \"numColumns\" :   <number> , \n                  \"numRows\" :   <number> , \n                  \"numRowsInMessage\" :   <number> , \n                  \"columns\" :   [   { \n                      \"name\" :   <string> , \n                      \"dataType\" :   { \n                          \"type\" :   <string> , \n                          \"precision\" :   <number> , \n                          \"scale\" :   <number> , \n                          \"size\" :   <number> , \n                          \"characterSet\" :   <number> , \n                          \"withLocalTimeZone\" :   < true   |   false > , \n                          \"fraction\" :   <number> , \n                          \"srid\" :   <number> \n                      } \n                  }   ], \n                  \"data\" :   [ \n                      [ \n                          <string   |   number   |   true   |   false   |   null > \n                      ] \n                  ] \n              } \n          }   ] \n      }, \n      //   in   case   of   \"error\" \n      \"exception\" :   { \n          \"text\" :   <string> , \n          \"sqlCode\" :   <string> \n      } \n  }",
            "title": "Execute: Executes an SQL statement"
        },
        {
            "location": "/methods/exasol-websockets/#fetch-retrieves-data-from-a-result-set",
            "text": "This command retrieves data from a result set.  Request fields:   command (string) => command name: \"fetch\"  attributes (object, optional) =>  attributes to set for the connection (see below)  resultSetHandle (number) => result set handle  startPosition (number) => row offset (0-based) from which to begin data retrieval  numBytes (number) => number of bytes to retrieve (max: 64MB)   Request JSON format   { \n      \"command\" :   \"fetch\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      \"resultSetHandle\" :   <number> , \n      \"startPosition\" :   <number> , \n      \"numBytes\" :   <number> \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  responseData (object, optional) => only present if status is \"ok\"  numRows (number) => number of rows fetched from the result set  data (array[]) => object containing the data for the prepared statement in column-major order    exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      //   in   case   of   \"ok\" \n      \"responseData\" :   { \n              \"numRows\" :   <number> , \n              \"data\" :   [ \n                  [ \n                      <string   |   number   |   true   |   false   |   null > \n                  ] \n              ] \n      }, \n      //   in   case   of   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      } \n  }",
            "title": "Fetch: Retrieves data from a result set"
        },
        {
            "location": "/methods/exasol-websockets/#closeresultset-closes-a-result-set",
            "text": "This command closes result sets.  Request fields:   command (string) => command name: \"closeResultSet\"  attributes (object, optional) => attributes to set for the connection (see below)  resultSetHandles (number[]) => array of result set handles   Request JSON format   { \n      \"command\" :   \"closeResultSet\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      \"resultSetHandles\" :   [   <number>   ] \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      //   in   case   of   \"error\" \n      \"exception\" :   {   //   Optional:   error \n              \"text\" :   <string> ,   //   Exception   text \n              \"sqlCode\" :   <string>   //   Five-character   exception   code   if   known ,   otherwise   \"00000\" \n      } \n  }",
            "title": "CloseResultSet: Closes a result set"
        },
        {
            "location": "/methods/exasol-websockets/#gethosts-gets-the-hosts-in-a-cluster",
            "text": "This command gets the number hosts and the IP address of each host in\nan EXASOL cluster.  Request fields:   command (string) => command name: \"getHosts\"  attributes (object, optional) => attributes to set for the connection (see below)   Request JSON format   { \n      \"command\" :   \"getHosts\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      } \n  }   Response fields:\n  * status (string) => command status: \"ok\" or \"error\"\n  * responseData (object, optional) => only present if status is \"ok\"\n    * numNodes (number) => number of nodes in the cluster\n    * nodes (string[]) => array of cluster node IP addresses\n  * exception (object, optional) =>  only present if status is \"error\"\n    * text (string) => exception message which provides error\n         details\n    * sqlCode (string) => five-character exception code if known,\n         otherwise \"00000\"  Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      //   in   case   of   \"ok\" \n      \"responseData\" :   { \n              \"numNodes\" :   <number> , \n              \"nodes\" :   [ \n                      <string> \n              ] \n      }, \n      //   in   case   of   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      } \n  }",
            "title": "GetHosts: Gets the hosts in a cluster"
        },
        {
            "location": "/methods/exasol-websockets/#executebatch-executes-multiple-sql-statements-as-a-batch",
            "text": "This command executes multiple SQL statements sequentially as a batch.  If the SQL statement returns a result set which has less than 1,000 rows of data, the data will be provided in the data field of resultSet. However if the SQL statement returns a result set which has 1,000 or more rows of data, a result set will be opened whose handle is returned in the resultSetHandle field of resultSet. Using this handle, the data from the result set can be retrieved using the Fetch command. Once the result set is no longer needed, it should be closed using the CloseResultSet command.  Request fields:   command (string) => command name: \"executeBatch\"  attributes (object, optional) => attributes to set for the connection (see below)  sqlTexts (string[]) => array of SQL statement to execute   Request JSON format   { \n      \"command\" :   \"executeBatch\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      \"sqlTexts\" :   [ \n              <string> \n      ] \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  attributes (object, optional) => attributes set for the connection (see below)  responseData (object, optional) => only present if status is \"ok\"  numResults (number) => number of result objects  results (object[]) => array of result objects  resultType (string) => type of result: \"resultSet\" or \"rowCount\"  rowCount (number, optional) => present if resultType is \"rowCount\", number of rows  resultSet (object, optional) => present if resultType is \"resultSet\", result set  resultSetHandle (number, optional) => result set handle  numColumns (number) => number of columns in the result set  numRows (number) => number of rows in the result set  numRowsInMessage (number) => number of rows in the current message  columns (object[]) => array of column metadata objects  name (string) => column name  dataType (object) => column metadata  type (string) => column data type  precision (number, optional) => column precision  scale (number, optional) => column scale  size (number, optional) => maximum size in bytes of a column value  characterSet (string, optional) => character encoding of a text column  withLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone  fraction (number, optional) => fractional part of number  srid (number, optional) => spatial reference system identifier    data (array[], optional) => object containing the data for the prepared statement in column-major order      exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      \"attributes\" :   { \n          //   as   defined   separately \n      }, \n      //   in   case   of   \"ok\" \n      \"responseData\" :   { \n          \"numResults\" :   <number> , \n          \"results\" :   [   { \n              \"resultType\" :   < \"resultSet\"   |   \"rowCount\" > , \n              //   if   type   is   \"rowCount\" \n              \"rowCount\" :   <number> , \n              //   if   type   is   \"resultSet\" \n              \"resultSet\" :   { \n                  \"resultSetHandle\" :   <number> , \n                  \"numColumns\" :   <number> , \n                  \"numRows\" :   <number> , \n                  \"numRowsInMessage\" :   <number> , \n                  \"columns\" :   [   { \n                      \"name\" :   <string> , \n                      \"dataType\" :   { \n                          \"type\" :   <string> , \n                          \"precision\" :   <number> , \n                          \"scale\" :   <number> , \n                          \"size\" :   <number> , \n                          \"characterSet\" :   <number> , \n                          \"withLocalTimeZone\" :   < true   |   false > , \n                          \"fraction\" :   <number> , \n                          \"srid\" :   <number> \n                      } \n                  }   ], \n                  \"data\" :   [ \n                      [ \n                          <string   |   number   |   true   |   false   |   null > \n                      ] \n                  ] \n              } \n          }   ] \n      }, \n      //   in   case   of   \"error\" \n      \"exception\" :   { \n          \"text\" :   <string> , \n          \"sqlCode\" :   <string> \n      } \n  }",
            "title": "ExecuteBatch: Executes multiple SQL statements as a batch"
        },
        {
            "location": "/methods/exasol-websockets/#enterparallel-opens-subconnections-for-parallel-execution",
            "text": "This command opens subconnections, which are additional connections to\nother nodes in the cluster, for the purpose of parallel execution. If\nthe requested number of subconnections is 0, all open subconnections\nare closed.  Request fields:   command (string) => command name: \"enterParallel\"  attributes (object, optional) => attributes to set for the connection (see below)  numRequestedConnections (number) => number of subconnections to open. If 0, all open subconnections are closed.   Request JSON format   { \n      \"command\" :   \"enterParallel\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      \"numRequestedConnections\" :   <number> \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  responseData (object, optional) => only present if status is \"ok\"  numOpenConnections (number) => number of subconnections actually opened  token (number) => token required for the login of subconnections  nodes (string[]) => IP addresses and ports of the nodes, to which subconnections may be established    exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      //   in   case   of   \"ok\" \n      \"responseData\" :   { \n              \"numOpenConnections\" :   <number> , \n              \"token\" :   <number> , \n              \"nodes\" :   [ \n                      <string> \n              ] \n      }, \n      //   in   case   of   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      } \n  }",
            "title": "EnterParallel: Opens subconnections for parallel execution"
        },
        {
            "location": "/methods/exasol-websockets/#getresultsetheader-gets-a-result-set-header",
            "text": "This command retrieves a header (i.e., empty result set) which contains\nthe metadata for an open result set.  Request fields:   command (string) => command name: \"getResultSetHeader\"  attributes (object, optional) => attributes to set for the connection (see below)  resultSetHandles (number[]) => array of open result set handles   Request JSON format   { \n      \"command\" :   \"getResultSetHeader\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      \"resultSetHandles\" :   [ \n          <number> \n      ] \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  attributes (object, optional) => attributes set for the connection (see below)  responseData (object, optional) => only present if status is \"ok\"  numResults (number) => number of result objects  results (object[]) => array of result objects  resultType (string) => type of result: \"resultSet\"  resultSet (object) => result set  resultSetHandle (number, optional) => result set handle  numColumns (number) => number of columns in the result set  numRows (number) => number of rows in the result set  numRowsInMessage (number) => number of rows in the current message  columns (object[]) => array of column metadata objects  name (string) => column name  dataType (object) => column metadata  type (string) => column data type  precision (number, optional) => column precision  scale (number, optional) => column scale  size (number, optional) => maximum size in bytes of a column value  characterSet (string, optional) => character encoding of a text column  withLocalTimeZone (true | false, optional) => specifies if a timestamp has a local time zone  fraction (number, optional) => fractional part of number  srid (number, optional) => spatial reference system identifier        exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      \"attributes\" :   { \n          //   as   defined   separately \n      }, \n      //   in   case   of   \"ok\" \n      \"responseData\" :   { \n          \"numResults\" :   <number> , \n          \"results\" :   [   { \n              \"resultType\" :   \"resultSet\" , \n              \"resultSet\" :   { \n                  \"resultSetHandle\" :   <number> , \n                  \"numColumns\" :   <number> , \n                  \"numRows\" :   <number> , \n                  \"numRowsInMessage\" :   <number> , \n                  \"columns\" :   [   { \n                      \"name\" :   <string> , \n                      \"dataType\" :   { \n                          \"type\" :   <string> , \n                          \"precision\" :   <number> , \n                          \"scale\" :   <number> , \n                          \"size\" :   <number> , \n                          \"characterSet\" :   <number> , \n                          \"withLocalTimeZone\" :   < true   |   false > , \n                          \"fraction\" :   <number> , \n                          \"srid\" :   <number> \n                      } \n                  }   ] \n              } \n          }   ] \n      }, \n      //   in   case   of   \"error\" \n      \"exception\" :   { \n          \"text\" :   <string> , \n          \"sqlCode\" :   <string> \n      } \n  }",
            "title": "GetResultSetHeader: Gets a result set header"
        },
        {
            "location": "/methods/exasol-websockets/#getoffset-gets-the-row-offset-of-a-result-set",
            "text": "This command retrieves the row offset of the result set of this\n(sub)connection. This is the row number of the first row of the current\n(sub)connection's result set in the global result set.  Request fields:   command (string) => command name: \"getOffset\"  attributes (object, optional) => attributes to set for the connection (see below)  resultSetHandle (number) => open result set handle   Request JSON format   { \n      \"command\" :   \"getOffset\" , \n      \"attributes\" :   { \n              //   as   defined   separately \n      }, \n      \"resultSetHandle\" :   <number> \n  }   Response fields:   status (string) => command status: \"ok\" or \"error\"  responseData (object, optional) => only present if status is \"ok\"  rowOffset (number) => row offset of connection's result set    exception (object, optional) =>  only present if status is \"error\"  text (string) => exception message which provides error details  sqlCode (string) => five-character exception code if known, otherwise \"00000\"     Response JSON format   { \n      \"status\" :   < \"ok\"   |   \"error\" > , \n      //   in   case   of   \"ok\" \n      \"responseData\" :   { \n              \"rowOffset\" :   <number> \n      }, \n      //   in   case   of   \"error\" \n      \"exception\" :   { \n              \"text\" :   <string> , \n              \"sqlCode\" :   <string> \n      } \n  }",
            "title": "GetOffset: Gets the row offset of a result set"
        },
        {
            "location": "/methods/exasol-websockets/#abortquery-aborts-a-running-query",
            "text": "This command aborts a running query. It does not have a response.  Request fields:   command (string) => command name: \"abortQuery\"   Request JSON format   { \n      \"command\" :   \"abortQuery\" \n  }",
            "title": "AbortQuery: Aborts a running query"
        },
        {
            "location": "/methods/exasol-websockets/#attributes",
            "text": "",
            "title": "Attributes"
        },
        {
            "location": "/methods/exasol-websockets/#attributes-session-and-database-properties",
            "text": "Attributes can be queried with the GetAttributes command and some of\nthem can be modified with the SetAttributes command. Modified\nattributes are included in command replies.     Name  JSON value  Read-only  Committable  Description      autocommit  true | false  no  no  If true, commit() will be executed automatically after each statement. If false, commit() and rollback() must be executed manually.    compressionEnabled  true | false  yes  no  If true, the WebSocket data frame payload data is compressed. If false, it is not compressed.    currentSchema  string  no  yes  Current schema name    dateFormat  string  yes  yes  Date format    dateLanguage  string  yes  yes  Language used for the day and month of dates.    datetimeFormat  string  yes  yes  Timestamp format    defaultLikeEscapeCharacter  string  yes  yes  Escape character in LIKE expressions.    feedbackInterval  number  no  no  Time interval (in seconds) specifying how often heartbeat/feedback packets are sent to the client during query execution.    numericCharacters  string  no  yes  Characters specifying the group and decimal separators (NLS_NUMERIC_CHARACTERS). For example, \",.\" would result in \"123,456,789.123\".    openTransaction  true | false  yes  no  If true, a transaction is open. If false, a transaction is not open.    queryTimeout  number  no  yes  Query timeout value (in seconds). If a query runs longer than the specified time, it will be aborted.    snapshotTransactionsEnabled  true | false  no  no  If true, snapshot transactions will be used. If false, they will not be used.    timestampUtcEnabled  true | false  no  no  If true, timestamps will be converted to UTC. If false, UTC will not be used.    timezone  string  yes  yes  Timezone of the session.    timeZoneBehavior  string  yes  yes  Specifies the conversion behavior of UTC timestamps to local timestamps when the time value occurs during a time shift because of daylight saving time (TIME_ZONE_BEHAVIOR).     Attributes are specified as an object of name/value pairs. Multiple attributes are separated by a comma.  Attribute JSON format   { \n      //   name:   value \n      <string>:   <string   |   number   |   true   |   false> \n  }",
            "title": "Attributes: Session and database properties"
        },
        {
            "location": "/methods/exasol-websockets/#data-types",
            "text": "",
            "title": "Data Types"
        },
        {
            "location": "/methods/exasol-websockets/#data-types-type-names-and-properties",
            "text": "The following data types and properties can be used to specify column\ntypes in the executePreparedStatement request.     Type  Required Properties  Optional Properties      BOOLEAN      CHAR  size     DATE      DECIMAL  precision, scale     DOUBLE      GEOMETRY      INTERVAL DAY TO SECOND  precision, fraction     INTERVAL YEAR TO MONTH  precision     TIMESTAMP   withLocalTimeZone    TIMESTAMP WITH LOCAL TIME ZONE   withLocalTimeZone    VARCHAR  size      The following data types and properties are used to specify column\ntypes in responses from EXASOL.     Type  Properties      BOOLEAN     CHAR  size, characterSet    DATE  size    DECIMAL  precision, scale    DOUBLE     GEOMETRY  size, srid    INTERVAL DAY TO SECOND  size, precision, fraction    INTERVAL YEAR TO MONTH  size, precision    TIMESTAMP  size, withLocalTimeZone    TIMESTAMP WITH LOCAL TIME ZONE  size, withLocalTimeZone    VARCHAR  size, characterSet",
            "title": "Data Types: Type names and properties"
        },
        {
            "location": "/methods/exasol-websockets/#compression",
            "text": "The data in the WebSocket data frames may be compressed using zlib. In\norder to enable compression, the client must set the useCompression\nfield in the login command to true. If compression is enabled during\nlogin, all messages sent and received after login completion must be\nbinary data frames, in which the payload data (i.e., command\nrequest/response) is zlib-compressed.",
            "title": "Compression"
        },
        {
            "location": "/methods/exasol-websockets/#heartbeatfeedback-messages",
            "text": "The feedbackInterval session attribute specifies how often (in seconds)\nunidirectional heartbeat/feedback messages are sent to the client\nduring query execution. These messages are sent using Pong WebSocket\ncontrol frames (see RFC 6455), and thus a response is not expected.  The client may send Ping WebSocket control frames (see RFC 6455) to\nEXASOL, for example, as client-initiated keepalives. EXASOL\nwill respond to a Ping frame with a Pong response.  EXASOL will not send Ping frames to the client.",
            "title": "Heartbeat/Feedback Messages"
        },
        {
            "location": "/methods/focal-getis-ord-raster/",
            "text": "Foal Getis-ord on rasters\n\u00b6\n\n\nThe rasterized Focal Getis-Ord formula looks as follows:\n\n\n\n\n\nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\n\n\n\nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\n\n\n\n\nwhere:\n\n\n\n\nR\nR\n is the input raster.\n\n\nW\nW\n is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g. \n5 \\times 5\n5 \\times 5\n, \n31 \\times 31\n31 \\times 31\n ...\n\n\nN\nN\n represents the focal count of pixels TODO (there can be NA values)\n\n\nM\nM\n represents the focal mean TODO.\n\n\nS\nS\n represents the focal standard deviation TODO.\n\n\n\n\nIt can be seen that the formula can be nicely refactored into:\n\n\n\n\nTODO",
            "title": "Foal Getis-ord on rasters"
        },
        {
            "location": "/methods/focal-getis-ord-raster/#foal-getis-ord-on-rasters",
            "text": "The rasterized Focal Getis-Ord formula looks as follows:   \nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }  \nFocalG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }   where:   R R  is the input raster.  W W  is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g.  5 \\times 5 5 \\times 5 ,  31 \\times 31 31 \\times 31  ...  N N  represents the focal count of pixels TODO (there can be NA values)  M M  represents the focal mean TODO.  S S  represents the focal standard deviation TODO.   It can be seen that the formula can be nicely refactored into:   TODO",
            "title": "Foal Getis-ord on rasters"
        },
        {
            "location": "/methods/focal-getis-ord/",
            "text": "Focal Getis-ord\n\u00b6\n\n\n\n\nTodo\n\n\nJulian Bruns : definition of Focal G* using points \n\n\n\n\nThe Focal Getis-Ord \nG^*_i\nG^*_i\n statistic differs from the \nStandard Getis-ord\n ...\n\n\n\n\n\n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\n\n\n\n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\n\n\n\n\nwhere:\n\n\n\n\nTODO\n\n\nTODO",
            "title": "Focal Getis-ord"
        },
        {
            "location": "/methods/focal-getis-ord/#focal-getis-ord",
            "text": "Todo  Julian Bruns : definition of Focal G* using points    The Focal Getis-Ord  G^*_i G^*_i  statistic differs from the  Standard Getis-ord  ...   \n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }  \n    FocalG^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }   where:   TODO  TODO",
            "title": "Focal Getis-ord"
        },
        {
            "location": "/methods/getis-ord-raster/",
            "text": "Getis-ord G* on rasters\n\u00b6\n\n\nThe \nStandard Getis-ord\n is defined on individual points (vector data).\nIn many situations, we want to compute \nG^*_i\nG^*_i\n in a raster rather than in a point cloud.\nThis way, the computation can be expressed using map algebra operations and nicely parallelized in frameworks \nsuch as \nGeotrellis\n.\n\n\nThe rasterized Getis-Ord formula looks as follows:\n\n\n\n\n\nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\n\n\n\nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }\n\n\n\n\n\nwhere:\n\n\n\n\nR\nR\n is the input raster.\n\n\nW\nW\n is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g. \n5 \\times 5\n5 \\times 5\n, \n31 \\times 31\n31 \\times 31\n ...\n\n\nN\nN\n represents the number of all pixels in \nR\nR\n (because there can be NA values)\n\n\nM\nM\n represents the global mean of \nR\nR\n.\n\n\nS\nS\n represents the global standard deviation of all pixels in \nR\nR\n.\n\n\n\n\nIt can be seen that the formula can be nicely refactored into:\n\n\n\n\nOne \nglobal operation\n that computes \nN\nN\n, \nM\nM\n, \nS\nS\n. These values are usually available because they\n  were pre-computed when the raster layer has been ingested into the catalog.\n\n\nOne \nfocal operation\n \nR{\\stackrel{\\mathtt{sum}}{\\circ}}W\nR{\\stackrel{\\mathtt{sum}}{\\circ}}W\n - the convolution of raster \nR\nR\n with\n  the weight matrix \nW\nW\n.\n\n\nOne \nlocal operation\n that puts all components toghether for each pixel in \nR\nR\n.",
            "title": "Getis-ord G* on rasters"
        },
        {
            "location": "/methods/getis-ord-raster/#getis-ord-g-on-rasters",
            "text": "The  Standard Getis-ord  is defined on individual points (vector data).\nIn many situations, we want to compute  G^*_i G^*_i  in a raster rather than in a point cloud.\nThis way, the computation can be expressed using map algebra operations and nicely parallelized in frameworks \nsuch as  Geotrellis .  The rasterized Getis-Ord formula looks as follows:   \nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }  \nG^*(R, W, N, M, S) =\n    \\frac{\n        R{\\stackrel{\\mathtt{sum}}{\\circ}}W\n        - M*\\sum_{w \\in W}{w}\n    }{\n        S \\sqrt{\n          \\frac{\n            N*\\sum_{w \\in W}{w^2} - (\\sum_{w \\in W}{w})^2\n          }{\n            N - 1\n          }\n        }\n    }   where:   R R  is the input raster.  W W  is a weight matrix of values between 0 and 1.\n  The matrix is square and has odd dimensions, e.g.  5 \\times 5 5 \\times 5 ,  31 \\times 31 31 \\times 31  ...  N N  represents the number of all pixels in  R R  (because there can be NA values)  M M  represents the global mean of  R R .  S S  represents the global standard deviation of all pixels in  R R .   It can be seen that the formula can be nicely refactored into:   One  global operation  that computes  N N ,  M M ,  S S . These values are usually available because they\n  were pre-computed when the raster layer has been ingested into the catalog.  One  focal operation   R{\\stackrel{\\mathtt{sum}}{\\circ}}W R{\\stackrel{\\mathtt{sum}}{\\circ}}W  - the convolution of raster  R R  with\n  the weight matrix  W W .  One  local operation  that puts all components toghether for each pixel in  R R .",
            "title": "Getis-ord G* on rasters"
        },
        {
            "location": "/methods/getis-ord/",
            "text": "Standards Getis-Ord G*\n\u00b6\n\n\nThe standard definition of Getis-Ord \nG^*_i\nG^*_i\n statistic assumes a study area with \nn\nn\n points with measurements\n\nX = [x_1, \\ldots, x_n]\nX = [x_1, \\ldots, x_n]\n. Moreover, it assumes weights \nw_{i,j}\nw_{i,j}\n to be defined between all pairs of points \ni\ni\n\nand \nj\nj\n (for all \ni,j \\in \\{ 1, \\ldots, n\\}\ni,j \\in \\{ 1, \\ldots, n\\}\n). The formula to compute \nG^*_i\nG^*_i\n at a given point \ni\ni\n is then:\n\n\n\n\n\n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\n\n\n\n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }\n\n\n\n\n\nwhere:\n\n\n\n\n\\bar{X}\n\\bar{X}\n is the mean of all measurements,\n\n\nS\nS\n is the standard deviation of all measurements.\n\n\n\n\nAs it is known, this statistic creates a z-score, which denotes the significance of an area in relation\nto its surrounding areas.\n\n\n\n\nNote\n\n\nFor \nx \\in X\nx \\in X\n, the \nzscore(x) = \\frac{x - mean(X)}{stdev(X)}\nzscore(x) = \\frac{x - mean(X)}{stdev(X)}\n\n\n\n\n\n\nTodo\n\n\nJulian Bruns: Add references to papers",
            "title": "Standards Getis-Ord G*"
        },
        {
            "location": "/methods/getis-ord/#standards-getis-ord-g",
            "text": "The standard definition of Getis-Ord  G^*_i G^*_i  statistic assumes a study area with  n n  points with measurements X = [x_1, \\ldots, x_n] X = [x_1, \\ldots, x_n] . Moreover, it assumes weights  w_{i,j} w_{i,j}  to be defined between all pairs of points  i i \nand  j j  (for all  i,j \\in \\{ 1, \\ldots, n\\} i,j \\in \\{ 1, \\ldots, n\\} ). The formula to compute  G^*_i G^*_i  at a given point  i i  is then:   \n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }  \n    G^*_i = \\frac{\n        \\sum_{j=1}^{n}w_{i,j}x_j - \\bar{X}\\sum_{j=1}^{n}w_{i,j}\n    }{\n        S \\sqrt{\n            \\frac{\n              n \\sum_{j=1}^{n}w_{i,j}^2 - (\\sum_{j=1}^{n}w_{i,j})^2\n            }{n-1}\n        }\n    }   where:   \\bar{X} \\bar{X}  is the mean of all measurements,  S S  is the standard deviation of all measurements.   As it is known, this statistic creates a z-score, which denotes the significance of an area in relation\nto its surrounding areas.   Note  For  x \\in X x \\in X , the  zscore(x) = \\frac{x - mean(X)}{stdev(X)} zscore(x) = \\frac{x - mean(X)}{stdev(X)}    Todo  Julian Bruns: Add references to papers",
            "title": "Standards Getis-Ord G*"
        },
        {
            "location": "/methods/one-vs-rest/",
            "text": "One vs. Rest\n\u00b6\n\n\nThe One-versus-Rest (or One-versus-All = OvA) Strategy is a Method to use Binary (Two-Class) Classifiers\n(such as \nSupport Vector Machines\n ) for classifying multiple\n(more than two) Classes.\n\n\n\n\nNote\n\n\n\n\nhttps://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest\n\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#one-vs-rest-classifier-aka-one-vs-all\n\n\n\n\n\n\nOne-vs.-rest\n\u00b6\n\n\nInputs\n\u00b6\n\n\n\n\nL \\text{ , a learner (training algorithm for binary classifiers) }\nL \\text{ , a learner (training algorithm for binary classifiers) }\n\n\n\\text{ samples } \\vec{X}\n\\text{ samples } \\vec{X}\n\n\n\\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i\n\\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i\n\n\n\n\nOutput\n\u00b6\n\n\n\n\n\\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K}\n\\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K}\n\n\n\n\nProcedure\n\u00b6\n\n\n\\text{ For each } k \\text{ in } {1,\\ldots,K}\n\\text{ For each } k \\text{ in } {1,\\ldots,K}\n\n\n\n\n\n\\begin{align}\n&\\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1, & \\text{ if } y_i = k \\\\\nz_i = 0, & \\text{ otherwise }\n\\end{cases}\\\\\n&\\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\\n\n\n\n\n\\begin{align}\n&\\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1, & \\text{ if } y_i = k \\\\\nz_i = 0, & \\text{ otherwise }\n\\end{cases}\\\\\n&\\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\\n\n\n\n\n\nMaking decisions means applying all classifiers to an unseen sample and\npredicting the label for which the corresponding classifier reports the\nhighest confidence score:\n\n\n\n\n\n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)\n\n\n\n\n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)\n\n\n\n\n\n\n\nTodo\n\n\nAdrian Klink: Add references, optimize description",
            "title": "One vs. Rest"
        },
        {
            "location": "/methods/one-vs-rest/#one-vs-rest",
            "text": "The One-versus-Rest (or One-versus-All = OvA) Strategy is a Method to use Binary (Two-Class) Classifiers\n(such as  Support Vector Machines  ) for classifying multiple\n(more than two) Classes.   Note   https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest  https://spark.apache.org/docs/latest/ml-classification-regression.html#one-vs-rest-classifier-aka-one-vs-all",
            "title": "One vs. Rest"
        },
        {
            "location": "/methods/one-vs-rest/#one-vs-rest_1",
            "text": "",
            "title": "One-vs.-rest"
        },
        {
            "location": "/methods/one-vs-rest/#inputs",
            "text": "L \\text{ , a learner (training algorithm for binary classifiers) } L \\text{ , a learner (training algorithm for binary classifiers) }  \\text{ samples } \\vec{X} \\text{ samples } \\vec{X}  \\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i \\text{ labels } \\vec{y} \\\\ \\text{ where } y_i \\in {1,\\ldots,K} \\text{ is the label for the sample } X_i",
            "title": "Inputs"
        },
        {
            "location": "/methods/one-vs-rest/#output",
            "text": "\\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K} \\text{ a list of classifiers } f_k \\text{ for } k \\in {1,\\ldots,K}",
            "title": "Output"
        },
        {
            "location": "/methods/one-vs-rest/#procedure",
            "text": "\\text{ For each } k \\text{ in } {1,\\ldots,K} \\text{ For each } k \\text{ in } {1,\\ldots,K}   \n\\begin{align}\n&\\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1, & \\text{ if } y_i = k \\\\\nz_i = 0, & \\text{ otherwise }\n\\end{cases}\\\\\n&\\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\  \n\\begin{align}\n&\\text{ Construct a new label vector } \\vec{z} \\text{ where }\n\\begin{cases}\nz_i = 1, & \\text{ if } y_i = k \\\\\nz_i = 0, & \\text{ otherwise }\n\\end{cases}\\\\\n&\\text{ Apply } L \\text{ to } \\vec{X}, \\vec{z} \\text{ to obtain } f_k \\\\\n\\end{align}\\\\   Making decisions means applying all classifiers to an unseen sample and\npredicting the label for which the corresponding classifier reports the\nhighest confidence score:   \n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)  \n\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\text{argmax}}\\; f_k(x)    Todo  Adrian Klink: Add references, optimize description",
            "title": "Procedure"
        },
        {
            "location": "/methods/soh/",
            "text": "Stability of hotspots\n\u00b6\n\n\n\n\nTodo\n\n\nIntro text\n\n\n\n\nRelated demos:\n- \nstability of hotspots",
            "title": "Stability of hotspots"
        },
        {
            "location": "/methods/soh/#stability-of-hotspots",
            "text": "Todo  Intro text   Related demos:\n-  stability of hotspots",
            "title": "Stability of hotspots"
        },
        {
            "location": "/methods/spectral-analysis-tci/",
            "text": "Triangular Chlorophyll Index (TCI) in Spectral Remote Sensing\n\u00b6\n\n\nThe Triangular Chlorophyll Index (TCI) is widely used in remote sensing in the field of agricultural studies.\nA typical use case is e.g. the quantification of vegetation in an area for the purpose of land-use classification.\n\n\nThe TCI bases on the absorption maximum and thus a minimum of reflectance of chlorophyll at a wavelength of roughly\n670 nm. The stronger the minimum of reflectance is expressed in the spectrum under survey, the higher the TCI value.\n\n\nThe TCI is calculated according to the following formula\n1\n:\n\n\n\n\n\n  TCI =\n    1.2 \\cdot {\n      \\left( {R_{700nm}-R_{550nm}} \\right)\n    }\n    -1.5 \\cdot {\n      \\left( {R_{670nm}-R_{550nm}} \\right)\n    }\n    \\cdot {\n      \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}\n    }\n\n\n\n\n  TCI =\n    1.2 \\cdot {\n      \\left( {R_{700nm}-R_{550nm}} \\right)\n    }\n    -1.5 \\cdot {\n      \\left( {R_{670nm}-R_{550nm}} \\right)\n    }\n    \\cdot {\n      \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}\n    }\n\n\n\n\n\nHere, \nR_{550nm}\nR_{550nm}\n, \nR_{670nm}\nR_{670nm}\n and \nR_{700nm}\nR_{700nm}\n denote the reflectance for the wavelengths 550nm, 670nm and 700nm,\nrespectively. In order to deduce the reflectance values from the recorded reflected intensities a proper white\nbalance has to be provided.\n\n\n\n\n\n\n\n\n\n\nhttps://www.indexdatabase.de/db/i-single.php?id=392\n\u00a0\n\u21a9",
            "title": "Triangular Chlorophyll Index (TCI) in Spectral Remote Sensing"
        },
        {
            "location": "/methods/spectral-analysis-tci/#triangular-chlorophyll-index-tci-in-spectral-remote-sensing",
            "text": "The Triangular Chlorophyll Index (TCI) is widely used in remote sensing in the field of agricultural studies.\nA typical use case is e.g. the quantification of vegetation in an area for the purpose of land-use classification.  The TCI bases on the absorption maximum and thus a minimum of reflectance of chlorophyll at a wavelength of roughly\n670 nm. The stronger the minimum of reflectance is expressed in the spectrum under survey, the higher the TCI value.  The TCI is calculated according to the following formula 1 :   \n  TCI =\n    1.2 \\cdot {\n      \\left( {R_{700nm}-R_{550nm}} \\right)\n    }\n    -1.5 \\cdot {\n      \\left( {R_{670nm}-R_{550nm}} \\right)\n    }\n    \\cdot {\n      \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}\n    }  \n  TCI =\n    1.2 \\cdot {\n      \\left( {R_{700nm}-R_{550nm}} \\right)\n    }\n    -1.5 \\cdot {\n      \\left( {R_{670nm}-R_{550nm}} \\right)\n    }\n    \\cdot {\n      \\sqrt{\\frac{R_{700nm}}{R_{670nm}}}\n    }   Here,  R_{550nm} R_{550nm} ,  R_{670nm} R_{670nm}  and  R_{700nm} R_{700nm}  denote the reflectance for the wavelengths 550nm, 670nm and 700nm,\nrespectively. In order to deduce the reflectance values from the recorded reflected intensities a proper white\nbalance has to be provided.      https://www.indexdatabase.de/db/i-single.php?id=392 \u00a0 \u21a9",
            "title": "Triangular Chlorophyll Index (TCI) in Spectral Remote Sensing"
        },
        {
            "location": "/methods/svm/",
            "text": "Support Vector Machine\n\u00b6\n\n\nA Support Vector Machine (SVM) is a supervised learning model (machine learning) which can be used to classify image data. \nFor Parallelization we use a Linear SVM from \nApache Spark\n.\nSince we have more than two classes the One versus All (or \nOne vs. Rest\n ) Strategy is used.\n\n\n\n\nNote\n\n\n\n\nhttps://en.wikipedia.org/wiki/Support_vector_machine\n\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine\n\n\n\n\n\n\nLinear SVM\n\u00b6\n\n\nWe are given a training dataset of \nn\nn\n points of the form\n\n\n\n\n\n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)\n\n\n\n\n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)\n\n\n\n\n\nwhere the \ny_i\ny_i\n are either 1 or \u22121, each indicating the class to which\nthe point \n\\vec{x}_i\n\\vec{x}_i\n belongs. Each \n\\vec{x}_i\n\\vec{x}_i\n is a \np\np\n-dimensional\n\nreal\n vector. We want to find the\n\"maximum-margin hyperplane\" that divides the group of points\n\n\\vec{x}_i\n\\vec{x}_i\n for which \ny_i=1\ny_i=1\n from the group of points for which\n\ny_i=-1\ny_i=-1\n, which is defined so that the distance between the hyperplane\nand the nearest point \n\\vec{x}_i\n\\vec{x}_i\n from either group is maximized.\n\n\nAny \nhyperplane\n can be written as the set of\npoints \n\\vec{x}\n\\vec{x}\n satisfying\n\n\n\n\n\n  \\vec{w}\\cdot\\vec{x} - b=0\n\n\n\n\n  \\vec{w}\\cdot\\vec{x} - b=0\n\n\n\n\n\nwhere \n{\\vec{w}}\n{\\vec{w}}\n is the (not necessarily normalized) \nnormal\nvector\n to the hyperplane. This is much\nlike \nHesse normal form\n, except that\n\n{\\vec{w}}\n{\\vec{w}}\n is not necessarily a unit vector. The parameter\n\n\\tfrac{b}{\\|\\vec{w}\\|}\n\\tfrac{b}{\\|\\vec{w}\\|}\n determines the offset of the hyperplane from\nthe origin along the normal vector \n{\\vec{w}}\n{\\vec{w}}\n.\n\n\n\n\n\n\nFigure:\n\nMaximum-margin hyperplane and margins for an SVM trained with samples from two classes.\nSamples on the margin are called the support vectors.\n\n\n\n\nHard-margin\n\u00b6\n\n\nIf the training data are \nlinearly\nseparable\n, we can select two parallel\nhyperplanes that separate the two classes of data, so that the distance\nbetween them is as large as possible. The region bounded by these two\nhyperplanes is called the \\\"margin\\\", and the maximum-margin hyperplane\nis the hyperplane that lies halfway between them. These hyperplanes can\nbe described by the equations\n$$\n  \\vec{w}\\cdot\\vec{x} - b=1\n$$\n\n\nand\n\n\n\n\n\n  \\vec{w}\\cdot\\vec{x} - b=-1\n\n\n\n\n  \\vec{w}\\cdot\\vec{x} - b=-1\n\n\n\n\n\nGeometrically, the distance between these two hyperplanes is\n\n\\tfrac{2}{\\|\\vec{w}\\|}\n\\tfrac{2}{\\|\\vec{w}\\|}\n, so to maximize the distance between the planes\nwe want to minimize \n\\|\\vec{w}\\|\n\\|\\vec{w}\\|\n. As we also have to prevent data\npoints from falling into the margin, we add the following constraint:\nfor each \ni\ni\n either\n\n\n\n\n\n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1\n\n\n\n\n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1\n\n\n\n\n\nor\n\n\n\n\n\n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1\n\n\n\n\n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1\n\n\n\n\n\nThese constraints state that each data point must lie on the correct\nside of the margin.\n\n\nThis can be rewritten as:\n\n\n\n\n\n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)\n\n\n\n\n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)\n\n\n\n\n\nWe can put this together to get the optimization problem:\n\n\n\n\n\n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n\n\n\n\n\n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n\n\n\n\n\n\nThe \n\\vec w\n\\vec w\n and \nb\nb\n that solve this problem determine our classifier,\n$$\n  \\vec{x} \\mapsto sgn(\\vec{w} \\cdot \\vec{x} - b)\n$$\n\n\nAn easy-to-see but important consequence of this geometric description\nis that the max-margin hyperplane is completely determined by those\n\n\\vec{x}_i\n\\vec{x}_i\n which lie nearest to it. These \n\\vec{x}_i\n\\vec{x}_i\n are called\n\nsupport vectors.\n\n\nSoft-margin\n\u00b6\n\n\nTo extend SVM to cases in which the data are not linearly separable, we\nintroduce the \nhinge loss\n function:\n\n\n\n\n\n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)\n\n\n\n\n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)\n\n\n\n\n\nThis function is zero if the constraint in (1) is satisfied, in other\nwords, if \n\\vec{x}_i\n\\vec{x}_i\n lies on the correct side of the margin. For data\non the wrong side of the margin, the function\\'s value is proportional\nto the distance from the margin.\n\n\nWe then wish to minimize\n\n\n\n\n\n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,\n\n\n\n\n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,\n\n\n\n\n\nwhere the parameter \n\\lambda\n\\lambda\n determines the tradeoff between increasing\nthe margin-size and ensuring that the \n\\vec{x}_i\n\\vec{x}_i\n lie on the correct\nside of the margin. Thus, for sufficiently small values of \n\\lambda\n\\lambda\n,\nthe soft-margin SVM will behave identically to the hard-margin SVM if\nthe input data are linearly classifiable, but will still learn if a\nclassification rule is viable or not.\n\n\n\n\nTodo\n\n\nAdrian Klink: Add references, optimize description",
            "title": "Support Vector Machine"
        },
        {
            "location": "/methods/svm/#support-vector-machine",
            "text": "A Support Vector Machine (SVM) is a supervised learning model (machine learning) which can be used to classify image data. \nFor Parallelization we use a Linear SVM from  Apache Spark .\nSince we have more than two classes the One versus All (or  One vs. Rest  ) Strategy is used.   Note   https://en.wikipedia.org/wiki/Support_vector_machine  https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine",
            "title": "Support Vector Machine"
        },
        {
            "location": "/methods/svm/#linear-svm",
            "text": "We are given a training dataset of  n n  points of the form   \n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)  \n  (\\vec{x}_1, y_1),\\, \\ldots ,\\, (\\vec{x}_n, y_n)   where the  y_i y_i  are either 1 or \u22121, each indicating the class to which\nthe point  \\vec{x}_i \\vec{x}_i  belongs. Each  \\vec{x}_i \\vec{x}_i  is a  p p -dimensional real  vector. We want to find the\n\"maximum-margin hyperplane\" that divides the group of points \\vec{x}_i \\vec{x}_i  for which  y_i=1 y_i=1  from the group of points for which y_i=-1 y_i=-1 , which is defined so that the distance between the hyperplane\nand the nearest point  \\vec{x}_i \\vec{x}_i  from either group is maximized.  Any  hyperplane  can be written as the set of\npoints  \\vec{x} \\vec{x}  satisfying   \n  \\vec{w}\\cdot\\vec{x} - b=0  \n  \\vec{w}\\cdot\\vec{x} - b=0   where  {\\vec{w}} {\\vec{w}}  is the (not necessarily normalized)  normal\nvector  to the hyperplane. This is much\nlike  Hesse normal form , except that {\\vec{w}} {\\vec{w}}  is not necessarily a unit vector. The parameter \\tfrac{b}{\\|\\vec{w}\\|} \\tfrac{b}{\\|\\vec{w}\\|}  determines the offset of the hyperplane from\nthe origin along the normal vector  {\\vec{w}} {\\vec{w}} .    Figure: \nMaximum-margin hyperplane and margins for an SVM trained with samples from two classes.\nSamples on the margin are called the support vectors.",
            "title": "Linear SVM"
        },
        {
            "location": "/methods/svm/#hard-margin",
            "text": "If the training data are  linearly\nseparable , we can select two parallel\nhyperplanes that separate the two classes of data, so that the distance\nbetween them is as large as possible. The region bounded by these two\nhyperplanes is called the \\\"margin\\\", and the maximum-margin hyperplane\nis the hyperplane that lies halfway between them. These hyperplanes can\nbe described by the equations\n$$\n  \\vec{w}\\cdot\\vec{x} - b=1\n$$  and   \n  \\vec{w}\\cdot\\vec{x} - b=-1  \n  \\vec{w}\\cdot\\vec{x} - b=-1   Geometrically, the distance between these two hyperplanes is \\tfrac{2}{\\|\\vec{w}\\|} \\tfrac{2}{\\|\\vec{w}\\|} , so to maximize the distance between the planes\nwe want to minimize  \\|\\vec{w}\\| \\|\\vec{w}\\| . As we also have to prevent data\npoints from falling into the margin, we add the following constraint:\nfor each  i i  either   \n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1  \n  \\vec{w}\\cdot\\vec{x}_i - b \\ge 1, \\text{if}\\; y_i = 1   or   \n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1  \n  \\vec{w}\\cdot\\vec{x}_i - b \\le -1,\\text{if}\\; y_i = -1   These constraints state that each data point must lie on the correct\nside of the margin.  This can be rewritten as:   \n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)  \n  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\quad \\text{ for all } 1 \\le i \\le n.\\qquad\\qquad(1)   We can put this together to get the optimization problem:   \n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n  \n  \\text{Minimize } \\|\\vec{w}\\| \\text{ subject to } y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1, \\text{ for } i = 1, \\ldots, n   The  \\vec w \\vec w  and  b b  that solve this problem determine our classifier,\n$$\n  \\vec{x} \\mapsto sgn(\\vec{w} \\cdot \\vec{x} - b)\n$$  An easy-to-see but important consequence of this geometric description\nis that the max-margin hyperplane is completely determined by those \\vec{x}_i \\vec{x}_i  which lie nearest to it. These  \\vec{x}_i \\vec{x}_i  are called support vectors.",
            "title": "Hard-margin"
        },
        {
            "location": "/methods/svm/#soft-margin",
            "text": "To extend SVM to cases in which the data are not linearly separable, we\nintroduce the  hinge loss  function:   \n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)  \n  \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i + b)\\right)   This function is zero if the constraint in (1) is satisfied, in other\nwords, if  \\vec{x}_i \\vec{x}_i  lies on the correct side of the margin. For data\non the wrong side of the margin, the function\\'s value is proportional\nto the distance from the margin.  We then wish to minimize   \n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,  \n  \\left[\n    \\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w}\\cdot \\vec{x}_i + b)\\right)\n  \\right] + \\lambda\\lVert \\vec{w} \\rVert^2,   where the parameter  \\lambda \\lambda  determines the tradeoff between increasing\nthe margin-size and ensuring that the  \\vec{x}_i \\vec{x}_i  lie on the correct\nside of the margin. Thus, for sufficiently small values of  \\lambda \\lambda ,\nthe soft-margin SVM will behave identically to the hard-margin SVM if\nthe input data are linearly classifiable, but will still learn if a\nclassification rule is viable or not.   Todo  Adrian Klink: Add references, optimize description",
            "title": "Soft-margin"
        },
        {
            "location": "/scenarios/smartcity/",
            "text": "Responsible person for this section\n\n\nJulian Bruns\n\n\n\n\nSmart City\n\u00b6",
            "title": "Smart City"
        },
        {
            "location": "/scenarios/smartcity/#smart-city",
            "text": "",
            "title": "Smart City"
        },
        {
            "location": "/scenarios/disaster/",
            "text": "Responsible person for this section\n\n\n\n\nAlexander Groeschel\n\n\nBodo Bernsdorf\n\n\n\n\n\n\nDisaster Management\n\u00b6\n\n\n\n\nGeneral\n\u00b6\n\n\nDisasters and accidents always happen somewhere. Therefore, it is necessary to use geodata even in the case of small\naccident scenarios in order to assess access routes and their impact on the environment. Information support for the\nsafety authorities is necessary in many cases.\n\n\nSome examples\n\u00b6\n\n\nComplex traffic accidents require indications of conceivable access routes for the rescue services and, in particular,\nindications of departure routes for the rescue service in order to be able to supply patients quickly.\n\n\nLocal storms lead to blocked or flooded roads and destroyed infrastructure, which is why rescue forces must be informed\nabout the main areas of operation, access routes and safe areas for the potential accommodation of evacuees.\n\n\nLarge fires - for example in a recycling plant - produce highly contaminated flue gases which, depending on the weather\nconditions, remain on site (e. g. inversion weather conditions) or drift and diffuse. In addition to the question of the\nappropriate approaching route for emergency forces, it becomes necessary to warn the population in a short time. For\nthat, weather and meteorological data are of interest.\n\n\nLarge scale disasters also require a lot of spatial data at an early stage in order to be able to identify the already\nor potentially affected areas and assess the development of the situation.\n\n\nChallenges\n\u00b6\n\n\nIn many accidents, immediate action is required with very little time planning. The example scenario of a dangerous\ngoods accident illustrates this, but can be easily transferred, for example, to attacks or incidents in production\nplants with leaking harmful substances. In order to plan the measures at short notice, considerable amounts of data have\nto be processed in the shortest possible time and \"converted\" into targeted information for the heads of operations. The\nbasis for this is the investigation carried out by the fire brigades' management cycle, which in such cases are always\nheaded by the fire brigades. In this case, important data must be collected and interpreted for all further steps.\nWithout accurate information, only incomplete assessments of the situation and thus decisions on measures based on\nincomplete assumptions are possible.\n\n\nDepending on the dynamics of the situation, the operations manager must quickly assess the dangers for humans, animals,\nthe environment and material assets and decide how to proceed. For example, liquid and gaseous pollutants spread very\nquickly depending on weather conditions and geographical characteristics of the location. Often the outside temperature\ndetermines the formation of flammable atmospheres and reaction products, which drift into the environment in the form of\nnoxious gas clouds - very difficult to predict in the event of changes in wind direction. In addition, some substances\nare invisible to the human eye and/or odourless or cannot be detected or determined due to the hazard and shut-off\nlimits in accordance with the fire brigade regulations.\n\n\nData Sources\n\u00b6\n\n\nAs a basis for assessing the situation, considerable amounts of geodata are needed, the processing and provision of\nwhich is the core of the subject area. The basis for this is the OGC1-compliant geodata portals of the federal states\nand municipalities, which provide information on topography, the water network, the environment, population, sewers and\ndirect dischargers, for example. Until recently, however, this data was only updated in multi-annual cycles, which made\nit necessary to supplement current data sources. Since June 2015, the Sentinel 2 mission, as part of ESA's COPERNICUS\nsystem, has been delivering high spatial resolution satellite data in the visible and radar range with 14-day update\ncycles.\n\n\nIn addition to the geodata, an on-site investigation is necessary to determine the necessary measures. This can be\ncomplicated by various factors. For example, substance identification can be problematic in accidents involving\ndangerous goods transporters from a distance, as the prescribed vehicle signage is not always up-to-date and loading\ndocuments are located in the inaccessible driver's cab. This makes the acquisition of the latest data at the place of\nuse indispensable. This includes measurement data obtained via portable sensors (catalytic multi-warning and Ex devices,\nchemo-sensors or test tubes). In larger locations, terrestrial measuring vehicles (so-called NBC scanners) or the\nAnalytical Task Force (ATF) are used. Such vehicles have photoionization detectors (PID) or optical sensors such as the\nSIGIS2 system. However, these measurements are often complex and associated with dangers, for example in the case of a\npotentially 30,000 litre petrol-contaminated vehicle.\n\n\nNew Solution Concepts within the BigGIS Project\n\u00b6\n\n\nBigGIS is intended to enable an improved operational environment evaluation by the head of operations by combining the\ncurrently acquired mass data with equally extensive and complex archive data. To this end, a new approach of\nexploration, analysis, integration and visualisation must be developed for the described application scenarios. The goal\nis a much faster and more reliable solution for providing approach routes to the site, the location of supply areas for\nemergency personnel moving up, as well as the situation evaluation at the site of operation, including the detection of\npossible pollutants and the prediction of their temporal and spatial dispersion and the derivation of suitable measures.\n\n\nIn addition to the previously available geo and meteorological data, the BigGIS project will use optical, airborne data\nto record traffic infrastructure, existing buildings, protected goods and, if necessary, the identification of\nendangered persons. This data can be captured by a \nflying robot equipped with multi-spectral or hyperspectral\nsensors\n (similar to SIGIS2) and flying autonomously over the scene of the\naccident. These so-called UAS (Unmanned Aerial Systems) continuously send data over a broad spectrum (visible light up\nto infrared or far infrared) of the underlying air layers to BigGIS. Hyperspectral sensors often generate data volumes\nin the gigabyte range for each photo flight and application. BigGIS should be able to handle these data volumes and\noffer suitable APIs to perform data analysis and integration at scale.\n\n\nFurther images taken with the aid of the flying robots indicate the direction and velocity of propagation of pollutants\n(liquid, gaseous). This allows for forecasts of the diffusion and drift of the pollutants more accurately than with the\nusual approaches (Halpaap's club, MET or Memplex club etc.). In principle, it is possible to classify escaping\nsubstances or reaction products in pollutant pools or noxious gas clouds by means of absorption spectra in the infrared\nand far infrared range. Sensors such as Hygas, SIGIS 2 and CHARM are already available as ground stated or aviated\nsystems for that purpose. In the BigGIS project, \ninitial feasibility\nstudies\n were carried out to test the substance identification of simulated\nnoxious gas clouds with multi-spectral cameras for the visible range, which are suitable for use with small drones due\nto their relatively low weight.\n\n\nIn order to make efficient use of such drones, the BigGIS project is offers an implementation of \nautonomous, optimal\nflight planning\n by the drone itself.\n\n\nAt the same time, models are being developed that can \npredict the propagation of a detected gas\ncloud\n. The aim is to predict possible future scenarios for the spread of the pollutant,\nwind directions and strengths (e. g. taken from FeWIS), the geo-positions of neighbouring residential areas and their\nsurrounding structure, for example via satellite images, geographical characteristics of the accident area, depending on\nthe information on the spread of the pollutant, wind direction and strength (e. g. taken from FeWIS). The predicted\nscenario is to be presented to the operations manager in a compact, understandable way by means of new visual techniques\ndirectly with the (unsure) knowledge about the spread of the pollutants and thus enable decision support.",
            "title": "Disaster Management"
        },
        {
            "location": "/scenarios/disaster/#disaster-management",
            "text": "",
            "title": "Disaster Management"
        },
        {
            "location": "/scenarios/disaster/#general",
            "text": "Disasters and accidents always happen somewhere. Therefore, it is necessary to use geodata even in the case of small\naccident scenarios in order to assess access routes and their impact on the environment. Information support for the\nsafety authorities is necessary in many cases.",
            "title": "General"
        },
        {
            "location": "/scenarios/disaster/#some-examples",
            "text": "Complex traffic accidents require indications of conceivable access routes for the rescue services and, in particular,\nindications of departure routes for the rescue service in order to be able to supply patients quickly.  Local storms lead to blocked or flooded roads and destroyed infrastructure, which is why rescue forces must be informed\nabout the main areas of operation, access routes and safe areas for the potential accommodation of evacuees.  Large fires - for example in a recycling plant - produce highly contaminated flue gases which, depending on the weather\nconditions, remain on site (e. g. inversion weather conditions) or drift and diffuse. In addition to the question of the\nappropriate approaching route for emergency forces, it becomes necessary to warn the population in a short time. For\nthat, weather and meteorological data are of interest.  Large scale disasters also require a lot of spatial data at an early stage in order to be able to identify the already\nor potentially affected areas and assess the development of the situation.",
            "title": "Some examples"
        },
        {
            "location": "/scenarios/disaster/#challenges",
            "text": "In many accidents, immediate action is required with very little time planning. The example scenario of a dangerous\ngoods accident illustrates this, but can be easily transferred, for example, to attacks or incidents in production\nplants with leaking harmful substances. In order to plan the measures at short notice, considerable amounts of data have\nto be processed in the shortest possible time and \"converted\" into targeted information for the heads of operations. The\nbasis for this is the investigation carried out by the fire brigades' management cycle, which in such cases are always\nheaded by the fire brigades. In this case, important data must be collected and interpreted for all further steps.\nWithout accurate information, only incomplete assessments of the situation and thus decisions on measures based on\nincomplete assumptions are possible.  Depending on the dynamics of the situation, the operations manager must quickly assess the dangers for humans, animals,\nthe environment and material assets and decide how to proceed. For example, liquid and gaseous pollutants spread very\nquickly depending on weather conditions and geographical characteristics of the location. Often the outside temperature\ndetermines the formation of flammable atmospheres and reaction products, which drift into the environment in the form of\nnoxious gas clouds - very difficult to predict in the event of changes in wind direction. In addition, some substances\nare invisible to the human eye and/or odourless or cannot be detected or determined due to the hazard and shut-off\nlimits in accordance with the fire brigade regulations.",
            "title": "Challenges"
        },
        {
            "location": "/scenarios/disaster/#data-sources",
            "text": "As a basis for assessing the situation, considerable amounts of geodata are needed, the processing and provision of\nwhich is the core of the subject area. The basis for this is the OGC1-compliant geodata portals of the federal states\nand municipalities, which provide information on topography, the water network, the environment, population, sewers and\ndirect dischargers, for example. Until recently, however, this data was only updated in multi-annual cycles, which made\nit necessary to supplement current data sources. Since June 2015, the Sentinel 2 mission, as part of ESA's COPERNICUS\nsystem, has been delivering high spatial resolution satellite data in the visible and radar range with 14-day update\ncycles.  In addition to the geodata, an on-site investigation is necessary to determine the necessary measures. This can be\ncomplicated by various factors. For example, substance identification can be problematic in accidents involving\ndangerous goods transporters from a distance, as the prescribed vehicle signage is not always up-to-date and loading\ndocuments are located in the inaccessible driver's cab. This makes the acquisition of the latest data at the place of\nuse indispensable. This includes measurement data obtained via portable sensors (catalytic multi-warning and Ex devices,\nchemo-sensors or test tubes). In larger locations, terrestrial measuring vehicles (so-called NBC scanners) or the\nAnalytical Task Force (ATF) are used. Such vehicles have photoionization detectors (PID) or optical sensors such as the\nSIGIS2 system. However, these measurements are often complex and associated with dangers, for example in the case of a\npotentially 30,000 litre petrol-contaminated vehicle.",
            "title": "Data Sources"
        },
        {
            "location": "/scenarios/disaster/#new-solution-concepts-within-the-biggis-project",
            "text": "BigGIS is intended to enable an improved operational environment evaluation by the head of operations by combining the\ncurrently acquired mass data with equally extensive and complex archive data. To this end, a new approach of\nexploration, analysis, integration and visualisation must be developed for the described application scenarios. The goal\nis a much faster and more reliable solution for providing approach routes to the site, the location of supply areas for\nemergency personnel moving up, as well as the situation evaluation at the site of operation, including the detection of\npossible pollutants and the prediction of their temporal and spatial dispersion and the derivation of suitable measures.  In addition to the previously available geo and meteorological data, the BigGIS project will use optical, airborne data\nto record traffic infrastructure, existing buildings, protected goods and, if necessary, the identification of\nendangered persons. This data can be captured by a  flying robot equipped with multi-spectral or hyperspectral\nsensors  (similar to SIGIS2) and flying autonomously over the scene of the\naccident. These so-called UAS (Unmanned Aerial Systems) continuously send data over a broad spectrum (visible light up\nto infrared or far infrared) of the underlying air layers to BigGIS. Hyperspectral sensors often generate data volumes\nin the gigabyte range for each photo flight and application. BigGIS should be able to handle these data volumes and\noffer suitable APIs to perform data analysis and integration at scale.  Further images taken with the aid of the flying robots indicate the direction and velocity of propagation of pollutants\n(liquid, gaseous). This allows for forecasts of the diffusion and drift of the pollutants more accurately than with the\nusual approaches (Halpaap's club, MET or Memplex club etc.). In principle, it is possible to classify escaping\nsubstances or reaction products in pollutant pools or noxious gas clouds by means of absorption spectra in the infrared\nand far infrared range. Sensors such as Hygas, SIGIS 2 and CHARM are already available as ground stated or aviated\nsystems for that purpose. In the BigGIS project,  initial feasibility\nstudies  were carried out to test the substance identification of simulated\nnoxious gas clouds with multi-spectral cameras for the visible range, which are suitable for use with small drones due\nto their relatively low weight.  In order to make efficient use of such drones, the BigGIS project is offers an implementation of  autonomous, optimal\nflight planning  by the drone itself.  At the same time, models are being developed that can  predict the propagation of a detected gas\ncloud . The aim is to predict possible future scenarios for the spread of the pollutant,\nwind directions and strengths (e. g. taken from FeWIS), the geo-positions of neighbouring residential areas and their\nsurrounding structure, for example via satellite images, geographical characteristics of the accident area, depending on\nthe information on the spread of the pollutant, wind direction and strength (e. g. taken from FeWIS). The predicted\nscenario is to be presented to the operations manager in a compact, understandable way by means of new visual techniques\ndirectly with the (unsure) knowledge about the spread of the pollutants and thus enable decision support.",
            "title": "New Solution Concepts within the BigGIS Project"
        },
        {
            "location": "/scenarios/environment/",
            "text": "Environment\n\u00b6\n\n\n\n\nResponsible person for this section\n\n\n\n\nHannes M\u00fcller (LUBW)\n\n\nAndreas Abecker (Disy)\n\n\nJohannes Kutterer (Disy)\n\n\n\n\n\n\n\n\nIn the globalized world invasive species are a growing concern. Certain invasive species can lead to ecological damage\nand commercial losses and eventually health problem for humans and animals. The geographical nature of these phenomena\nadvise the analysis, evaluation and visualization with geographic information system (GIS).\n\n\n\n\nDrosophila suzukii (male)\n\n\n\nClearly recognizable is the dot on the wing. The spotted wing indicates that the picture\nshows an male individual. (Photo: Judy Gallagher, \nhttp://www.flickr.com/photos/52450054@N04/15675291741/\n,\n\nCreative Commons Attribution 2.0 Generic\n)\n\n\n\n\nOne invasive species which called attention in the last years is the Drosophila suzukii, commonly named the spotted wing\ndrosophila. The vinegar fly was widely observed in parts of Japan,Korea and China. From there the fly was spreaded\narround the world. In the last 10 years they could be observed in Swisserland and in South-West Germany (Rhine Vallay).\nIn other parts of the world e.g. the United States this spread has also been observed.\n\n\n\n\nVisualisation in Disy Cadenza\n\n\n\nDistribution of egg finds of the Drosophila suzukii in 2016 in south west Germany\n\n\n\n\nUnlike other Drosophila, it infests non-rotting and healthy fruits and is therefore of concern to fruit growers, such as\nvintners. The main commercial inpact are on summer fruit including cherries, blueberries, grapes, nectarines, pears,\nplums, pluots, peaches, raspberries, and strawberries. In the United States the yield loss estimates widely vary and\nreached 80% loss in some areas and crops \n1\n.\n\n\nFor the control of pests it is necessary to combine and analyze different information sources in real time.The BIGGIS\nproject adresses this challenge by covering the following aspects:\n\n\n\n\nCollecting data from several sources like the current distribution of the fly, wether conditions, elavation, landuse etc.\n\n\nData analysis of risk areas\n\n\nInteractive visualisation of risk areas and infestion likelyhood\n\n\n\n\n\n\n\n\n\n\n\n\nMark P. Bolda; Rachael E. Goodhue & Frank G. Zalom (2009). Spotted Wing Drosophila: Potential Economic Impact of\n  Newly Established Pest (\nPDF\n).\n  Giannini Foundation of Agricultural Economics, University of California.\u00a0\n\u21a9",
            "title": "Environment"
        },
        {
            "location": "/scenarios/environment/#environment",
            "text": "Responsible person for this section   Hannes M\u00fcller (LUBW)  Andreas Abecker (Disy)  Johannes Kutterer (Disy)     In the globalized world invasive species are a growing concern. Certain invasive species can lead to ecological damage\nand commercial losses and eventually health problem for humans and animals. The geographical nature of these phenomena\nadvise the analysis, evaluation and visualization with geographic information system (GIS).   Drosophila suzukii (male)  \nClearly recognizable is the dot on the wing. The spotted wing indicates that the picture\nshows an male individual. (Photo: Judy Gallagher,  http://www.flickr.com/photos/52450054@N04/15675291741/ , Creative Commons Attribution 2.0 Generic )   One invasive species which called attention in the last years is the Drosophila suzukii, commonly named the spotted wing\ndrosophila. The vinegar fly was widely observed in parts of Japan,Korea and China. From there the fly was spreaded\narround the world. In the last 10 years they could be observed in Swisserland and in South-West Germany (Rhine Vallay).\nIn other parts of the world e.g. the United States this spread has also been observed.   Visualisation in Disy Cadenza  \nDistribution of egg finds of the Drosophila suzukii in 2016 in south west Germany   Unlike other Drosophila, it infests non-rotting and healthy fruits and is therefore of concern to fruit growers, such as\nvintners. The main commercial inpact are on summer fruit including cherries, blueberries, grapes, nectarines, pears,\nplums, pluots, peaches, raspberries, and strawberries. In the United States the yield loss estimates widely vary and\nreached 80% loss in some areas and crops  1 .  For the control of pests it is necessary to combine and analyze different information sources in real time.The BIGGIS\nproject adresses this challenge by covering the following aspects:   Collecting data from several sources like the current distribution of the fly, wether conditions, elavation, landuse etc.  Data analysis of risk areas  Interactive visualisation of risk areas and infestion likelyhood       Mark P. Bolda; Rachael E. Goodhue & Frank G. Zalom (2009). Spotted Wing Drosophila: Potential Economic Impact of\n  Newly Established Pest ( PDF ).\n  Giannini Foundation of Agricultural Economics, University of California.\u00a0 \u21a9",
            "title": "Environment"
        }
    ]
}